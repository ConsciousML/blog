{
  
    
        "post0": {
            "title": "Support Vector Machines using cvxopt",
            "content": ". This blog post is about implementing Support Vector Machines from scratch using CVXOPT. We will go through the math behind the SVM method and test out the kernel RBF and linear kernel on generated data. . import os import torch import numpy as np import matplotlib.pyplot as plt from cvxopt import matrix as cvxmat from cvxopt import solvers as solver from sklearn.datasets import make_blobs from sklearn.preprocessing import StandardScaler . def generate_data(n_samples=200, n_features=2, centers=2, center_box=(-15., 15.), cluster_std=1.): &quot;&quot;&quot;Generates the clusters for testing linear SVMs Args: n_samples: An integer for the number of data points to be generated. n_features: An integer for the number of the columns in the output data. centers: An integer for the number of cluster to generate. center_box: A tuple of float for the location of the clusters. cluster_std: A float that controls the standard deviation of the clusters. &quot;&quot;&quot; X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=centers, center_box=center_box, cluster_std=cluster_std) y[y == 0] = -1 y = y.astype(float) st = StandardScaler() X_s = st.fit_transform(X) return X_s, y . X, y = generate_data() . plt.scatter(X[:, 0], X[:, 1], c=y) . &lt;matplotlib.collections.PathCollection at 0x16c4dd86448&gt; . Here are two linearly separable clusters that we will use to test the Hard Margin SVM method. . The kernel Trick . First let&#39;s define the kernel trick to understand the magic behind SVMs. A kernel function $ kappa( boldsymbol{x}, boldsymbol{x^{&#39;}})$ is a measure of similarity between two objects $ boldsymbol{x}, boldsymbol{x^{&#39;}} in chi$ where $ chi$ is an abstract space of higher dimension. Let&#39;s assume that we have $ boldsymbol{X}$ a non-linearly separable data. We would like to transform the data into a higher dimensional space to make it linearly separable. Let us define $ phi$ a function that maps the data into a higher dimensional space. A classical approach would be to compute $ phi( boldsymbol{X})$ but this transformation may be very computationally heavy. Instead we use the kernel trick. . The kernel trick method stands for calculating the relationship of data in higher dimension without actually transforming the whole data. The kernel trick reduces the amount of computation required for kernelized algorithms. The main idea is, instead of computing $ phi( boldsymbol{X})$, we will compute the dot product $ phi( boldsymbol{X}_i)^ top phi( boldsymbol{X}_j)$ for all the components of $ boldsymbol{X}$. In other words, we will know the relationship of the data in higher dimension, without ever computing the transformation of the whole data. . The kernel function is defined as: $$ kappa( boldsymbol{x}, boldsymbol{x^{&#39;}}) = phi( boldsymbol{x})^ top phi( boldsymbol{x^{&#39;}}) $$ . The kernel function has the following properties: $$ begin{align} kappa( boldsymbol{x}, boldsymbol{x^{&#39;}}) &amp; in mathbb{R} kappa( boldsymbol{x}, boldsymbol{x}^{&#39;}) &amp;= kappa( boldsymbol{x}^{&#39;}, boldsymbol{x}) kappa( boldsymbol{x}, boldsymbol{x^{&#39;}}) &amp; geq 0 end{align} $$ There is a multitude of different kernels like: $$ begin{align} kappa( boldsymbol{x}, boldsymbol{x^{&#39;}}) &amp;= boldsymbol{x}^ top boldsymbol{x}^{&#39;} &amp;&amp; text{linear kernel} kappa( boldsymbol{x}, boldsymbol{x^{&#39;}}) &amp;= left( boldsymbol{x}^ top boldsymbol{x^{&#39;}} + r right)^d &amp;&amp; text{polynomial kernel} kappa( boldsymbol{x}, boldsymbol{x^{&#39;}}) &amp;= exp left(- frac{|| boldsymbol{x} - boldsymbol{x^{&#39;}} ||^2}{2 sigma^{2}} right) &amp;&amp; text{kernel RBF} end{align} $$ where $r$ is the coefficient of the polynomial and $d$ its degree, while $ sigma$ is known as the bandwith of the radial basis function (RBF). . Hard Margin Classifier . Support vector machines were originally designed for binary classification but can be extended to regression and multi-class classification. SVMs are not probabilistic models, their output is a raw prediction. We will only discuss how SVMs work for classification. Let&#39;s get the basic intuition with the help of the following figure. . . SVMs separate the data in two classes using a hyperplane. The distance between the dotted hyperplanes and the hyperplane at the center is known as margin. A margin is passing by one or multiple points, these points are known as support vectors. Any point above the red hyperplane will be classified as $1$ respectively $-1$ for the points below the hyperplane. In order to predict on an input sample $ boldsymbol{x}$, we compute the linear equation: $$ begin{align} f( boldsymbol{x}) &amp;= boldsymbol{w}^ top boldsymbol{x} + b hat{y} &amp;= begin{cases} ; ; ;1 &amp; text{if} quad f( boldsymbol{x}) geq 0 -1 &amp; text{otherwise} end{cases} end{align} $$ . SVMs try to maximize the margin when separating the data. This fact plays a role in the generalization of SVM models. We can find the value of the margin by solving the equation: $$ begin{align} boldsymbol{w}^ top ( boldsymbol{x}_2 - boldsymbol{x}_1) &amp;= 2 frac{ boldsymbol{w}^ top}{|| boldsymbol{w}||} ( boldsymbol{x}_2 - boldsymbol{x}_1) &amp;= frac{2}{|| boldsymbol{w}||} end{align} $$ . In other words, we need to solve the optimization problem known as primal: $$ begin{equation} min_{ boldsymbol{w}, b} frac{1}{2}|| boldsymbol{w}||^2 quad text{s.t} quad y_i( boldsymbol{w}^ top boldsymbol{x}_i + b) geq 1, quad forall i in {1, dots, N } end{equation} $$ . The reason we minimize $|| boldsymbol{w}||^2$ instead of $|| boldsymbol{w}||$ is that minimizing the second option is non-convex. The condition ensures that all samples are on the right side of the hyperplane. Note that this classifier does not allow any misclassification and is known as hard margin classifier. . Primal form to Dual form . The primal form is can be transformed to the dual for which is more suited for implementation. We can use the generalized Lagrangian to get the dual form that has more feasible constrains: $$ begin{equation} min_{ boldsymbol{x}, b}L( boldsymbol{x}, b, boldsymbol{ sigma}) = frac{1}{2}|| boldsymbol{w}||^2 - sum_{i=1}^{N} sigma_i y_i( boldsymbol{w}^ top phi( boldsymbol{x}_i) + b - 1) label{eq:lagrange} end{equation} $$ . We also need to satisfy the KKT conditions, such as: $$ begin{align} frac{ partial L}{ partial boldsymbol{w}} = boldsymbol{w} - sum_{i=1}^{N} sigma_i y_i phi( boldsymbol{x}_i) &amp;= 0 Rightarrow boldsymbol{w} = sum_{i=1}^{N} sigma_i y_i phi( boldsymbol{x}_i) frac{ partial L}{ partial boldsymbol{b}} = - sum_{i=1}^{N} sigma_i y_i &amp;= 0 Rightarrow sum_{i=1}^{N} sigma_i y_i = 0 end{align} $$ . Because the dual problem is a lower bound of the primal problem, we need to maximize the equation above: $$ begin{equation} max_{ boldsymbol{ sigma}} min_{ boldsymbol{x}, b}L( boldsymbol{x}, b, boldsymbol{ sigma}) sim min_{ boldsymbol{w}}f( boldsymbol{x}) end{equation} $$ . By substituting the formulas that we got while solving1 the KKT conditions in $L$ we get the final dual form: $$ begin{align} &amp; max_{ boldsymbol{ sigma}} : sum_{i=1}^{N} sigma_i - frac{1}{2} sum_{i=1}^{N} sum_{j=1}^{N} sigma_i sigma_j y_i y_j phi( boldsymbol{x}_i)^ top phi( boldsymbol{x}_j) &amp; text{s.t.} quad sigma_i geq 0 &amp;&amp; forall i in {1, dots, N } &amp; text{s.t.} quad sum_{i=1}^{N} sigma_iy_i = 0 &amp;&amp; forall i in {1, dots, N } end{align} $$ . Then, we transform the optimization problem into a minimization and introducting the Gram matrix $ boldsymbol{K}_{ij} = phi( boldsymbol{x}_i)^ top phi( boldsymbol{x}_j)$: $$ begin{align} &amp; min_{ boldsymbol{ sigma}; boldsymbol{ sigma} geq 0} frac{1}{2} sum_{i=1}^{N} sum_{j=1}^{N} sigma_i sigma_j y_i y_j boldsymbol{K}_{ij} - sum_{i=1}^{N} sigma_i &amp; text{s.t.} quad sigma_i geq 0 &amp;&amp; forall i in {1, dots, N } &amp; text{s.t.} quad sum_{i=1}^{N} sigma_iy_i = 0 &amp;&amp; forall i in {1, dots, N } end{align} $$ . If the data is not linearly separable, even after using the kernel trick, we can use a soft margin classifier. . Soft Margin Classifier . Soft-margin classifiers introduce slack variables $ zeta_i geq 0$ as $ zeta_i = |y_i - f_i|$ to allow the classifier to mis-classify some samples. . . In fact, we allow the model to make mistakes to be able to separate non linearly separable data where: . $ zeta_i = 0$ means that the point lies in the correct margin boundary. | $0 &lt; zeta_i leq 1$ the point lies inside the margin and on the correct side. | $ zeta_i &gt; 1$ the point lies in the incorrect boundary. | . The primal optimization is then: $$ begin{align} &amp; min_{ boldsymbol{w}, b} frac{1}{2}|| boldsymbol{w}||^2 - C sum_{i=1}^{N} zeta_i &amp; text{s.t} quad y_i( boldsymbol{w}^ top boldsymbol{x}_i + b) geq 1 - zeta_i, &amp;&amp; forall i in {1, dots, N } &amp; text{s.t} quad zeta_i geq 0 &amp;&amp; forall i in {1, dots, N } end{align} $$ where $C$ is the regularization parameter that controls the number of errors we are willing to tolerate on the training set. . Then the dual problem for the soft margin model is: $$ begin{align} &amp; max_{ boldsymbol{ sigma}} sum_{i=1}^{N} sigma_i - frac{1}{2} sum_{i=1}^{N} sum_{j=1}^{N} sigma_i sigma_j y_i y_j phi( boldsymbol{x}_i)^ top phi( boldsymbol{x}_j) &amp; text{s.t.} quad 0 leq sigma_i leq C &amp;&amp; forall i in {1, dots, N } &amp; text{s.t.} quad sum_{i=1}^{N} sigma_iy_i = 0 &amp;&amp; forall i in {1, dots, N } end{align} $$ . Implementation . We will compute the convex optimizations using the CVXOPT package. The solver performs the following optimization: $$ begin{align} min &amp; frac{1}{2} boldsymbol{ sigma}^ top boldsymbol{P} boldsymbol{ sigma} - 1^ top boldsymbol{ sigma} &amp; text{s.t.} &amp;- sigma_i leq 0 &amp; boldsymbol{y}^ top boldsymbol{ sigma} = 0 end{align} $$ . As you have probably guess, we will have to convert the dual problem in matrix form, let&#39;s define: $$ begin{align} &amp;P_{ij} = y_i y_j boldsymbol{K}_{ij} &amp; boldsymbol{w} = sum_{i=1}^{N} y_i sigma_i phi( boldsymbol{x}_i) &amp;b = y_s - sum_{m in S} sigma_m y_m phi( boldsymbol{x}_m)^ top phi( boldsymbol{x}_s) end{align} $$ where $S$ is the set of support vectors such as $ sigma_i &gt; 0$. We can solve the hard margin optimization problem by solving: $$ begin{align} min &amp; frac{1}{2} boldsymbol{x}^ top boldsymbol{P} boldsymbol{x} + boldsymbol{q}^ top boldsymbol{x} &amp; text{s.t.} &amp; boldsymbol{G} boldsymbol{x} leq h &amp; boldsymbol{A} boldsymbol{x} = 0 end{align} $$ where $h$ is a column of ones, $ boldsymbol{A}$ contains the labels and $ boldsymbol{G}$ is a diagonal matrix full of $-1$. . Here is the pseudo-algorithm for training an SVM: . Compute $ boldsymbol{P}$. | Compute $ boldsymbol{w}$. | Find the support vectors $S$ by finding any data point where $ sigma_i &gt; 0$. | Compute $b$. | To predict, compute: $$ y_{test} = text{sign} left( boldsymbol{w}^ top phi({ boldsymbol{x}_{test}}) + b right) $$ | . In matrix form the dual optimization is: $$ begin{align} min &amp; frac{1}{2} boldsymbol{ sigma}^ top boldsymbol{P} boldsymbol{ sigma} - 1^ top boldsymbol{ sigma} &amp; text{s.t.} &amp;- sigma_i leq 0 &amp; boldsymbol{y}^ top boldsymbol{ sigma} = 0 end{align} $$ . For training a soft margin classifier, we add a constraint on $C$: $$ begin{align} min &amp; frac{1}{2} boldsymbol{ sigma}^ top boldsymbol{P} boldsymbol{ sigma} - 1^ top boldsymbol{ sigma} &amp; text{s.t.} &amp;- sigma_i leq 0 &amp; sigma_i leq C &amp; boldsymbol{y}^ top boldsymbol{ sigma} = 0 end{align} $$ . def linear_kernel(X_1, X_2): &quot;&quot;&quot;Computes a dot product between two vectors&quot;&quot;&quot; return np.dot(X_1, X_2) . class SupportVectorClassifier(): &quot;&quot;&quot;Class for both the hard margin and soft margin classifiers Attributes: kernel: A function for the kernel function to apply. C: If using soft margin, it is a float controling the tolerance to misclassification. sv_sigma: A torch tensor for the slack variables of the support vectors. sv: A torch tensor for the features of the support vectors. sv_y: A torch tensor for the labels of the support vectors. w: A torch tensor for the weights of the model, only used if the kernel is linear. b: A float for the bias. &quot;&quot;&quot; def __init__(self, kernel, C=None): &quot;&quot;&quot;Init function of the SVC&quot;&quot;&quot; self.kernel = kernel self.C = C def fit(self, X, y): &quot;&quot;&quot;Trains the SVC model Args: X: A torch tensor for the data. y: A torch tensor for the labels. &quot;&quot;&quot; # Building the cvxopt matrices if len(y.shape) == 1: y = np.expand_dims(y, axis=1) y = y.astype(float) n_samples, n_features = X.shape K = np.zeros((n_samples, n_samples)) for i in range(n_samples): for j in range(n_samples): K[i, j] = self.kernel(X[i, :], X[j, :]) q = cvxmat(np.ones((n_samples, 1)) * -1.) A = cvxmat(y.T) b = cvxmat(0.) P = cvxmat(np.outer(y, y) * K) if self.C is None: G = cvxmat(np.eye(n_samples) * -1.) h = cvxmat(np.zeros((n_samples, 1))) else: col_1 = np.eye(n_samples) * -1. col_2 = np.identity(n_samples) G = cvxmat(np.vstack((col_1, col_2))) col_1 = np.zeros(n_samples) col_2 = np.ones(n_samples) * self.C h = cvxmat(np.hstack((col_1, col_2))) # Compute the convex optimization sigma = np.array(solver.qp(P, q, G, h, A, b)[&#39;x&#39;]) # Gather support vectors sv_idx = sigma &gt; 1e-5 self.sv_sigma = np.expand_dims(sigma[sv_idx], axis=1) sv_idx = np.squeeze(sv_idx) self.sv = X[sv_idx, :] self.sv_y = y[sv_idx] # If linear kernel, compute w, otherwise calculate it in pairwise # fashion once we predict if self.kernel == linear_kernel: w = np.sum(self.sv_y * self.sv_sigma * self.sv, axis=0) self.w = np.expand_dims(w, axis=1) else: self.w = None # Gather all pairwise distance of the support vectors idx = sv_idx.astype(int).nonzero()[0] b = 0 sv_sigma_flat = self.sv_sigma.flatten() sv_y_flat = self.sv_y.flatten() for i in range(len(idx)): b += self.sv_y[i] b -= np.sum(sv_sigma_flat * sv_y_flat * K[idx[i], sv_idx]) b /= len(idx) self.b = b def project(self, X): &quot;&quot;&quot;Projects the data into the SVC hyperspace&quot;&quot;&quot; if self.w is not None: return np.dot(X, self.w) + self.b else: pred = np.zeros(X.shape[0]) for i in range(X.shape[0]): cur_pred = 0 for j in range(self.sv_sigma.shape[0]): cur_pred += self.sv_sigma[j] * self.sv_y[j] * self.kernel(X[i], self.sv[j]) pred[i] = cur_pred return pred + self.b def predict(self, X): &quot;&quot;&quot;Projects the data than return the sign of the prediction as label&quot;&quot;&quot; return np.sign(self.project(X)) . Hard Margin Classifier with Linear Kernel . svc = SupportVectorClassifier(linear_kernel) svc.fit(X, y) . pcost dcost gap pres dres 0: -1.6591e+02 -3.9116e+02 2e+02 4e-14 2e+00 1: -3.5588e+02 -3.6335e+02 7e+00 5e-15 1e+00 2: -1.7374e+04 -1.7380e+04 6e+00 7e-12 1e+00 3: -5.0598e+07 -5.0598e+07 2e+02 1e-08 1e+00 4: -5.4654e+10 -5.4655e+10 2e+05 2e-05 1e+00 Terminated (singular KKT matrix). . pred = svc.predict(X) . def plot_svc(X, y, svc, pred, plot_sv=True): &quot;&quot;&quot;Plots the SVC hyperplane&quot;&quot;&quot; plt.scatter(X[:, 0], X[:, 1], c=y) if plot_sv: plt.scatter(svc.sv[:, 0], svc.sv[:, 1]) axis_min, axis_max = plt.gca().get_xlim() w = svc.w a = -w[0] / w[1] xx = np.linspace(axis_min, axis_max) yy = a * xx - svc.b[0] / w[1] # Plot the hyperplane b = svc.sv[np.where(svc.sv_y == 1.)[0]][0] yy_down = a * xx + (b[1] - a * b[0]) b = svc.sv[np.where(svc.sv_y == -1.)[0]][0] yy_up = a * xx + (b[1] - a * b[0]) plt.plot(xx, yy, &#39;k-&#39;) plt.plot(xx, yy_down, &#39;k--&#39;) plt.plot(xx, yy_up, &#39;k--&#39;) . plot_svc(X, y, svc, pred) . The SVC separates very well the data, the support vectors appear in blue. . Soft Margin Classifier with Linear Kernel . X, y = generate_data(n_samples=200, n_features=2, centers=2, center_box=(-5, 5), cluster_std=2.5) plt.scatter(X[:, 0], X[:, 1], c=y) . &lt;matplotlib.collections.PathCollection at 0x16c4dfb1148&gt; . This time we generated non separable data. . svc_soft = SupportVectorClassifier(linear_kernel, C=0.1) . pcost dcost gap pres dres 0: -4.2577e+01 -4.1240e+01 1e+03 3e+01 2e-15 1: -7.5598e+00 -3.8621e+01 9e+01 1e+00 1e-15 2: -5.1212e+00 -1.8202e+01 1e+01 3e-02 1e-15 3: -6.0958e+00 -8.1423e+00 2e+00 4e-03 7e-16 4: -6.7688e+00 -7.1911e+00 4e-01 5e-04 5e-16 5: -6.8928e+00 -7.0247e+00 1e-01 1e-04 5e-16 6: -6.9357e+00 -6.9653e+00 3e-02 2e-05 6e-16 7: -6.9460e+00 -6.9522e+00 6e-03 3e-06 5e-16 8: -6.9488e+00 -6.9489e+00 9e-05 4e-08 6e-16 9: -6.9489e+00 -6.9489e+00 9e-07 4e-10 6e-16 Optimal solution found. . pred = svc_soft.predict(X) . plot_svc(X, y, svc_soft, pred, plot_sv=False) . The soft margin classifier places the classification plane in the middle of the two cluster in the direction of highest variance. Well done ! . Hard Margin Classifier with Kernel RBF . def generate_non_linear(mean_1, mean_2, mean_3, mean_4, cov): &quot;&quot;&quot;Generates non linear data from gaussian distributions Mean 1 to 4 is are the means that generate 4 clusters and cov is their covariance matrix. &quot;&quot;&quot; X_1 = np.random.multivariate_normal(mean_1, cov, 50) X_1 = np.vstack((X_1, np.random.multivariate_normal(mean_3, cov, 50))) y_1 = np.ones(len(X_1)) X_2 = np.random.multivariate_normal(mean_2, cov, 50) X_2 = np.vstack((X_2, np.random.multivariate_normal(mean_4, cov, 50))) y_2 = np.ones(len(X_2)) * -1 X = np.vstack([X_1, X_2]) y = np.hstack([y_1, y_2]) X = StandardScaler().fit_transform(X) return X, y . X, y = generate_non_linear(mean_1=[-2, 2], mean_2=[1, -1], mean_3=[4, -4], mean_4=[-4, 4], cov=[[1.0,0.8], [0.8, 1.0]]) . plt.scatter(X[:, 0], X[:, 1], c=y) . &lt;matplotlib.collections.PathCollection at 0x16c4c9caf08&gt; . Even the linear soft margin model cannot fit well this data. For solving this problem we will use the kernel radial basis function (RBF). . def polynomial_kernel(X_1, X_2, p): &quot;&quot;&quot;Computes the polynomial kernel where p is the degree of the polynomial&quot;&quot;&quot; return (1 + np.dot(X_1, X_2)) ** p def kernel_rbf(X_1, X_2, sigma=1.0): &quot;&quot;&quot;Computes the kernel RBF where sigma is the bandwith&quot;&quot;&quot; return np.exp(-np.linalg.norm(X_1 - X_2) ** 2 / (2 * sigma ** 2)) . svc_rbf = SupportVectorClassifier(kernel_rbf) svc_rbf.fit(X, y) . pcost dcost gap pres dres 0: -1.5702e+01 -4.7310e+01 5e+02 2e+01 2e+00 1: -7.4712e+00 -5.0258e+01 9e+01 3e+00 2e-01 2: -1.0976e+01 -4.0857e+01 5e+01 1e+00 9e-02 3: -1.9295e+01 -4.0147e+01 2e+01 1e-01 1e-02 4: -2.7419e+01 -3.4022e+01 7e+00 1e-02 1e-03 5: -3.0658e+01 -3.2568e+01 2e+00 2e-03 2e-04 6: -3.1971e+01 -3.2192e+01 2e-01 2e-04 1e-05 7: -3.2154e+01 -3.2158e+01 4e-03 2e-06 2e-07 8: -3.2157e+01 -3.2157e+01 5e-05 2e-08 2e-09 9: -3.2157e+01 -3.2157e+01 5e-07 2e-10 2e-11 Optimal solution found. . pred = svc_rbf.predict(X) . def plot_svc_rbf(X, y, svc): &quot;&quot;&quot;Plots the hyperplane and margins of a SVC with rbf kernel&quot;&quot;&quot; plt.scatter(X[:, 0], X[:, 1], c=y) sv = svc.sv plt.scatter(sv[:, 0], sv[:, 1], c=&#39;g&#39;) axis_min, axis_max = plt.gca().get_xlim() X1, X2 = np.meshgrid(np.linspace(axis_min, axis_max, 50), np.linspace(axis_min, axis_max,50)) X = np.array([[x1, x2] for x1, x2 in zip(np.ravel(X1), np.ravel(X2))]) Z = svc.project(X).reshape(X1.shape) plt.contour(X1, X2, Z, [0.0], colors=&#39;k&#39;, linewidths=1, origin=&#39;lower&#39;) plt.contour(X1, X2, Z + 1, [0.0], colors=&#39;grey&#39;, linewidths=1, origin=&#39;lower&#39;) plt.contour(X1, X2, Z - 1, [0.0], colors=&#39;grey&#39;, linewidths=1, origin=&#39;lower&#39;) plt.axis(&quot;tight&quot;) plt.show() . plot_svc_rbf(X, y, svc_rbf) . Conclusion . Support Vector Machines have a variety of kernels to fit the most complex data. SVMs work really well when there is a clear margin of separation between classes and are very effective on data with high dimensionality. Unfortunately, SVMs are not suited for large scale datasets due to their computational complexity. Furthermore, there is no probabilistic explanation of their prediction and they do not perform well on very noisy data. .",
            "url": "https://consciousml.github.io/blog/svm/kernel-trick/pytorch/eda/from-scratch/2020/09/25/Support-Vector-Machines.html",
            "relUrl": "/svm/kernel-trick/pytorch/eda/from-scratch/2020/09/25/Support-Vector-Machines.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Clustering Methods from scratch",
            "content": ". During this experiment, we will implement the K-means clustering and gaussian mixture model algorithms from scratch using Pytorch. The data used for training the unsupervised models was generated to show the distinction between K-means and Gaussian Mixture. . import os import math import torch import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import StandardScaler . def data_generator(cluster_nums, cluster_means, cluster_var, background_range, background_noise_nums): &quot;&quot;&quot;Generates data using a mutivariate normal distribution Args: cluster_nums: A positive integer for number of clusters. cluster_means: A list or numpy array describing the mean of each cluster. cluster_var: A list or numpy array describing the variance of each cluster. background_range: A tuple containing two float for the range of the background noise. background_noise_nums: Number of background noise points. Returns: A 2d numpy array. &quot;&quot;&quot; data = [] for num, mu, var in zip(cluster_nums, cluster_means, cluster_var): data += [np.random.multivariate_normal(mu, np.diag(var), num)] data = np.vstack(data) noise = np.random.uniform(background_range[0], background_range[1], size=(background_noise_nums, data.shape[-1])) data = np.append(data, noise, axis=0) return data . X = data_generator(cluster_nums=[400,600,800], cluster_means=[[0.5, 0.5], [6, 1.5], [1, 7]], cluster_var=[[1, 3], [2, 2], [6, 2]], background_range=[[-10, -15], [15, 20]], background_noise_nums=30) . plt.scatter(X[:, 0], X[:, 1]) plt.show() . Our goal is to separate the data into groups where the samples in each group are similar. . Standardize the data . The K-means method is a distance based clustering approach. It is mandatory to standardize the data to prevent the features with higher values to contribute more than the others. In a real world application, we would have standardize the data performing the following lines. . X = StandardScaler().fit_transform(X) X = torch.from_numpy(X).float() . K-Means Clustering . K-means algorithm is an iterative approach that tries to partition a dataset into $K$ predefined clusters where each data point belongs to only one cluster. . This algorithm works that way: . specify number of clusters $K$ | randomly initialize one centroid in space for each cluster | for each point, compute the euclidean distance between the point and each centroid and assign the point to the closest centroid | change the centroids values based on the points present in each cluster and repeat the previous step until the centroids do not change anymore | . The approach, K-means follows to solve this problem is called Expectation-Maximization. The E-step assign each point to a cluster, and the M-step refines the values of the centroid based on the points inside each cluster. . More formally, the objective function to minimize is as follows: $$ J = sum_{i=1}^{m} sum_{k=1}^{K} mathbb{I}(z_i = k)||x_i - mu_k||_2^2 $$ where $z_i$ is the cluster assigned to $x_i$ and $ mu_k$ is the mean of the cluster $k$. . The E-step is defined as: $$ z_i^{*} = text{argmin}_{k} ||x_i - mu_k||_2^2 $$ . And the M-step is defined as: $$ mu_k = frac{1}{ sum_{i=1}^m mathbb{I}(z_i = k)} sum_{i=1}^m x_i mathbb{I}(z_i = k) $$ . In practice, we should run multiple K-means with different initialization of the centroids and keep the parameters that minimizes the objective function. Since K-means is a distance based algorithm, is it mandatory to standardize the data. . class KMeansClustering(): &quot;&quot;&quot;K-Means class implemented in Pytorch Attributes: n_clusters: A integer describing in how many clusters to separate the data. centroids: A torch tensor containing in each column the mean of the n&#39;th cluster. Usage: kms = KMeansClustering(n_clusters=3) pred = kms.fit_transform(X) &quot;&quot;&quot; def __init__(self, n_clusters=5): &quot;&quot;&quot;Inits KmeansClustering class setting the number of clusters&quot;&quot;&quot; self.n_clusters = n_clusters self.centroids = None def fit_transform(self, X, n_iter=20): &quot;&quot;&quot;Trains the KMeans and clusterize the input data Args: X: A torch tensor for the data to clusterize. n_iters: A integer describing the number of iterations of expectation maximization to perform. &quot;&quot;&quot; size = X.shape[0] # Find min and max values to generate a random centroid in this range xmax = X.max(dim=0)[0].unsqueeze(1) xmin = X.min(dim=0)[0].unsqueeze(1) dists = torch.zeros((size, self.n_clusters)) best_loss = 1e10 pred = None for _ in range(n_iter): centroids = (xmin - xmax) * torch.rand((X.shape[1], self.n_clusters)) + xmax old_loss = -1 while 1: for i in range(self.n_clusters): # E-step: assign each point to a cluster ctr = centroids[:, i].unsqueeze(1) dists[:, i] = (X - ctr.T).pow(2).sum(dim=1).sqrt() dists_min, labels = dists.min(dim=1) for i in range(self.n_clusters): # M-step: re-compute the centroids idx = torch.where(labels == i)[0] if len(idx) == 0: continue centroids[:, i] = X[idx].mean(dim=0) new_loss = dists_min.sum() # Loss: sum distance between points and centroid if old_loss == new_loss: break old_loss = new_loss if new_loss &lt; best_loss: best_loss = new_loss pred = labels return pred . X = torch.from_numpy(X).float() kms = KMeansClustering(n_clusters=3) pred = kms.fit_transform(X) plt.scatter(X[:, 0], X[:, 1], c=pred) . &lt;matplotlib.collections.PathCollection at 0x22813c02548&gt; . The result is pretty satisfying. Each cluster is well separated from each other. Let&#39;s see if we can get a better result using a Gaussian Mixture model. . Gaussian Mixture Model . In order to understand how we train a Gaussian Mixture model, we must explain the expectation-maximization algorithm (EM) first. The EM algorithm is a general technique for finding maximum likelihood solutions for probabilistic models having latent variables. EM is an iterative algorithm that starts from an initial estimate of the parameters of a probabilistic model $ boldsymbol{ theta}$ and then proceeds to iteratively update $ boldsymbol{ theta}$ until convergence. This algorithm consists to iteratively apply an expectation step and a maximization step. . Let&#39;s consider the EM algorithm for a multivariate Gaussian Mixture model with $K$ mixture components, observed variables $ boldsymbol{X}$ and latent variables $ boldsymbol{Z}$ such as: $$ begin{aligned} P( boldsymbol{x}_i | boldsymbol{ theta}) &amp;= sum_{k=1}^{K} pi_k P( boldsymbol{x}_i | boldsymbol{z}_i, boldsymbol{ theta}_k) &amp;= sum_{k=1}^{K} P( boldsymbol{z}_i = k) mathcal{N}( boldsymbol{x}_i; boldsymbol{ mu}_k, boldsymbol{ Sigma}_k) end{aligned} $$ where each component of the mixture model is the normal distribution. Let&#39;s see why the MLE of a gaussian mixture model is hard to compute. The likelihood of a gaussian mixture distribution is: $$ L( boldsymbol{ theta} | boldsymbol{X}) = prod_{i=1}^N sum_{k=1}^{K} pi_k mathcal{N}( boldsymbol{x}_i; boldsymbol{ mu}_k, boldsymbol{ Sigma}_k) $$ . This the log-likelihood is as follows: $$ mathcal{L}( boldsymbol{ theta} | boldsymbol{X}) = sum_{i=1}^N log left( sum_{k=1}^{K} pi_k mathcal{N}( boldsymbol{x}_i; boldsymbol{ mu}_k, boldsymbol{ Sigma}_k) right) $$ . Because of the sum inside the $ log$, we cannot estimate $ boldsymbol{ mu}_k$ and $ boldsymbol{ sigma^2}$ without knowing $ boldsymbol{Z}$. That is why we would use EM in this kind of situation. . The EM algorithm has two steps, the expectation step known as E-step, assigns to each data point the probability that they belong to each components of the mixture model. Whereas the maximization step, known as M-step, re-evaluate the parameters of each mixture component based on the estimated values generated in the E-step. . More formally, during the E-step, we calculate the likelihood of each data point using the estimated parameters: $$ f( boldsymbol{x_i}| mu_k, boldsymbol{ Sigma}_k) = frac{1}{ sqrt{(2 pi)^m| boldsymbol{ Sigma}_k|}} exp left(- frac{( boldsymbol{x}_i - boldsymbol{ mu}_k)^ top boldsymbol{ Sigma}_k^{-1}( boldsymbol{x}_i - boldsymbol{ mu}_k)}{2} right) $$ where $m$ is the number of features in the input data. . Then we compute the probability that $ boldsymbol{x}_i$ came from the $k^{th}$ gaussian distribution: $$ p_{ik} = frac{ pi_k f( boldsymbol{x_i}| mu_k, boldsymbol{ Sigma}_k)}{ sum_{j=1}^{K} pi_j f( boldsymbol{x_i}| mu_j, boldsymbol{ Sigma}_j)} $$ . For the M-step, we update the parameters of the mixture as follows: $$ begin{aligned} pi_k &amp;= frac{1}{N} sum_{i=1}^{N} p_{ik} boldsymbol{ mu}_k &amp;= frac{1}{ sum_{i=1}^{N} p_{ik}} sum_{i=1}^{N} p_{ik} boldsymbol{x_i} boldsymbol{ Sigma}_k &amp;= frac{1}{ sum_{i=1}^{N} p_{ik}} sum_{i=1}^{N} p_{ik} ( boldsymbol{x}_i - boldsymbol{ mu}_k) ( boldsymbol{x}_i - boldsymbol{ mu}_k)^ top end{aligned} $$ . class GaussianMixture(): &quot;&quot;&quot;Gaussian Mixture model class separating data in clusters The KMeans class is used to initialize the probabilities p_ik of each sample. Attributes: n_components: An integer for the number of gaussian distributions in the mixture. n_iter: A positive integer for the number of iterations to perform e-steps and m-steps. priors: A torch tensor containing the probabilities of each class. mu: A torch tensor for the mean of each gaussian. sigma: A torch tensor for the covariance of each gaussian. likelihoods: A torch tensor containing the likelihoods of each data in regards to each gaussian. probs: A torch tensor holding the probabilities for each data to belong to each gaussian. Usage: gmm = GaussianMixture(n_components=3, n_iter=3) pred = gmm.fit_predict(X) &quot;&quot;&quot; def __init__(self, n_components, n_iter): &quot;&quot;&quot;Inits the GaussianMixture class by setting n_components and n_iter&quot;&quot;&quot; self.n_components = n_components self.n_iter = n_iter def gaussian_likelihood(self, X, n): &quot;&quot;&quot;Returns the gaussian likelihood of X from the nth gaussian&quot;&quot;&quot; two_pi = torch.tensor(2 * math.pi, dtype=torch.float64) fact = 1 / torch.sqrt(torch.pow(two_pi, X.shape[0]) * torch.det(self.sigma[n])) X_minus_mu = X - self.mu[n].T sigma_inv = torch.inverse(self.sigma[n]) return fact * torch.exp(-0.5 * X_minus_mu.mm(sigma_inv).mm(X_minus_mu.T)) def e_step(self, X): &quot;&quot;&quot;Assigns to each data the probability to belong to each gaussian&quot;&quot;&quot; for j in range(self.n_components): for i in range(self.n_samples): self.likelihoods[i, j] = self.gaussian_likelihood(X[i], j) # Tensor to hold the probabilities that each data belongs to each gaussian prob_num = self.priors.T * self.likelihoods prob_den = prob_num.sum(dim=1).unsqueeze(1) self.probs = prob_num / prob_den def m_step(self, X): &quot;&quot;&quot;Re-computes the parameters of each gaussian component&quot;&quot;&quot; for j in range(self.n_components): probs_j = self.probs[:, j] # All probabilities from the j&#39;th guassian probs_j_sum = probs_j.sum() self.priors[j] = probs_j.sum() / self.n_samples probs_j_uns = probs_j.unsqueeze(1) # Recomputes the means of each gaussian based on the probabilities of # each data. self.mu[j] = (probs_j_uns * X).sum(dim=0).unsqueeze(1) self.mu[j] /= probs_j_sum # Recomputes the covariance matrix of each data X_minus_mu = X - self.mu[j].T self.sigma[j].fill_(0.) for i in range(self.n_samples): row = X_minus_mu[i, :].unsqueeze(1) self.sigma[j] += probs_j[i] * row.mm(row.T) self.sigma[j] /= probs_j_sum def fit_predict(self, X): &quot;&quot;&quot;Trains the models and returns the clusters&quot;&quot;&quot; self.n_samples = X.shape[0] self.n_features = X.shape[1] self.sigma = torch.zeros((self.n_components, self.n_features, self.n_features), dtype=torch.float64) self.mu = torch.zeros((self.n_components, self.n_features, 1), dtype=torch.float64) self.priors = torch.zeros((self.n_components, 1), dtype=torch.float64).fill_(1 / self.n_components) # Initialize the probabilities of each samples to belong # to each gaussian by a KMeans kms = KMeansClustering(n_clusters=self.n_components) pred = kms.fit_transform(X).unsqueeze(1) self.probs = torch.zeros(self.n_samples, self.n_components) self.probs.scatter_(1, pred, 1) # Set the covariance parameter of each components to the # covariance of the data X_m = X - X.mean() X_cov = X_m.T.mm(X_m) for j in range(self.n_components): self.sigma[j] = X_cov.clone() # Select a random sample as the mean of each gaussian self.mu[j] = X[torch.randint(0, self.n_samples, (1,))].T self.likelihoods = torch.zeros((self.n_samples, self.n_components), dtype=torch.float64) self.m_step(X) # Start with m_step to compute params of gaussians based on KMeans probabilities for _ in range(self.n_iter): self.e_step(X) self.m_step(X) return self.probs.argmax(dim=1) . gmm = GaussianMixture(n_components=3, n_iter=3) pred_gmm = gmm.fit_predict(X) . _, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)) ax1.scatter(X[:, 0], X[:, 1], c=pred) ax2.scatter(X[:, 0], X[:, 1], c=pred_gmm) ax1.set_title(&#39;KMeans clusters&#39;) ax2.set_title(&#39;GMM clusters&#39;) . Text(0.5, 1.0, &#39;GMM clusters&#39;) . Conclusion . The KMeans algorithm can only describe spherical clusters while GMM can express elliptic clusters. In this experiment, the GMM undoubtably found the gaussian distributions that we generated. Whereas KMeans did a great job to separate the data but still lacks expressivity for clustering elliptic clusters. . KMeans is significantly faster and simpler than other EM algorithm, but for data that has different sized and shaped clusters, GMM does a better job. However, GMM is known to be unstable. Indeed, the clusters can be a little bit different each time. . If we had to choose one assertion to take out from this experiment, it would be that clustering methods are very powerful techniques suited for finding latent or missing variables. .",
            "url": "https://consciousml.github.io/blog/kmeans/gmm/unsupervised/pytorch/eda/from-scratch/2020/09/25/Clustering-Methods.html",
            "relUrl": "/kmeans/gmm/unsupervised/pytorch/eda/from-scratch/2020/09/25/Clustering-Methods.html",
            "date": " • Sep 25, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Naive Bayes Classifier from scratch",
            "content": ". In this experiment, the Naive Bayes Classifier method will be implemented from scratch using PyTorch. We will train our model on the Student Alcohol Consumption dataset to try to predict if a student frequently drink alcohol or not. . The data contains the following attributes: . school - student&#39;s school (binary: &#39;GP&#39; - Gabriel Pereira or &#39;MS&#39; - Mousinho da Silveira) | sex - student&#39;s sex (binary: &#39;F&#39; - female or &#39;M&#39; - male) | age - student&#39;s age (numeric: from 15 to 22) | address - student&#39;s home address type (binary: &#39;U&#39; - urban or &#39;R&#39; - rural) | famsize - family size (binary: &#39;LE3&#39; - less or equal to 3 or &#39;GT3&#39; - greater than 3) | Pstatus - parent&#39;s cohabitation status (binary: &#39;T&#39; - living together or &#39;A&#39; - apart) | Medu - mother&#39;s education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education) | Fedu - father&#39;s education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education) | Mjob - mother&#39;s job (nominal: &#39;teacher&#39;, &#39;health&#39; care related, civil &#39;services&#39; (e.g. administrative or police), &#39;at_home&#39; or &#39;other&#39;) | Fjob - father&#39;s job (nominal: &#39;teacher&#39;, &#39;health&#39; care related, civil &#39;services&#39; (e.g. administrative or police), &#39;at_home&#39; or &#39;other&#39;) | reason - reason to choose this school (nominal: close to &#39;home&#39;, school &#39;reputation&#39;, &#39;course&#39; preference or &#39;other&#39;) | guardian - student&#39;s guardian (nominal: &#39;mother&#39;, &#39;father&#39; or &#39;other&#39;) | traveltime - home to school travel time (numeric: 1 - 1 hour) | studytime - weekly study time (numeric: 1 - 10 hours) | failures - number of past class failures (numeric: n if 1&lt;=n&lt;3, else 4) | schoolsup - extra educational support (binary: yes or no) | famsup - family educational support (binary: yes or no) | paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no) | activities - extra-curricular activities (binary: yes or no) | nursery - attended nursery school (binary: yes or no) | higher - wants to take higher education (binary: yes or no) | internet - Internet access at home (binary: yes or no) | romantic - with a romantic relationship (binary: yes or no) | famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent) | freetime - free time after school (numeric: from 1 - very low to 5 - very high) | goout - going out with friends (numeric: from 1 - very low to 5 - very high) | health - current health status (numeric: from 1 - very bad to 5 - very good) | absences - number of school absences (numeric: from 0 to 93) | G1 - first period grade (numeric: from 0 to 20) | G2 - second period grade (numeric: from 0 to 20) | G3 - final grade (numeric: from 0 to 20, output target) | Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high) | Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high) | . import os import math import torch import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from pprint import pprint from sklearn.base import BaseEstimator from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import cross_val_score . Read csv with Pandas . students_mat = pd.read_csv(os.path.join(&#39;data&#39;, &#39;student-mat.csv&#39;)) students_por = pd.read_csv(os.path.join(&#39;data&#39;, &#39;student-por.csv&#39;)) # Concatenating students data from math and portuguese class students = pd.concat([students_mat, students_por], axis=0) # Averaging three grades into one single grade students[&#39;grade&#39;] = (students[&#39;G1&#39;] + students[&#39;G1&#39;] + students[&#39;G3&#39;]) / 3 # Combining weekly and weekend alcohol consumption into a single attribute students[&#39;alc&#39;] = students[&#39;Walc&#39;] + students[&#39;Dalc&#39;] # Drop the combined columns students = students.drop(columns=[&#39;G1&#39;, &#39;G2&#39;, &#39;G3&#39;, &#39;school&#39;]) students.head(5) . sex age address famsize Pstatus Medu Fedu Mjob Fjob reason ... romantic famrel freetime goout Dalc Walc health absences grade alc . 0 F | 18 | U | GT3 | A | 4 | 4 | at_home | teacher | course | ... | no | 4 | 3 | 4 | 1 | 1 | 3 | 6 | 5.333333 | 2 | . 1 F | 17 | U | GT3 | T | 1 | 1 | at_home | other | course | ... | no | 5 | 3 | 3 | 1 | 1 | 3 | 4 | 5.333333 | 2 | . 2 F | 15 | U | LE3 | T | 1 | 1 | at_home | other | other | ... | no | 4 | 3 | 2 | 2 | 3 | 3 | 10 | 8.000000 | 5 | . 3 F | 15 | U | GT3 | T | 4 | 2 | health | services | home | ... | yes | 3 | 2 | 2 | 1 | 1 | 5 | 2 | 15.000000 | 2 | . 4 F | 16 | U | GT3 | T | 3 | 3 | other | other | home | ... | no | 4 | 3 | 2 | 1 | 2 | 5 | 4 | 7.333333 | 3 | . 5 rows × 31 columns . Transform string to categorical values . categorical_dict = {} for col in students.columns: # For each column of type object, use sklearn label encoder and add the mapping to a dictionary if students[col].dtype == &#39;object&#39;: le = LabelEncoder() students[col] = le.fit_transform(students[col]) categorical_dict[col] = dict(zip(le.classes_, le.transform(le.classes_))) . pprint(categorical_dict) . {&#39;Fjob&#39;: {&#39;at_home&#39;: 0, &#39;health&#39;: 1, &#39;other&#39;: 2, &#39;services&#39;: 3, &#39;teacher&#39;: 4}, &#39;Mjob&#39;: {&#39;at_home&#39;: 0, &#39;health&#39;: 1, &#39;other&#39;: 2, &#39;services&#39;: 3, &#39;teacher&#39;: 4}, &#39;Pstatus&#39;: {&#39;A&#39;: 0, &#39;T&#39;: 1}, &#39;activities&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;address&#39;: {&#39;R&#39;: 0, &#39;U&#39;: 1}, &#39;famsize&#39;: {&#39;GT3&#39;: 0, &#39;LE3&#39;: 1}, &#39;famsup&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;guardian&#39;: {&#39;father&#39;: 0, &#39;mother&#39;: 1, &#39;other&#39;: 2}, &#39;higher&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;internet&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;nursery&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;paid&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;reason&#39;: {&#39;course&#39;: 0, &#39;home&#39;: 1, &#39;other&#39;: 2, &#39;reputation&#39;: 3}, &#39;romantic&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;schoolsup&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;sex&#39;: {&#39;F&#39;: 0, &#39;M&#39;: 1}} . Here is the dictonary of the categorical labels . Correlation heatmap . plt.figure(figsize=(20, 15)) sns.heatmap(students.corr(), annot=True, square=True, fmt=&#39;0.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a6856d48&gt; . Just from the correlation heatmap, we can have an overview of the impact of alcohol consumption on students. Based on the linear correlation between the target and the features, the is a tendency for students consuming alcohol frequently to have more chance to: . have lower grades | have more absences | hang out more often | does not aim to achieve higher education | study less | . Among all theses cases, the attributes that are the most correlated with the target are the grades, the study time and if the student is a men. . Impact of alcohol consumption on students life . def plot_pie(data, column, ax): &quot;&quot;&quot;Plots a pie diagram Args: data: A pandas data frame for the data. columns: A list containing the columns we are interested in. ax: The plt ax from which to plot the pie. &quot;&quot;&quot; counts = data[column].value_counts() percent = counts / counts.sum() * 100 labels = counts.index ax.pie(x=percent, labels=labels, autopct=&#39;%1.0f%%&#39;) . _, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7)) plot_pie(students, column=&#39;Dalc&#39;, ax=ax1) plot_pie(students, column=&#39;Walc&#39;, ax=ax2) ax1.set_title(&#39;Weekday Alcohol Consumption&#39;) ax2.set_title(&#39;Weekend Alcohol Consumption&#39;) . Text(0.5, 1.0, &#39;Weekend Alcohol Consumption&#39;) . The alcohol consumption during workdays is relatively low compared to the weekend consumption. Most students prefer to stay sober during workdays. Let&#39;s see how those behaviors have an impact on students success and life. . _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4)) sns.boxplot(y=&#39;Dalc&#39;, x=&#39;studytime&#39;, orient=&#39;h&#39;, data=students, ax=ax1) sns.boxplot(y=&#39;Walc&#39;, x=&#39;studytime&#39;, orient=&#39;h&#39;, data=students, ax=ax2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a8863cc8&gt; . Students who does not drink alcohol during weekdays usually study more than those who do. But the amount of study hours of students who does not drink during weekend is much more than the ones who do. . sober_absences = students.loc[students[&#39;Dalc&#39;] &lt;= 2, &#39;absences&#39;] drunk_absences = students.loc[students[&#39;Dalc&#39;] &gt; 2, &#39;absences&#39;] _, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4)) ax1.set_xlim(-10, 35) ax2.set_xlim(-10, 35) ax1.set_ylim(0, 0.30) ax2.set_ylim(0, 0.30) ax1.set_title(&#39;Absences distribution of sober workdays students&#39;) ax2.set_title(&#39;Absences distribution of drunk workdays students&#39;) sns.distplot(sober_absences, ax=ax1) sns.distplot(drunk_absences, ax=ax2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a8de7b08&gt; . Students who drink two times or more a week have a tendency to be more absent in class. . _, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4)) sober_grades = students.loc[students[&#39;Dalc&#39;] == 1, &#39;grade&#39;] drunk_grades = students.loc[students[&#39;Dalc&#39;] &gt; 1, &#39;grade&#39;] severe_drunk_grades = students.loc[students[&#39;Dalc&#39;] == 5, &#39;grade&#39;] ax1.set_ylim(0, 0.18) ax2.set_ylim(0, 0.18) ax3.set_ylim(0, 0.18) ax1.set_xlim(0, 20) ax2.set_xlim(0, 20) ax3.set_xlim(0, 20) ax1.set_title(&#39;Grades distribution of sober weekdays students&#39;) ax2.set_title(&#39;Grades distribution of students consuming alcohol on weekdays&#39;) ax3.set_title(&#39;Grades distribution of students with high weekdays consumption&#39;) sns.distplot(sober_grades, ax=ax1) sns.distplot(drunk_grades, ax=ax2) sns.distplot(severe_drunk_grades, ax=ax3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a7d7e7c8&gt; . Students who drink even a little during workdays have lower grades than those who do not. The impact on grades is much more important for students with severe consumption. . _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4)) sober_grades = students.loc[students[&#39;Walc&#39;] == 1, &#39;grade&#39;] severe_drunk_grades = students.loc[students[&#39;Walc&#39;] == 5, &#39;grade&#39;] ax1.set_ylim(0, 0.18) ax2.set_ylim(0, 0.18) ax1.set_xlim(0, 20) ax2.set_xlim(0, 20) ax1.set_title(&#39;Grades distribution of students sober during the weekend&#39;) ax2.set_title(&#39;Grades distribution of students with severe consumption during weekends&#39;) sns.distplot(sober_grades, ax=ax1) sns.distplot(severe_drunk_grades, ax=ax2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a8e2e188&gt; . However, even heavy alcohol consumption during the weekend has little impact on student grades. . Converting alcohol consumption to a categorical label . The original label goes from 1 to 5 from no consumption to severe consumption. It makes more sense to try to predict the weekly consumption of students so we combined the two attributes by summing them. . The fourth values seems like a good threshold to create two categorical classes: . $[2, 3]$ little alcohol consumption | $[4, 5]$ moderate alcohol consumption | $[6, 10]$ severe alcohol consumption | . sns.countplot(students[&#39;alc&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a7f9ae88&gt; . # Converting weekly consumption to classes students.loc[students[&#39;alc&#39;] &lt;= 3, &#39;alc&#39;] = 0 # little students.loc[(students[&#39;alc&#39;] &gt; 3) &amp; (students[&#39;alc&#39;] &lt;= 5), &#39;alc&#39;] = 1 # moderate students.loc[students[&#39;alc&#39;] &gt; 5, &#39;alc&#39;] = 2 # severe # We need to distinguish categorical from numeric values to fit different distribution # when we will fit the model numeric_cols = [&#39;age&#39;, &#39;traveltime&#39;, &#39;studytime&#39;, &#39;failures&#39;, &#39;famrel&#39;, &#39;freetime&#39;, &#39;goout&#39;, &#39;health&#39;, &#39;absences&#39;, &#39;grade&#39;, &#39;alc&#39;] students = students.drop(columns=[&#39;Walc&#39;, &#39;Dalc&#39;]) is_categorical = [] for col in students.columns: if col in numeric_cols: is_categorical.append(0) else: is_categorical.append(1) # Convert data to torch tensor X = torch.from_numpy(students.iloc[:, :-1].values).float() y = torch.from_numpy(students.iloc[:, -1].values).float() . Naive Bayes Classifier . Considering a vector of discrete values $ boldsymbol{x} in {1, dots, K }^D$, where $K$ is the number of samples and $D$ the number of features. The naive bayes classifier assumes that the data is conditionally independant given the class label i.e $p( boldsymbol{x}|y=c)$. This assumption allows us to write the class conditional density as a product: $$ p( boldsymbol{x} | y = c, boldsymbol{ theta}) = prod_{i=1}^{K}p(x_i | y_i = c, boldsymbol{ theta}_{ic}) $$ The model is called “naive” since we do not expect the features to be independent, even conditional on the class label. . Assuming the bayes theorem: $$ P(A|B) = frac{P(A)P(B|A)}{P(B)} $$ To have an easier understanding of the relation between the training of a model and the bayes theorem, let&#39;s reformulate this equation in terms of class and samples: $$ P( text{class}| text{sample}) = frac{P( text{class})P( text{sample}| text{class})}{P( text{sample})} $$ Given a new sample, we want to predict its class. We will compute $P( text{sample}| text{class})P( text{sample})$ according to the training data. When predicting, we utilize the following approximation: $$ P(c_j|x_i) sim P(c_j)P(x_i|c_j) dots P(x_D|c_j) $$ . In other words, for each potential class, we multiply the probability of the class (prior) with the probability of finding each features of $x_i$ in each class $c_j$ (posterior). . For categorical or binay features, we group the training samples according to each class The form of the class-conditional density depends on the type of each feature. We give some possibilities below: . For real values, we can use the Gaussian distribution: $$ p( boldsymbol{x} | y = c, boldsymbol{ theta}) = prod_{i=1}^{D} mathcal{N}(x_i| mu_{ic}, sigma_{ic}^2) $$ | For binary values, we can use a Bernouilli distribution, where $ mu_{ic}$ is the probability that feature $i$ occurs in class $c$: $$ p( boldsymbol{x} | y = c, boldsymbol{ theta}) = prod_{i=1}^{D} text{Ber}(x_i | mu_{ic}) $$ | For categorical features, we can use a Multinouilli distribution, where $ boldsymbol{ mu}_{ic}$ is an histogram over the possible values for $x_i$ in class $c$: $$ p( boldsymbol{x} | y = c, boldsymbol{ theta}) = prod_{i=1}^{D} text{Cat}(x_i | boldsymbol{ mu}_{ic}) $$ | . These are the training steps: . group data according to the class label $c_i$ | compute the prior probability i.e $p(c_i)$ the proportion of samples inside each class $c_i$ of the whole training set | for each feature: if the feature is categorical, compute $p( boldsymbol{x_j} | c_i)$ for $j = 1, dots, D$ and $i = 1, dots, C$: for each possible values of this feature in the training samples of class $c_i$, compute the probability that this feature appears in class $c_i$ | . | if the feature is continuous, compute $p( boldsymbol{x_j} | c_i)$ for $j = 1, dots, D$ and $i = 1, dots, C$: compute the mean $ mu$ and standard deviation $ sigma$ of the training samples of class $c_i$ and fit a normal distribution $ mathcal{N}( mu, sigma^2)$ | . | . | . To predict on a new samples: . for each class $c_i$, compute $p(c_i | x)$ as: multiply the prior of each class $p(c_i)$ by: | for each features $k$: if categorical, multiply by the probabilities calculated earlier $p( boldsymbol{x_k} | c_i)$ where $x_k$ is the value of the input on feature $k$. | if continuous, multiply by $ mathcal{N}(x_k | mu, sigma^2)$ the likelihood of the gaussian distribution given the input $x_k$ | . | . | return the highest probability $p(c_i | x)$ of all classes | . class NaiveBayesClassifier(BaseEstimator): &quot;&quot;&quot;Class for the naive bayes classifier Inherits from sklearn BaseEstimator class to use cross validation. Attributes: offset: An integer to increment the conditional probabilities in order to smooth probabilities to avoid that a posterior probability be 0. is_categorical: A list containing 0 and 1 for indicating if a feature is categorical or numerical. nb_features: An integer for the numbers of feature of the data. nb_class: An integer for the number of classes in the labels. class_probs: A torch tensor for the proportion of each class. cond_probs: A torch tensor for the conditional probability of having a given value on a certain feature in the population of each class. &quot;&quot;&quot; def __init__(self, offset=1): &quot;&quot;&quot;Init function for the naive bayes class&quot;&quot;&quot; self.offset = offset def fit(self, X, y, **kwargs): &quot;&quot;&quot;Fits the model given data and labels as input Args: X: A torch tensor for the data. y: A torch tensor for the labels. &quot;&quot;&quot; # It is mandatory to pass a list describing if each feature is categorical or numerical if &#39;is_categorical&#39; not in kwargs: raise ValueError(&#39;must pass &#39;is_categorical &#39; to fit through **kwargs&#39;) self.is_categorical = kwargs[&#39;is_categorical&#39;] size = X.shape[0] self.nb_features = X.shape[1] y_uvals = y.unique() self.nb_class = len(y_uvals) # Probability of each class in the training set self.class_probs = y.int().bincount().float() / size features_maxvals = torch.zeros((self.nb_features,), dtype=torch.int32) for j in range(self.nb_features): features_maxvals[j] = X[:, j].max() # All the posterior probabilites cond_probs = [] for i in range(self.nb_class): cond_probs.append([]) # Group samples by class idx = torch.where(y == y_uvals[i])[0] elts = X[idx] size_class = elts.shape[0] for j in range(self.nb_features): cond_probs[i].append([]) if self.is_categorical[j]: # If categorical # For each features for k in range(features_maxvals[j] + 1): # Count the number of occurence of each value in this feature given the group class # Divided by the number of samples in the class p_x_k = (torch.where(elts[:, j] == k)[0].shape[0] + self.offset) / size_class # Append to posteriors probabilities cond_probs[i][j].append(p_x_k) else: # If numerical features_class = elts[:, j] # Compute mean and std mean = features_class.mean() std = (features_class - mean).pow(2).mean().sqrt() # Store these value to use them for the gaussian likelihood cond_probs[i][j] = [mean, std] self.cond_probs = cond_probs return 0 def gaussian_likelihood(self, X, mean, std): &quot;&quot;&quot;Computes the gaussian likelihood Args: X: A torch tensor for the data. mean: A float for the mean of the gaussian. std: A flot for the standard deviation of the gaussian. &quot;&quot;&quot; return (1 / (2 * math.pi * std.pow(2))) * torch.exp(-0.5 * ((X - mean) / std).pow(2)) def predict(self, X): &quot;&quot;&quot;Predicts labels given an input Args: X: A torch tensor containing a batch of data. &quot;&quot;&quot; if len(X.shape) == 1: X = X.unsqueeze(0) nb_samples = X.shape[0] pred_probs = torch.zeros((nb_samples, self.nb_class), dtype=torch.float32) for k in range(nb_samples): elt = X[k] for i in range(self.nb_class): # Set probability by the prior (class probability) pred_probs[k][i] = self.class_probs[i] prob_feature_per_class = self.cond_probs[i] for j in range(self.nb_features): if self.is_categorical[j]: # If categorical get the probability of drawing the value of the input on feature j # inside class i pred_probs[k][i] *= prob_feature_per_class[j][elt[j].int()] else: # If numerical, multiply by the gaussian likelihood with parameters # mean and std of the class i on feature j mean, std = prob_feature_per_class[j] pred_probs[k][i] *= self.gaussian_likelihood(elt[j], mean, std) # Get to highest probability among all classes return pred_probs.argmax(dim=1) . nbc = NaiveBayesClassifier() fit_params = {&#39;is_categorical&#39; : is_categorical} cross_val_score(nbc, X, y, cv=5, scoring=&#39;accuracy&#39;, fit_params=fit_params).mean() . 0.5689409274935591 . Conclusion . Even if the naive bayes model makes a strong assumption that the features are conditionaly independant given the class label, it achieved almost 57% accuracy on three output classes. This model does not perform as well as the more sophisticated models but it is very fast and suited as a baseline model for most classification tasks. On the other hand, naive bayes models can be descent predictors but they are considered as bad estimators i.e the output probabilities are not to be taken seriously. The naive bayes technique is performing better than logistic regression on small datasets, whereas it is the opposite for large datasets. .",
            "url": "https://consciousml.github.io/blog/naive-bayes/alcohol/pytorch/eda/from-scratch/2020/09/24/Naive-Bayes-Classifier.html",
            "relUrl": "/naive-bayes/alcohol/pytorch/eda/from-scratch/2020/09/24/Naive-Bayes-Classifier.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "K-Nearest Neighbors from scratch",
            "content": ". During this experiment, we will train a K-nearest neighbors model on physicochemical data to predict the quality of a red or white wines. The KNN will be implemented from scratch using Pytorch along with clear explanation of how the model works. Exploratory data analysis techniques are also discussed in the last section of the notebook. . import os import sys sys.path.append(&#39;..&#39;) import torch import numpy as np import pandas as pd import seaborn as sns import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder . Read csv using Pandas . wine_red = pd.read_csv(os.path.join(&#39;data&#39;, &#39;winequality-red.csv&#39;), sep=&#39;;&#39;) wine_white = pd.read_csv(os.path.join(&#39;data&#39;, &#39;winequality-white.csv&#39;), sep=&#39;;&#39;) . wine_red.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11.0 | 34.0 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . 1 7.8 | 0.88 | 0.00 | 2.6 | 0.098 | 25.0 | 67.0 | 0.9968 | 3.20 | 0.68 | 9.8 | 5 | . 2 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15.0 | 54.0 | 0.9970 | 3.26 | 0.65 | 9.8 | 5 | . 3 11.2 | 0.28 | 0.56 | 1.9 | 0.075 | 17.0 | 60.0 | 0.9980 | 3.16 | 0.58 | 9.8 | 6 | . 4 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11.0 | 34.0 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . wine_white.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.0 | 0.27 | 0.36 | 20.7 | 0.045 | 45.0 | 170.0 | 1.0010 | 3.00 | 0.45 | 8.8 | 6 | . 1 6.3 | 0.30 | 0.34 | 1.6 | 0.049 | 14.0 | 132.0 | 0.9940 | 3.30 | 0.49 | 9.5 | 6 | . 2 8.1 | 0.28 | 0.40 | 6.9 | 0.050 | 30.0 | 97.0 | 0.9951 | 3.26 | 0.44 | 10.1 | 6 | . 3 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . 4 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . Merge red and white data . Both dataframes have the same labels, so we can easily concatenate them. . wines = pd.concat([wine_white, wine_red]) . Visualize correlation heatmap . plt.figure(figsize=(16, 8)) wines_corr = wines.corr() sns.heatmap(wines_corr, annot=True, square=True, fmt=&#39;0.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2969e19ed08&gt; . The features most correlated with the quality are: . alcohol | density | volatile acidity | . Most of the features are very highly correlated to each other. We will try to select the best features to avoid redundancies using backward elimination further in the notebook. . Convert quality label to categorical values . sns.barplot(wines[&#39;quality&#39;], wines[&#39;alcohol&#39;], palette=&#39;hot&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2969ef0e448&gt; . Wines between 7 and 9 quality have a higher alcohol values, we will choose this range for the good wines. Thus, it looks appropriate to divide the quality label into low, medium and good wines. . tmp = wines[&#39;quality&#39;].values quality_type = np.empty(tmp.shape, dtype=&#39;U6&#39;) quality_type[tmp &lt; 5] = &#39;bad&#39; quality_type[(4 &lt; tmp) &amp; (tmp &lt; 7)] = &#39;medium&#39; quality_type[6 &lt; tmp] = &#39;good&#39; quality_type = pd.DataFrame(columns=[&#39;quality&#39;], data=quality_type) wines = wines.drop(&#39;quality&#39;, axis=1) wines = pd.concat([wines.reset_index(drop=True), quality_type.reset_index(drop=True)], axis=1) sns.countplot(wines.iloc[:, -1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2969e3b9f08&gt; . Isolate features from the target . Feature scaling is mandatory when using a K-nearest neighbors model. The KNN algorithm has a distance decision based approach to regress or classify an input. In fact, if some features have higher values than other, they will contribute more in the overall distance calculation thus biasing the outcome. . X = wines.iloc[:, :-1].to_numpy() y = wines.iloc[:, -1].to_numpy() . Convert category to integer . y = LabelEncoder().fit_transform(y) . Split data in train test . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Convert data to pytorch tensor X_train = torch.from_numpy(X_train) X_test = torch.from_numpy(X_test) y_train = torch.from_numpy(y_train) y_test = torch.from_numpy(y_test) . We will use Pytorch for this experiment . K-Nearest Neighbors . The K-nearest neighbors classifier looks at the $K^{th}$ closest points in the training set of an input $x$ counts how many members of each class are present and returns the empirical fraction as the estimate. $$p( mathbf{y} = c| boldsymbol{ mathbf{x}}, D, K) = frac{1}{K} sum_{i in N_K( boldsymbol{ mathbf{x}}, D)} mathbb{I}(y_i = c)$$ $$ mathbb{I}(e) = begin{cases} 1 &amp; text{if} ;e ; text{is true} 0 &amp; text{otherwise} end{cases} $$ where $c$ is the class and $N_K$ are the closest sample of $ boldsymbol{ text{x}}$ in $D$. . def knn(sample, X, y, k_neighbors=10): &quot;&quot;&quot;Instance of K-Nearest Neigbhors model Args: X: A torch tensor for the data. y: A torch tensor for the labels. k_neighbors: An integer for the number of nearest neighbors to consider. &quot;&quot;&quot; sample = sample.unsqueeze(1).T # Compute the distance with the train set dist = (X - sample).pow(2).sum(axis=1).sqrt() # Sort the distances _, indices = torch.sort(dist) y = y[indices] # Get the Kth most similar samples and return the predominant class return y[:k_neighbors].bincount().argmax().item() . def train_knn(X_train, X_test, y_train, y_test, k_neighbors=1): &quot;&quot;&quot;Trains a K-Nearest Neigbhors model Args: X_train: A torch tensor for the training data. X_test: A torch tensor for the test data. y_train: A torch tensor for the training labels. y_test: A torch tensor for the test labels. k_neighbors: An integer for the number of nearest neighbors to consider. &quot;&quot;&quot; # Allocate space for the prediction y_pred_test = np.zeros(y_test.shape, dtype=np.uint8) y_pred_train = np.zeros(y_train.shape, dtype=np.uint8) X_train_c = X_train.clone() # Predict on each sample of the train and test for i in range(X_test.shape[0]): y_pred_test[i] = knn(X_test[i], X_train, y_train, k_neighbors=k_neighbors) y_pred_test = torch.from_numpy(y_pred_test).float() return y_pred_test . pred_test = train_knn(X_train, X_test, y_train, y_test, k_neighbors=1) print(classification_report(y_test, pred_test)) # Save data for future visualization X_train_viz = X_train X_test_viz = X_test y_train_viz = y_train y_test_viz = y_test . precision recall f1-score support 0 0.39 0.17 0.24 53 1 0.65 0.67 0.66 252 2 0.88 0.90 0.89 995 accuracy 0.83 1300 macro avg 0.64 0.58 0.60 1300 weighted avg 0.82 0.83 0.82 1300 . The KNN achieves a good performance compared to its simplicity. . Exploratory Data Analysis . Let&#39;s try to discard some features based on p-value but to see if it improves the results. More on p-value here: https://www.statsdirect.com/help/basics/p_values.htm#:~:text=The%20P%20value%2C%20or%20calculated,the%20hypothesis%20is%20being%20tested. . columns = wines.columns[:-1] while len(columns) &gt; 0: pvalues = [] X_1 = wines[columns] X_1 = sm.add_constant(X_1) model = sm.OLS(y, X_1).fit() pvalues = pd.Series(model.pvalues[1:], index=columns) max_idx = np.argmax(pvalues) max_pval = pvalues[max_idx] if max_pval &gt; 0.10: columns = columns.drop(columns[max_idx]) print(&#39;Dropping column &#39; + columns[max_idx] + &#39;, pvalue is: &#39; + str(max_pval)) else: break . Dropping column residual sugar, pvalue is: 0.6594184624811075 Dropping column total sulfur dioxide, pvalue is: 0.40799376734207193 Dropping column total sulfur dioxide, pvalue is: 0.16949863758492337 . columns . Index([&#39;fixed acidity&#39;, &#39;volatile acidity&#39;, &#39;residual sugar&#39;, &#39;total sulfur dioxide&#39;, &#39;density&#39;, &#39;pH&#39;, &#39;sulphates&#39;, &#39;alcohol&#39;], dtype=&#39;object&#39;) . X = wines[columns].to_numpy() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) X_train = torch.from_numpy(X_train) X_test = torch.from_numpy(X_test) y_train = torch.from_numpy(y_train) y_test = torch.from_numpy(y_test) y_pred = train_knn(X_train, X_test, y_train, y_test, k_neighbors=1) print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.14 0.14 0.14 58 1 0.53 0.53 0.53 308 2 0.86 0.86 0.86 1259 accuracy 0.77 1625 macro avg 0.51 0.51 0.51 1625 weighted avg 0.77 0.77 0.77 1625 . Feature selection based on p-value made the performance worse. From 83% to 77%. We tested with a value $k in [1, 10]$ and the best result is given $k = 1$. . Feature Selection by hand . Let&#39;s try to select all the feature that has more than $0.10$ correlation with the target. . columns = [&#39;alcohol&#39;, &#39;density&#39;, &#39;chlorides&#39;, &#39;volatile acidity&#39;] X = wines[columns].to_numpy() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) X_train = torch.from_numpy(X_train) X_test = torch.from_numpy(X_test) y_train = torch.from_numpy(y_train) y_test = torch.from_numpy(y_test) y_pred = train_knn(X_train, X_test, y_train, y_test, k_neighbors=1) print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.19 0.17 0.18 58 1 0.51 0.51 0.51 308 2 0.85 0.86 0.85 1259 accuracy 0.77 1625 macro avg 0.52 0.51 0.51 1625 weighted avg 0.76 0.77 0.77 1625 . Same result as backward elimination. Sometimes all the features are relevant for a given model. . Conclusion . The K-nearest neighbors algorithm is a simple but rather effective approach in some contexts. The KNN method has no training step which is very handy when we have an increasing amount of data. In fact, it is not required to train the KNN model. However, KNN is not a model to pick when the data has a high dimentionality. Computing the neighbor distances accross a large number of dimension is not effective. This phenomena is also called the &quot;curse of dimensionality&quot;. The KNN model is also uneffective when dealing with outliers. .",
            "url": "https://consciousml.github.io/blog/knn/wine-quality/pytorch/eda/from-scratch/2020/09/23/K-Nearest-Neigbors.html",
            "relUrl": "/knn/wine-quality/pytorch/eda/from-scratch/2020/09/23/K-Nearest-Neigbors.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Polynomial Regression from scratch",
            "content": ". During this experiment, we will see how to implement polynomial regression from scratch using Pytorch. . import torch import numpy as np import matplotlib.pyplot as plt . Generate a polynomial distribution with random noise . # Function for creating a vector with value between [r1, r2] def randvec(r1, r2, shape): return (r1 - r2) * torch.rand(shape) + r2 # Defining the range of our distribution X = torch.tensor([i for i in range(-30, 30)]).float() # Creating random points from a gaussian with random noise y = randvec(-1e4, 1e4, X.shape) - (1/2) * X + 3 * X.pow(2) - (6/4) * X.pow(3) plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x23a8500ea88&gt; . Create the polynomial features . The formula of linear regression is as follow: $$ boldsymbol{ hat{y}} = boldsymbol{X} boldsymbol{w} $$ where $ boldsymbol{ hat{y}}$ is the target, $ boldsymbol{w}$ are the weights learned by the model and $ boldsymbol{X}$ is training data. Polynomial regression is still considered as a linear regression because there is only linear learning parameters: $$ boldsymbol{y} = boldsymbol{w}_0 + boldsymbol{X} boldsymbol{w}_1 + boldsymbol{X}^2 boldsymbol{w}_2 + dots + boldsymbol{X}^n boldsymbol{w}_n $$ As you have probably guessed, this equation is not linear. We use a trick to make it linear: . We gather all the $ boldsymbol{X}^2$ to $ boldsymbol{X}^n$ as new features that we created and we concatenate them to $ boldsymbol{X}$. | All the $ boldsymbol{w}_1$ to $ boldsymbol{w}_n$ are concatenated to $ boldsymbol{w}_0$. | . At the end, the polynomial regression has the same formula as the linear regression but with the aggregated arrays. . def create_features(X, degree=2, standardize=True): &quot;&quot;&quot;Creates the polynomial features Args: X: A torch tensor for the data. degree: A intege for the degree of the generated polynomial function. standardize: A boolean for scaling the data or not. &quot;&quot;&quot; if len(X.shape) == 1: X = X.unsqueeze(1) # Concatenate a column of ones to has the bias in X ones_col = torch.ones((X.shape[0], 1), dtype=torch.float32) X_d = torch.cat([ones_col, X], axis=1) for i in range(1, degree): X_pow = X.pow(i + 1) # If we use the gradient descent method, we need to # standardize the features to avoid exploding gradients if standardize: X_pow -= X_pow.mean() std = X_pow.std() if std != 0: X_pow /= std X_d = torch.cat([X_d, X_pow], axis=1) return X_d def predict(features, weights): return features.mm(weights) features = create_features(X, degree=3, standardize=False) y_true = y.unsqueeze(1) . Method 1: Normal equation . The first method is analytical and uses the normal equation. Training a linear model using least square regression is equivalent to minimize the mean squared error: . $$ begin{align} text{Mse}( hat{y}, y) &amp;= frac{1}{n} sum_{i=1}^{n}{|| hat{y}_i - y_i ||_{2}^{2}} text{Mse}( hat{y}, y) &amp;= frac{1}{n}|| boldsymbol{X} boldsymbol{w} - boldsymbol{y} ||_2^2 end{align} $$where $n$ is the number of samples, $ hat{y}$ is the predicted value of the model and $y$ is the true target. The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$. . Minimizing the $ text{Mse}$ can be achieved by solving the gradient of this equation equals to zero in regards to the weights $ boldsymbol{w}$: . $$ begin{align} nabla_{ boldsymbol{w}} text{Mse}( hat{y}, y) &amp;= 0 ( boldsymbol{X}^ top boldsymbol{X})^{-1} boldsymbol{X}^ top boldsymbol{y} &amp;= boldsymbol{w} end{align} $$For more information on how to find $ boldsymbol{w}$ please visit the section &quot;Linear Least Squares&quot; of this link. . def normal_equation(y_true, X): &quot;&quot;&quot;Computes the normal equation Args: y_true: A torch tensor for the labels. X: A torch tensor for the data. &quot;&quot;&quot; XTX_inv = (X.T.mm(X)).inverse() XTy = X.T.mm(y_true) weights = XTX_inv.mm(XTy) return weights weights = normal_equation(y_true, features) y_pred = predict(features, weights) plt.scatter(X, y) plt.plot(X, y_pred, c=&#39;red&#39;) . [&lt;matplotlib.lines.Line2D at 0x23a8689c448&gt;] . With the normal equation method, the polynomial regressor fits well the synthetic data. . Method 2: Gradient Descent . The Gradient descent method takes steps proportional to the negative of the gradient of a function at a given point, in order to iteratively minimize the objective function. The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of $f$ is the vector containing all of the partial derivatives, denoted $ nabla_{ boldsymbol{x}}f( boldsymbol{x})$. . The directional derivative in direction $ boldsymbol{u}$ (a unit vector) is the slope of the function $f$ in direction $ boldsymbol{u}$. In other words, the directional derivative is the derivative of the function $f( boldsymbol{x} + sigma boldsymbol{u})$ with respect to $ sigma$ close to 0. To minimize $f$, we would like to find the direction in which $f$ decreases the fastest. We can do this using the directional derivative: $$ begin{align} &amp; min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{ boldsymbol{u}^ top nabla_{ boldsymbol{x}} f( boldsymbol{x})} = &amp; min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{|| boldsymbol{u}||_2 || nabla_{ boldsymbol{x}}f( boldsymbol{x})||_2 cos theta} end{align} $$ ignoring factors that do not depend on $ boldsymbol{u}$, this simplifies to $ min_{u}{ cos theta}$. This is minimized when $ boldsymbol{u}$ points in the opposite direction as the gradient. Each step of the gradient descent method proposes a new points: $$ boldsymbol{x&#39;} = boldsymbol{x} - epsilon nabla_{ boldsymbol{x}}f( boldsymbol{x}) $$ where $ epsilon$ is the learning rate. In the context of polynomial regression, the gradient descent is as follow: $$ boldsymbol{w} = boldsymbol{w} - epsilon nabla_{ boldsymbol{w}} text{MSE} $$ where: $$ begin{align} nabla_{ boldsymbol{w}} text{MSE} &amp;= nabla_{ boldsymbol{w}} left( frac{1}{n}{|| boldsymbol{X} boldsymbol{w} - boldsymbol{y} ||_2^2} right) &amp;= frac{2}{N} boldsymbol{X}^ top( boldsymbol{X} boldsymbol{w} - boldsymbol{y}) end{align} $$ . def gradient_descent(X, y_true, lr=0.001, it=30000): &quot;&quot;&quot;Computes the gradient descent Args: X: A torch tensor for the data. y_true: A torch tensor for the labels. lr: A scalar for the learning rate. it: A scalar for the number of iteration or number of gradient descent steps. &quot;&quot;&quot; weights_gd = torch.ones((X.shape[1], 1)) n = X.shape[0] fact = 2 / n for _ in range(it): y_pred = predict(X, weights_gd) grad = fact * X.T.mm(y_pred - y_true) weights_gd -= lr * grad return weights_gd features = create_features(X, degree=3, standardize=True) weights_gd = gradient_descent(features, y_true) pred_gd = predict(features, weights_gd) . The mean squared error is even lower when using gradient descent. . plt.scatter(X, y) plt.plot(X, pred_gd, c=&#39;red&#39;) . [&lt;matplotlib.lines.Line2D at 0x23a86812c48&gt;] . Conclusion . The polynomial regression is an appropriate example to learn more about the concept of normal equation and gradient descent. This method work well with data that has polynomial shapes but we need to choose the right polynomial degree for a good bias/variance trade-off. However, the polynomial regression method has an important drawback. In fact, it is necessary to transform the data to a higher dimensional space which can be unfeasable if the data is very large. .",
            "url": "https://consciousml.github.io/blog/polynomial-regression/pytorch/gradient-descent/from-scratch/2020/09/22/Polynomial-Regression.html",
            "relUrl": "/polynomial-regression/pytorch/gradient-descent/from-scratch/2020/09/22/Polynomial-Regression.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Logistic Regression from scratch",
            "content": ". During this experiment, we will train logistic regression on diabetes data, from scratch using Pytorch. The Pima Indians Diabetes Database has been gathered by the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of this dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. This dataset contains the following features: . Pregnancies | Glucose | BloodPressure | SkinThickness | Insuline | BMI | DiabetesPedigreeFunction | Age | Outcome (has diabetes or not) | . import os import torch import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split . Read csv using Pandas . diabetes = pd.read_csv(os.path.join(&#39;data&#39;, &#39;diabetes.csv&#39;)) diabetes.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . diabetes.describe() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . count 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | . mean 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578 | 0.471876 | 33.240885 | 0.348958 | . std 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160 | 0.331329 | 11.760232 | 0.476951 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.078000 | 21.000000 | 0.000000 | . 25% 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000 | 0.243750 | 24.000000 | 0.000000 | . 50% 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000 | 0.372500 | 29.000000 | 0.000000 | . 75% 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000 | 0.626250 | 41.000000 | 1.000000 | . max 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 | 67.100000 | 2.420000 | 81.000000 | 1.000000 | . On this sample of data, the standard devitation of the columns looks reasonably high except for the DiabetesPedigree but it is acceptable because the mean is relatively low. A feature having low std is likely to provide close to no information to the model. Which is not the case here. . sns.countplot(x=&#39;Outcome&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x14ccf75a2c8&gt; . The target distribution is very unbalanced with two times more negative than positives. . Heatmap Correlation . plt.figure(figsize=(20, 5)) sns.heatmap(diabetes.corr(), annot=True, square=True, fmt=&#39;0.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x14ccf67da48&gt; . The glucose level, BMI, age and number of pregnancies are highly correlated with the outcome. Suprisingly, the insulin level is not very correlated with the outcome. Most likely because the insulin is correlated with the glucose and the glucose has 0.47 correlation with the target. . plt.figure(figsize=(15, 3)) sns.boxplot(x=&#39;Glucose&#39;, y=&#39;Outcome&#39;, orient=&#39;h&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x14ccf2508c8&gt; . For patient with diabetes, the glucose level is significantly higher. In other words, a patient with high glucose level is very likely to have diabetes. . plt.figure(figsize=(15, 3)) sns.boxplot(x=&#39;Pregnancies&#39;, y=&#39;Outcome&#39;, orient=&#39;h&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x14ccdeb66c8&gt; . Surprisingly, the number of pregnancies is correlated with diabetes. . Convert data to Torch tensors . X = diabetes.iloc[:, :-1].values y = torch.from_numpy(diabetes.iloc[:, -1].values).float().unsqueeze(1) # Standardize the data X = StandardScaler().fit_transform(X) X = torch.from_numpy(X).float() # Add column of ones for the bias ones_col = torch.ones((X.shape[0], 1), dtype=torch.float32) X = torch.cat([ones_col, X], axis=1) . Split data into train and test . X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25) . Train Logistic Regression . The prediction of a logistic model is as follow: $$ large hat{y} = sigma( boldsymbol{X} boldsymbol{w}) $$ Where $ sigma$ is the sigmoid or logit function: $$ large sigma( boldsymbol{x}) = frac{1}{1 + exp(-x)} $$ The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$ given as input to the logit function. The sigmoid function is used here because it squashes the input in the $[0, 1]$ range suitable for describing a Bernouilli distribution. Where the Bernouilli cases are: . the patient has diabetes with $p$ probability | the patient does not have diabetes with $1 - p$ probability | . It is important to note that the bias is included in $ boldsymbol{X}$ as a column of ones. . Training a classification model can be expressed as maximizing the likelihood of the observed data. In other words, we want the predicted probability of our model that a patient has diabetes to be as close as the true probability of the data. In practice, maximizing the likelihood is equivalent to minimize the negative log likelihood: $$ large L( boldsymbol{ theta}) = - frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) $$ Because we dealing with a binary target, it is appropriate to use the binary cross entropy: $$ large L( boldsymbol{ theta}) = - frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) + (1 - boldsymbol{y_i}) log(1 - hat{ boldsymbol{y}}_i) $$ . Gradient Descent for Logistic Regression . We will use Gradient Descent to train the logistic regression model. The Gradient descent method takes steps proportional to the negative of the gradient of a function at a given point, in order to iteratively minimize the objective function. The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of $f$ is the vector containing all of the partial derivatives, denoted $ nabla_{ boldsymbol{x}}f( boldsymbol{x})$. . The directional derivative in direction $ boldsymbol{u}$ (a unit vector) is the slope of the function $f$ in direction $ boldsymbol{u}$. In other words, the directional derivative is the derivative of the function $f( boldsymbol{x} + sigma boldsymbol{u})$ with respect to $ sigma$ close to 0. To minimize $f$, we would like to find the direction in which $f$ decreases the fastest. We can do this using the directional derivative: $$ large min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{ boldsymbol{u}^ top nabla_{ boldsymbol{x}} f( boldsymbol{x})} $$ $$ large = min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{|| boldsymbol{u}||_2 || nabla_{ boldsymbol{x}}f( boldsymbol{x})||_2 cos theta}$$ ignoring factors that do not depend on $ boldsymbol{u}$, this simplifies to $ min_{u}{ cos theta}$. This is minimized when $ boldsymbol{u}$ points in the opposite direction as the gradient. Each step of the gradient descent method proposes a new point: $$ large boldsymbol{x&#39;} = boldsymbol{x} - epsilon nabla_{ boldsymbol{x}}f( boldsymbol{x}) $$ where $ epsilon$ is the learning rate. In the context of logistic regression, the gradient descent is as follow: $$ large boldsymbol{w} = boldsymbol{w} - epsilon nabla_{ boldsymbol{w}}L( boldsymbol{ theta}) $$ where: $$ large nabla_{ boldsymbol{w}}L( boldsymbol{ theta}) = nabla_{ boldsymbol{w}} left(- frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) + (1 - boldsymbol{y_i}) log(1 - hat{ boldsymbol{y}}_i) right) $$ $$ large = frac{1}{N} boldsymbol{X}^ top( sigma( boldsymbol{X} boldsymbol{w}) - boldsymbol{y}) $$ Here is a nice explanation of how to find the gradient of the binary cross entropy: https://www.youtube.com/watch?v=hWLdFMccpTY . def sigmoid(x): &quot;&quot;&quot;Sigmoid function that squashes the input between 0 and 1&quot;&quot;&quot; return 1 / (1 + torch.exp(-x)) def predict(X, weights): &quot;&quot;&quot;Pedicts the class given the data and the weights Args: X: A torch tensor for the input data. weights: A torch tensor for the parameters calculated during the training of the Logistic regression. &quot;&quot;&quot; return sigmoid(X.mm(weights)) . def binary_cross_entropy(y_true, y_pred): &quot;&quot;&quot;Loss function for the training of the logistic regression We add an epsilon inside the log functions to avoid Nans. Args: y_true: A torch tensor for the labels of the data. y_pred: A torch tensor for the values predicted by the model. &quot;&quot;&quot; fact = 1 / y_true.shape[0] return -fact * (y_true * torch.log(y_pred + 1e-10) + (1 - y_true) * torch.log(1 - y_pred + 1e-10 )).sum() . def train_logit_reg(X, y_true, weights, lr=0.001, it=2000): &quot;&quot;&quot;Trains the logistic regression model Args: X: A torch tensor for the training data. y: A torch tensor for the labels of the data. weights: A torch tensor for the learning parameters of the model. lr: A scalar describing the learning rate for the gradient descent. it: A scalar for the number of steps in the gradient descent. &quot;&quot;&quot; for _ in range(it): y_pred = predict(X, weights) err = (y_pred - y_true) grad = X.T.mm(err) weights -= lr * grad bn_train = binary_cross_entropy(y_true, y_pred).item() return weights, bn_train . # Training the model weights = torch.ones((X.shape[1], 1), dtype=torch.float32) weights, bn_train = train_logit_reg(X_train, y_train, weights) y_pred = predict(X_test, weights) print(&#39;Binary cross-entropy on the train set:&#39;, bn_train) . Binary cross-entropy on the train set: 0.4595394730567932 . Binary cross entropy on test set . # Test the model bn_test = binary_cross_entropy(y_test, y_pred).item() print(&#39;Binary cross-entropy on the test set:&#39;, bn_test) . Binary cross-entropy on the test set: 0.5321948528289795 . Accuracy on test set . To compute the accuracy, we have to find the best threshold to convert our probability output in binary values. . def get_binary_pred(y_true, y_pred): &quot;&quot;&quot;Finds the best threshold based on the prediction and the labels Args: y_true: A torch tensor for the labels of the data. y_pred: A torch tensor for the values predicted by the model. &quot;&quot;&quot; y_pred_thr = y_pred.clone() accs = [] thrs = [] for thr in np.arange(0, 1, 0.01): y_pred_thr[y_pred &gt;= thr] = 1 y_pred_thr[y_pred &lt; thr] = 0 cur_acc = classification_report(y_test, y_pred_thr, output_dict=True)[&#39;accuracy&#39;] accs.append(cur_acc) thrs.append(thr) accs = torch.FloatTensor(accs) thrs = torch.FloatTensor(thrs) idx = accs.argmax() best_thr = thrs[idx].item() best_acc = accs[idx].item() y_pred[y_pred &gt;= best_thr] = 1 y_pred[y_pred &lt; best_thr] = 0 return y_pred . import sklearn y_pred = get_binary_pred(y_test, y_pred) print(classification_report(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) . precision recall f1-score support 0.0 0.79 0.90 0.84 123 1.0 0.76 0.57 0.65 69 accuracy 0.78 192 macro avg 0.78 0.73 0.75 192 weighted avg 0.78 0.78 0.77 192 [[111 12] [ 30 39]] . With a threshold of 0.66, we achieve an accuracy of 78% which is quite good for a linear model. . Polynomial Logistic Regression . In this section, we will add some polynomial features to the logistic regressor. It is the same principle as the logistic regression except that $ boldsymbol{X}$ is the concatenation of $ boldsymbol{X_1} dots boldsymbol{X_m}$ where $m$ is the degree of the polynomial function and $ boldsymbol{w}$ is the concatenation of $ boldsymbol{w_1} dots boldsymbol{w_m}$ such as: $$ large boldsymbol{y} = sigma( boldsymbol{w}_0 + boldsymbol{X} boldsymbol{w}_1 + boldsymbol{X}^2 boldsymbol{w}_2 + dots + boldsymbol{X}^m boldsymbol{w}_m) $$ This method is still linear because predicting $ hat{y} = sigma( boldsymbol{X} boldsymbol{w})$ is still linear in the parameters. . X = torch.from_numpy(diabetes.iloc[:, :-1].values).float() y = torch.from_numpy(diabetes.iloc[:, -1].values).float().unsqueeze(1) . def create_poly_features(X, degree=2): &quot;&quot;&quot;Creates the augmented features for the polynomial model This function concatenates the augmented data into a single torch tensor. Args: X: A torch tensor for the data. degree: A integer for the degree of the polynomial function that we model. &quot;&quot;&quot; if len(X.shape) == 1: X = X.unsqueeze(1) ones_col = torch.ones((X.shape[0], 1)) # Standardize the output to avoid exploding gradients X_cat = X.clone() X_cat = (X_cat - X_cat.mean()) / X_cat.std() X_cat = torch.cat([ones_col, X_cat], axis=1) for i in range(1, degree): X_p = X.pow(i + 1) X_p = torch.from_numpy(StandardScaler().fit_transform(X_p)).float() X_cat = torch.cat([X_cat, X_p], axis=1) return X_cat def create_weights(features): &quot;&quot;&quot;Creates a column of ones&quot;&quot;&quot; return torch.ones((features.shape[1], 1), dtype=torch.float32) . features = create_poly_features(X, degree=2) X_train, X_test, y_train, y_test = train_test_split(features, y, random_state=42, test_size=0.25) weights = create_weights(X_train) weights, bn_train = train_logit_reg(X_train, y_train, weights, it=10000) y_pred = predict(X_test, weights) print(&#39;Binary cross-entropy on the train set:&#39;, bn_train) . Binary cross-entropy on the train set: 0.44449102878570557 . binary_cross_entropy(y_test, y_pred).item() . 0.5331151485443115 . y_pred = get_binary_pred(y_test, y_pred) print(classification_report(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) . precision recall f1-score support 0.0 0.75 0.95 0.84 123 1.0 0.84 0.45 0.58 69 accuracy 0.77 192 macro avg 0.80 0.70 0.71 192 weighted avg 0.78 0.77 0.75 192 [[117 6] [ 38 31]] . Conclusion . The polynomial logistic regression model overfitted compared to the classic logistic regression model because it lost 1% accuracy on the test set. For some very highly correlated data, logistic regression without polynomial features has better performance than with polynomial features. Logistic regression is a very simple and interpretable model suited as a baseline in most classification problems. However, it does not perform well when the feature space is large. In fact, it is difficult to compute feature transformation (such as polynomials) when the data doesn&#39;t fit in ram. .",
            "url": "https://consciousml.github.io/blog/logistic-regression/polynomial-regression/diabetes/pytorch/from-scratch/2020/09/17/Logistic-Regression.html",
            "relUrl": "/logistic-regression/polynomial-regression/diabetes/pytorch/from-scratch/2020/09/17/Logistic-Regression.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Multivariate Linear Regression from scratch",
            "content": ". The CarDekho dataset contains information about used cars listed on the website of the same name. This dataset has 301 unique entities with the following features: . car name | year of release | selling price | present price | kilometers driven | fuel: such as petrol or diesel | transmission: such as manual or automatic | owner: how many times the car changed owner | . The goal of this experiment is to train a linear model to predict the selling price of a car. We will use the framework Pytorch for the matrix calculus. . import os import sys import torch import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split . Read data from csv using Pandas . import os import sys import torch import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split . df = pd.read_csv(os.path.join(&#39;data&#39;, &#39;car_data.csv&#39;)) df.head() . Car_Name Year Selling_Price Present_Price Kms_Driven Fuel_Type Seller_Type Transmission Owner . 0 ritz | 2014 | 3.35 | 5.59 | 27000 | Petrol | Dealer | Manual | 0 | . 1 sx4 | 2013 | 4.75 | 9.54 | 43000 | Diesel | Dealer | Manual | 0 | . 2 ciaz | 2017 | 7.25 | 9.85 | 6900 | Petrol | Dealer | Manual | 0 | . 3 wagon r | 2011 | 2.85 | 4.15 | 5200 | Petrol | Dealer | Manual | 0 | . 4 swift | 2014 | 4.60 | 6.87 | 42450 | Diesel | Dealer | Manual | 0 | . Convert categorical variable into indicator variables . f_continuous = df[[&#39;Year&#39;, &#39;Selling_Price&#39;, &#39;Present_Price&#39;, &#39;Kms_Driven&#39;, &#39;Owner&#39;]] f_categorical = pd.get_dummies(df[[&#39;Fuel_Type&#39;, &#39;Seller_Type&#39;, &#39;Transmission&#39;]]) df = pd.concat([f_continuous, f_categorical], axis=1) # Drop refundant features df.drop([&#39;Transmission_Automatic&#39;, &#39;Seller_Type_Dealer&#39;, &#39;Fuel_Type_CNG&#39;], axis=1, inplace=True) df.head() . Year Selling_Price Present_Price Kms_Driven Owner Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 0 2014 | 3.35 | 5.59 | 27000 | 0 | 0 | 1 | 0 | 1 | . 1 2013 | 4.75 | 9.54 | 43000 | 0 | 1 | 0 | 0 | 1 | . 2 2017 | 7.25 | 9.85 | 6900 | 0 | 0 | 1 | 0 | 1 | . 3 2011 | 2.85 | 4.15 | 5200 | 0 | 0 | 1 | 0 | 1 | . 4 2014 | 4.60 | 6.87 | 42450 | 0 | 1 | 0 | 0 | 1 | . Visualize histogram of all features . df.hist(bins=14, color=&#39;steelblue&#39;, edgecolor=&#39;black&#39;, linewidth=1.0, xlabelsize=8, ylabelsize=8, grid=False) plt.tight_layout(rect=(0, 0, 1.2, 1.2)) . Most cars on sales are consuming petrol instead of diesel, have had only one owner, are from 2012-present, are manual and have a selling price between 1000 and 10000 dollars. . Heatmap correlation . plt.figure(figsize=(16, 8)) sns.heatmap(df.corr(), square= True, annot=True, fmt=&#39;.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1f5bb762b08&gt; . Most of the variables are highly correlated. As expected, the present price variable is the most correlated with the target selling price. . Pairwise Plots . cols_viz = [&#39;Kms_Driven&#39;, &#39;Year&#39;, &#39;Selling_Price&#39;, &#39;Present_Price&#39;] pp = sns.pairplot(df[cols_viz], height=1.8, aspect=1.8, plot_kws=dict(edgecolor=&quot;k&quot;, linewidth=0.5), diag_kind=&quot;kde&quot;, diag_kws=dict(shade=True)) fig = pp.fig fig.subplots_adjust(top=0.93, wspace=0.3) t = fig.suptitle(&#39;Wine Attributes Pairwise Plots&#39;, fontsize=14) . Most of the features are highly correlated to each other. Some outliers are present but as there is very few, we will keep them in the training set. The year feature have a polynomial correlation with the selling price. A polynomial regression will most likely overperform a linear regression. . Make train test split . # Separate the target from the dataFrame Y = df[&#39;Selling_Price&#39;] X = df.drop(&#39;Selling_Price&#39;, axis=1) # Convert data to Pytorch tensor X_t = torch.from_numpy(X.to_numpy()).float() Y_t = torch.from_numpy(Y.to_numpy()).float().unsqueeze(1) X_train, X_test, Y_train, Y_test = train_test_split(X_t, Y_t, test_size=0.33, random_state=42) . Train a multivariate linear regression . Training a linear model using least square regression is equivalent to minimize the mean squared error: . $$ begin{align} large text{Mse}( boldsymbol{ hat{y}}, boldsymbol{y}) &amp;= frac{1}{n} sum_{i=1}^{n}{|| hat{y}_i - y_i ||_{2}^{2}} &amp;= frac{1}{n}|| boldsymbol{X} boldsymbol{w} - boldsymbol{y} ||_2^2 end{align} $$where $n$ is the number of samples, $ hat{y}$ is the predicted value of the model and $y$ is the true target. The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$. . Minimizing the $ text{Mse}$ can be achieved by solving the gradient of this equation equals to zero in regards to the weights $ boldsymbol{w}$: . $$ begin{align} large nabla_{ boldsymbol{w}} text{Mse} &amp;= 0 large ( boldsymbol{X}^ top boldsymbol{X})^{-1} boldsymbol{X}^ top boldsymbol{y} &amp;= boldsymbol{w} end{align} $$For more information on how to find $ boldsymbol{w}$ please visit the following link. . def add_ones_col(X): &quot;&quot;&quot;Add a column a one to the input torch tensor&quot;&quot;&quot; x_0 = torch.ones((X.shape[0],), dtype=torch.float32).unsqueeze(1) X = torch.cat([x_0, X], dim=1) return X def multi_linear_reg(X, y): &quot;&quot;&quot;Multivariate linear regression function Args: X: A torch tensor for the data. y: A torch tensor for the labels. &quot;&quot;&quot; X = add_ones_col(X) # Add a column of ones to X to agregate the bias to the input matrices Xt_X = X.T.mm(X) Xt_y = X.T.mm(y) Xt_X_inv = Xt_X.inverse() w = Xt_X_inv.mm(Xt_y) return w def prediction(X, w): &quot;&quot;&quot;Predicts a selling price for each input Args: X: A torch tensor for the data. w: A torch tensor for the weights of the linear regression mode. &quot;&quot;&quot; X = add_ones_col(X) return X.mm(w) . # Fit the training set into the model to get the weights w = multi_linear_reg(X_train, Y_train) # Predict using matrix multiplication with the weights Y_pred_train = prediction(X_train, w) Y_pred_test = prediction(X_test, w) . Compute prediction error . def mse(Y_true, Y_pred): error = Y_pred - Y_true return (error.T.mm(error) / Y_pred.shape[0]).item() def mae(Y_true, Y_pred): error = Y_pred - Y_true return error.abs().mean().item() . mse_train = mse(Y_train, Y_pred_train) mae_train = mae(Y_train, Y_pred_train) print(&#39;MSE Train: t&#39;, mse_train) print(&#39;MAE Train: t&#39;, mae_train, end=&#39; n n&#39;) mse_test = mse(Y_test, Y_pred_test) mae_test = mae(Y_test, Y_pred_test) print(&#39;MSE Test: t&#39;, mse_test) print(&#39;MAE Test: t&#39;, mae_test, end=&#39; n n&#39;) . MSE Train: 2.808985471725464 MAE Train: 1.1321566104888916 MSE Test: 3.7205495834350586 MAE Test: 1.2941011190414429 . The model has an error of 1.29 on average on the training test. Not bad for a linear model, taking into consideration that the mean of the present price is 7.62. . Principal component analysis visualization . In this section, we will use PCA to reduce the number of feature to two, in order to visualize the plane of the linear regressor. . Suppose a collection of $m$ points $ { boldsymbol{x}^{(1)}, dots, boldsymbol{x}^{(m)} }$ in $ mathbb{R}^n$. The principal components analysis aims to reduce the dimensionality of the points while losing the least precision as possible. For each point $ boldsymbol{x}^{(i)} in mathbb{R}^n$ we will find a corresponding code vector $ boldsymbol{c}^{(i)} in mathbb{R}^l$ where $l$ is smaller than $n$. Let $f$ be the encoding function and $g$ be the decoding function and $ boldsymbol{D} in mathbb{R}^{n,l}$ is the decoding matrix whose columns are orthonormal: $$ begin{align} f( boldsymbol{x}) &amp;= boldsymbol{D}^ top boldsymbol{x} g(f( boldsymbol{x})) &amp;= boldsymbol{D} boldsymbol{D}^ top boldsymbol{x} end{align} $$ . def cov(X): &quot;&quot;&quot;Computes the covariance of the input The covariance matrix gives some sense of how much two values are linearly related to each other, as well as the scale of these variables. It is computed by (1 / (N - 1)) * (X - E[X]).T (X - E[X]). Args: X: A torch tensor as input. &quot;&quot;&quot; X -= X.mean(dim=0, keepdim=True) fact = 1.0 / (X.shape[0] - 1) cov = fact * X.T.mm(X) return cov def pca(X, target_dim=2): &quot;&quot;&quot;Computes the n^th first principal components of the input PCA can be implemented using the n^th principal components of the covariance matrix. We could have been using an eigen decomposition because the covariance matrix is always squared but singular value decomposition does also the trick if we take the right singular vectors and perform a matrix multiplication to the right. Args: X: A torch tensor as the input. target_dim: An integer for selecting the n^th first components. &quot;&quot;&quot; cov_x = cov(X) U, S, V = torch.svd(cov_x) transform_mat = V[:, :target_dim] X_reduced = X.mm(transform_mat) return X_reduced, transform_mat . X_test_pca, _ = pca(X_test, target_dim=2) X_train_pca, _ = pca(X_train, target_dim=2) points = torch.cat([X_test_pca[:3], Y_pred_test[:3]], axis=1) v1 = points[2, :] - points[0, :] v2 = points[1, :] - points[0, :] cp = torch.cross(v1, v2) a, b, c = cp d = cp.dot(points[2, :]) min_mesh_x = min(X_test_pca[:, 0].min(), X_train_pca[:, 0].min()) max_mesh_x = max(X_test_pca[:, 0].max(), X_train_pca[:, 0].max()) min_mesh_y = min(X_test_pca[:, 1].min(), X_train_pca[:, 1].min()) max_mesh_y = max(X_test_pca[:, 1].max(), X_train_pca[:, 1].max()) mesh_x = np.linspace(min_mesh_x, max_mesh_x, 25) mesh_y = np.linspace(min_mesh_y, max_mesh_y, 25) mesh_xx, mesh_yy = np.meshgrid(mesh_x, mesh_y) mesh_zz = (d - a * mesh_xx - b * mesh_yy) / c . Here we recreate the prediction plane using three points of the prediction. More information at this link. . fig = plt.figure(figsize=(25,7)) ax1 = fig.add_subplot(131, projection=&#39;3d&#39;) ax2 = fig.add_subplot(132, projection=&#39;3d&#39;) ax3 = fig.add_subplot(133, projection=&#39;3d&#39;) axes = [ax1, ax2, ax3] for ax in axes: ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], Y_test, color=&#39;red&#39;, edgecolor=&#39;black&#39;) ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], Y_train, color=&#39;green&#39;, edgecolor=&#39;black&#39;) ax.scatter(mesh_xx.flatten(), mesh_yy.flatten(), mesh_zz.flatten(), facecolor=(0, 0, 0, 0), s=20, edgecolor=&#39;#70b3f0&#39;) ax.set_xlabel(&#39;1st component&#39;, fontsize=12) ax.set_ylabel(&#39;2nd component&#39;, fontsize=12) ax.set_zlabel(&#39;Selling Price&#39;, fontsize=12) ax.ticklabel_format(axis=&quot;x&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax.ticklabel_format(axis=&quot;y&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax.ticklabel_format(axis=&quot;z&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax1.view_init(elev=60, azim=50) ax2.view_init(elev=10, azim=0) ax3.view_init(elev=-15, azim=140) . The plane is fitting pretty well the data ! . Exploratory Data Analysis . We made an attempt to discard some features based on p-value but it didn&#39;t improve the results. More on p-value here. . columns = X.columns while len(columns) &gt; 0: pvalues = [] X_1 = X[columns] X_1 = sm.add_constant(X) # add a columns of ones for the bias model = sm.OLS(Y, X_1).fit() # fit a linear regression pvalues = pd.Series(model.pvalues[1:], index=columns) max_idx = np.argmax(pvalues) max_pval = pvalues[max_idx] if max_pval &gt; 0.05: # if the p_values is greater than 0.05, the feature has not enough # informational value for the training columns = columns.drop(columns[max_idx]) print(&#39;Dropping column &#39; + columns[max_idx] + &#39;, pvalue is: &#39; + str(max_pval)) else: break . Dropping column Seller_Type_Individual, pvalue is: 0.6296373292654155 Dropping column Fuel_Type_Diesel, pvalue is: 0.11176717429491591 Dropping column Seller_Type_Individual, pvalue is: 0.05428653381413104 . # Keeping only the columns with very low p-value X = df[columns] X_t = torch.from_numpy(X.to_numpy()).float() X_train, X_test, Y_train, Y_test = train_test_split(X_t, Y_t, test_size=0.33, random_state=42) w = multi_linear_reg(X_train, Y_train) Y_pred_train = prediction(X_train, w) Y_pred_test = prediction(X_test, w) mse_train = mse(Y_train, Y_pred_train) mse_test = mse(Y_test, Y_pred_test) print(&#39;MSE Train: t&#39;, mse_train) print(&#39;MSE Test: t&#39;, mse_test) . MSE Train: 3.4574925899505615 MSE Test: 3.8332533836364746 . Conclusion . A linear regression can perform pretty well if the data is highly correlated. It is an appropriate method as a baseline in the majority of the regression tasks. The pvalue is not always a good indicator for feature selection. .",
            "url": "https://consciousml.github.io/blog/linear-regression/car-price/pca/pytorch/from-scratch/2020/09/14/Multivariate-Linear-Regression.html",
            "relUrl": "/linear-regression/car-price/pca/pytorch/from-scratch/2020/09/14/Multivariate-Linear-Regression.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Linear Regression from scratch",
            "content": ". This blog post is about implementing linear regression from scratch using Pytorch. We will train the model on insurance data to predict the total payment received by a customer in regards to his number of claims. . For more details, the swedish auto insurance dataset contains the following attributes: . X = number of claims | Y = total payment for all the claims in thousands of Swedish Kronor for geographical zones in Sweden | . import os import sys import torch import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split . Reading Excel file with Pandas . data_path = os.path.join(&#39;data&#39;, &#39;slr06.xls&#39;) df = pd.read_excel(data_path, encoding_override=&quot;cp1252&quot;) df.head() . *** No CODEPAGE record, no encoding_override: will use &#39;ascii&#39; . X Y . 0 108 | 392.5 | . 1 19 | 46.2 | . 2 13 | 15.7 | . 3 124 | 422.2 | . 4 40 | 119.4 | . Data Visualization Using Seaborn . fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5)) ax1.set_title(&#39;Distribution of number of claims&#39;) ax2.set_title(&#39;Distribution of total payment&#39;) sns.distplot(df.X, bins=50, hist=True, ax=ax1) sns.distplot(df.Y, ax=ax2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d49e403b08&gt; . Two outliers are present between 100 and 125 number of claims. We will keep them in the training set. . sns.heatmap(df.corr(), annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d49e004088&gt; . The correlation heatmap and the plot below highlight that the number of claims and total payment are very highly correlated. . plt.xlabel(&#39;Number of claims&#39;) plt.ylabel(&#39;Total payment&#39;) plt.scatter(df[&#39;X&#39;], df[&#39;Y&#39;]) . &lt;matplotlib.collections.PathCollection at 0x1d49e6b9fc8&gt; . Training a Least Square Regressor . Convert the data to pytorch and separate the data into train &amp; test. . data = df.to_numpy() X = torch.from_numpy(data[:, 0]).float().unsqueeze(1) y = torch.from_numpy(data[:, 1]).float().unsqueeze(1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=50) . We will train a least square regressor on one dimension using the above formula: $ large y = b_0 + b_1x$ $ large b_1 = frac{ text{Cov}(x, y)}{Var(x)} = frac{ sum_i{(x_i - mathbb{E}[x]) * (y_i - mathbb{E}[y])}}{ sum_i{(x_i - mathbb{E}[x])^2}}$ where the expectation $ mathbb{E}[x]$ of a vector of random variables is its mean. . def linear_regression_1d(X, y): &quot;&quot;&quot;Trains a linear regression on 1D data Args: X: A numpy array for the training samples y: A numpy array for the labels of each sample &quot;&quot;&quot; X_m = X.mean(dim=0) y_m = y.mean(dim=0) X_c = (X - X_m) # Compute covariance and variance covar = (X_c * (y - y_m)).sum(dim=0) var = X_c.pow(2).sum(dim=0) # Divide covariance by variance b_1 = covar / var # Get bias b_0 = y_m - b_1 * X_m.sum(dim=0) return b_0, b_1 . We are using Pytorch for the matrix calculus. . Plot Regression Line . b_0, b_1 = linear_regression_1d(X_train, Y_train) plt.scatter(X_train, Y_train, marker=&#39;*&#39;) plt.scatter(X_test, Y_test, marker=&#39;.&#39;, color=&#39;red&#39;) x = [int(elt) for elt in range(0, int(X.cpu().numpy().max()))] y = b_0.numpy() + b_1.numpy() * x plt.plot(x, y, color=&#39;red&#39;) ax = plt.gca() ax.set_xlabel(&#39;Number of claims&#39;) ax.set_ylabel(&#39;Total payment&#39;) plt.show() . Because both the data and target are very highly correlated, the regression line fits pretty well the data distribution. . Compute Mean Square Error . pred_train = b_0 + b_1 * X_train err_train = Y_train - pred_train mse_train = err_train.T.mm(err_train) / Y_train.shape[0] pred_test = b_0 + b_1 * X_test err_test = Y_test - pred_test mse_test = err_test.T.mm(err_test) / Y_test.shape[0] print(&#39;Train MSE: t&#39;, mse_train.item()) print(&#39;Test MSE: t&#39;, mse_test.item()) . Train MSE: 1241.5714111328125 Test MSE: 1279.6766357421875 . The train and test MSE are close to each other. The model neither underfit nor overfit. . Conclusion . Linear regression is a simple and interpretable model that fits very well the purpose of a baseline. As we have seen during this experiment, if the features are highly correlated to the target, linear regression performs pretty well. .",
            "url": "https://consciousml.github.io/blog/linear-regression/insurance-data/pytorch/from-scratch/2020/09/12/Linear-Regression.html",
            "relUrl": "/linear-regression/insurance-data/pytorch/from-scratch/2020/09/12/Linear-Regression.html",
            "date": " • Sep 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://consciousml.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://consciousml.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}