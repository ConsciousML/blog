{
  
    
        "post0": {
            "title": "Logistic Regression From Scratch",
            "content": "During this experiment, we will train logistic regression on diabetes data, from scratch using Pytorch. The Pima Indians Diabetes Database has been gathered by the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of this dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. This dataset contains the following features: . Pregnancies | Glucose | BloodPressure | SkinThickness | Insuline | BMI | DiabetesPedigreeFunction | Age | Outcome (has diabetes or not) | . import os import torch import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split . Read csv using Pandas . diabetes = pd.read_csv(os.path.join(&#39;data&#39;, &#39;diabetes.csv&#39;)) . diabetes.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . diabetes.describe() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . count 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | . mean 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578 | 0.471876 | 33.240885 | 0.348958 | . std 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160 | 0.331329 | 11.760232 | 0.476951 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.078000 | 21.000000 | 0.000000 | . 25% 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000 | 0.243750 | 24.000000 | 0.000000 | . 50% 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000 | 0.372500 | 29.000000 | 0.000000 | . 75% 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000 | 0.626250 | 41.000000 | 1.000000 | . max 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 | 67.100000 | 2.420000 | 81.000000 | 1.000000 | . On this sample of data, the standard devitation of the columns looks reasonably high except for the DiabetesPedigree but it is acceptable because the mean is relatively low. A feature having low std is likely to provide close to no information to the model. Which is not the case here. . sns.countplot(x=&#39;Outcome&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1827e88b588&gt; . The target distribution is very unbalanced with two times more negative than positives. . Heatmap Correlation . plt.figure(figsize=(20, 5)) sns.heatmap(diabetes.corr(), annot=True, square=True, fmt=&#39;0.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1827ea30e48&gt; . The glucose level, BMI, age and number of pregnancies are highly correlated with the outcome. Suprisingly, the insulin level is not very correlated with the outcome. Most likely because the insulin is correlated with the glucose and the glucose has 0.47 correlation with the target. . plt.figure(figsize=(15, 3)) sns.boxplot(x=&#39;Glucose&#39;, y=&#39;Outcome&#39;, orient=&#39;h&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1827ee46e08&gt; . For patient with diabetes, the glucose level is significantly higher. In other words, a patient with high glucose level is very likely to have diabetes. . plt.figure(figsize=(15, 3)) sns.boxplot(x=&#39;Pregnancies&#39;, y=&#39;Outcome&#39;, orient=&#39;h&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1827fe92108&gt; . Surprisingly, the number of pregnancies is correlated with diabetes. . Convert data to Torch tensors . X = diabetes.iloc[:, :-1].values y = torch.from_numpy(diabetes.iloc[:, -1].values).float().unsqueeze(1) . X = StandardScaler().fit_transform(X) X = torch.from_numpy(X).float() # Add column of ones for the bias ones_col = torch.ones((X.shape[0], 1), dtype=torch.float32) X = torch.cat([ones_col, X], axis=1) . Split data into train and test . X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25) . Train Logistic Regression . The prediction of a logistic model is as follow: $$ large hat{y} = sigma( boldsymbol{X} boldsymbol{w}) $$ Where $ sigma$ is the sigmoid or logit function: $$ large sigma( boldsymbol{x}) = frac{1}{1 + exp(-x)} $$ The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$ given as input to the logit function. The sigmoid function is used here because it squashes the input in the $[0, 1]$ range suitable for describing a Bernouilli distribution. Where the Bernouilli cases are: . the patient has diabetes with $p$ probability | the patient does not have diabetes with $1 - p$ probability | . It is important to note that the bias is included in $ boldsymbol{X}$ as a column of ones. . Training a classification model can be expressed as maximizing the likelihood of the observed data. In other words, we want the predicted probability of our model that a patient has diabetes to be as close as the true probability of the data. In practice, maximizing the likelihood is equivalent to minimize the negative log likelihood: $$ large L( boldsymbol{ theta}) = - frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) $$ Because we dealing with a binary target, it is appropriate to use the binary cross entropy: $$ large L( boldsymbol{ theta}) = - frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) + (1 - boldsymbol{y_i}) log(1 - hat{ boldsymbol{y}}_i) $$ . Gradient Descent for Logistic Regression . We will use Gradient Descent to train the logistic regression model. The Gradient descent method takes steps proportional to the negative of the gradient of a function at a given point, in order to iteratively minimize the objective function. The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of $f$ is the vector containing all of the partial derivatives, denoted $ nabla_{ boldsymbol{x}}f( boldsymbol{x})$. . The directional derivative in direction $ boldsymbol{u}$ (a unit vector) is the slope of the function $f$ in direction $ boldsymbol{u}$. In other words, the directional derivative is the derivative of the function $f( boldsymbol{x} + sigma boldsymbol{u})$ with respect to $ sigma$ close to 0. To minimize $f$, we would like to find the direction in which $f$ decreases the fastest. We can do this using the directional derivative: $$ large min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{ boldsymbol{u}^ top nabla_{ boldsymbol{x}} f( boldsymbol{x})} $$ $$ large = min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{|| boldsymbol{u}||_2 || nabla_{ boldsymbol{x}}f( boldsymbol{x})||_2 cos theta}$$ ignoring factors that do not depend on $ boldsymbol{u}$, this simplifies to $ min_{u}{ cos theta}$. This is minimized when $ boldsymbol{u}$ points in the opposite direction as the gradient. Each step of the gradient descent method proposes a new point: $$ large boldsymbol{x&#39;} = boldsymbol{x} - epsilon nabla_{ boldsymbol{x}}f( boldsymbol{x}) $$ where $ epsilon$ is the learning rate. In the context of logistic regression, the gradient descent is as follow: $$ large boldsymbol{w} = boldsymbol{w} - epsilon nabla_{ boldsymbol{w}}L( boldsymbol{ theta}) $$ where: $$ large nabla_{ boldsymbol{w}}L( boldsymbol{ theta}) = nabla_{ boldsymbol{w}} left(- frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) + (1 - boldsymbol{y_i}) log(1 - hat{ boldsymbol{y}}_i) right) $$ $$ large = frac{1}{N} boldsymbol{X}^ top( sigma( boldsymbol{X} boldsymbol{w}) - boldsymbol{y}) $$ Here is a nice explanation of how to find the gradient of the binary cross entropy: https://www.youtube.com/watch?v=hWLdFMccpTY . def sigmoid(x): &quot;&quot;&quot;Sigmoid function that squashes the input between 0 and 1&quot;&quot;&quot; return 1 / (1 + torch.exp(-x)) def predict(X, weights): &quot;&quot;&quot;Pedicts the class given the data and the weights Args: X: A torch tensor for the input data. weights: A torch tensor for the parameters calculated during the training of the Logistic regression. &quot;&quot;&quot; return sigmoid(X.mm(weights)) . def binary_cross_entropy(y_true, y_pred): &quot;&quot;&quot;Loss function for the training of the logistic regression We add an epsilon inside the log functions to avoid Nans. Args: y_true: A torch tensor for the labels of the data. y_pred: A torch tensor for the values predicted by the model. &quot;&quot;&quot; fact = 1 / y_true.shape[0] return -fact * (y_true * torch.log(y_pred + 1e-10) + (1 - y_true) * torch.log(1 - y_pred + 1e-10 )).sum() . def train_logit_reg(X, y_true, weights, lr=0.001, it=2000): &quot;&quot;&quot;Trains the logistic regression model Args: X: A torch tensor for the training data. y: A torch tensor for the labels of the data. weights: A torch tensor for the learning parameters of the model. lr: A scalar describing the learning rate for the gradient descent. it: A scalar for the number of steps in the gradient descent. &quot;&quot;&quot; for _ in range(it): y_pred = predict(X, weights) err = (y_pred - y_true) grad = X.T.mm(err) weights -= lr * grad bn_train = binary_cross_entropy(y_true, y_pred).item() return weights, bn_train . weights = torch.ones((X.shape[1], 1), dtype=torch.float32) weights, bn_train = train_logit_reg(X_train, y_train, weights) y_pred = predict(X_test, weights) print(&#39;Binary cross-entropy on the train set:&#39;, bn_train) . Binary cross-entropy on the train set: 0.4595394730567932 . Binary cross entropy on test set . bn_test = binary_cross_entropy(y_test, y_pred).item() print(&#39;Binary cross-entropy on the test set:&#39;, bn_test) . Binary cross-entropy on the test set: 0.5321948528289795 . Accuracy on test set . To compute the accuracy, we have to find the best threshold to convert our probability output in binary values. . def get_binary_pred(y_true, y_pred): &quot;&quot;&quot;Finds the best threshold based on the prediction and the labels Args: y_true: A torch tensor for the labels of the data. y_pred: A torch tensor for the values predicted by the model. &quot;&quot;&quot; y_pred_thr = y_pred.clone() accs = [] thrs = [] for thr in np.arange(0, 1, 0.01): y_pred_thr[y_pred &gt;= thr] = 1 y_pred_thr[y_pred &lt; thr] = 0 cur_acc = classification_report(y_test, y_pred_thr, output_dict=True)[&#39;accuracy&#39;] accs.append(cur_acc) thrs.append(thr) accs = torch.FloatTensor(accs) thrs = torch.FloatTensor(thrs) idx = accs.argmax() best_thr = thrs[idx].item() best_acc = accs[idx].item() y_pred[y_pred &gt;= best_thr] = 1 y_pred[y_pred &lt; best_thr] = 0 return y_pred . import sklearn y_pred = get_binary_pred(y_test, y_pred) print(classification_report(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) . precision recall f1-score support 0.0 0.79 0.90 0.84 123 1.0 0.76 0.57 0.65 69 accuracy 0.78 192 macro avg 0.78 0.73 0.75 192 weighted avg 0.78 0.78 0.77 192 [[111 12] [ 30 39]] . With a threshold of 0.66, we achieve an accuracy of 78% which is quite good for a linear model. . Polynomial Logistic Regression . In this section, we will add some polynomial features to the logistic regressor. It is the same principle as the logistic regression except that $ boldsymbol{X}$ is the concatenation of $ boldsymbol{X_1} dots boldsymbol{X_m}$ where $m$ is the degree of the polynomial function and $ boldsymbol{w}$ is the concatenation of $ boldsymbol{w_1} dots boldsymbol{w_m}$ such as: $$ large boldsymbol{y} = sigma( boldsymbol{w}_0 + boldsymbol{X} boldsymbol{w}_1 + boldsymbol{X}^2 boldsymbol{w}_2 + dots + boldsymbol{X}^m boldsymbol{w}_m) $$ This method is still linear because predicting $ hat{y} = sigma( boldsymbol{X} boldsymbol{w})$ is still linear in the parameters. . X = torch.from_numpy(diabetes.iloc[:, :-1].values).float() y = torch.from_numpy(diabetes.iloc[:, -1].values).float().unsqueeze(1) . def create_poly_features(X, degree=2): &quot;&quot;&quot;Creates the augmented features for the polynomial model This function concatenates the augmented data into a single torch tensor. Args: X: A torch tensor for the data. degree: A integer for the degree of the polynomial function that we model. &quot;&quot;&quot; if len(X.shape) == 1: X = X.unsqueeze(1) ones_col = torch.ones((X.shape[0], 1)) # Standardize the output to avoid exploding gradients X_cat = X.clone() X_cat = (X_cat - X_cat.mean()) / X_cat.std() X_cat = torch.cat([ones_col, X_cat], axis=1) for i in range(1, degree): X_p = X.pow(i + 1) X_p = torch.from_numpy(StandardScaler().fit_transform(X_p)).float() X_cat = torch.cat([X_cat, X_p], axis=1) return X_cat . features = create_poly_features(X, degree=2) . def create_weights(features): &quot;&quot;&quot;Creates a column of ones&quot;&quot;&quot; return torch.ones((features.shape[1], 1), dtype=torch.float32) . X_train, X_test, y_train, y_test = train_test_split(features, y, random_state=42, test_size=0.25) . weights = create_weights(X_train) weights, bn_train = train_logit_reg(X_train, y_train, weights, it=10000) y_pred = predict(X_test, weights) print(&#39;Binary cross-entropy on the train set:&#39;, bn_train) . Binary cross-entropy on the train set: 0.44449102878570557 . binary_cross_entropy(y_test, y_pred).item() . 0.5331151485443115 . y_pred = get_binary_pred(y_test, y_pred) print(classification_report(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) . precision recall f1-score support 0.0 0.75 0.95 0.84 123 1.0 0.84 0.45 0.58 69 accuracy 0.77 192 macro avg 0.80 0.70 0.71 192 weighted avg 0.78 0.77 0.75 192 [[117 6] [ 38 31]] . Conclusion . The polynomial logistic regression model overfitted compared to the classic logistic regression model because it lost 1% accuracy on the test set. For some very highly correlated data, logistic regression without polynomial features has better performance than with polynomial features. Logistic regression is a very simple and interpretable model suited as a baseline in most classification problems. However, it does not perform well when the feature space is large. In fact, it is difficult to compute feature transformation (such as polynomials) when the data doesn&#39;t fit in ram. .",
            "url": "https://consciousml.github.io/blog/logistic-regression/polynomial-regression/diabetes/pytorch/from-scratch/2020/09/17/Logistic-Regression.html",
            "relUrl": "/logistic-regression/polynomial-regression/diabetes/pytorch/from-scratch/2020/09/17/Logistic-Regression.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Multivariate Linear Regression from scratch",
            "content": "The CarDekho dataset contains information about used cars listed on the website of the same name. This dataset has 301 unique entities with the following features: . car name | year of release | selling price | present price | kilometers driven | fuel: such as petrol or diesel | transmission: such as manual or automatic | owner: how many times the car changed owner | . The goal of this experiment is to train a linear model to predict the selling price of a car. We will use the framework Pytorch for the matrix calculus. . import os import sys import torch import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split . Read data from csv using Pandas . df = pd.read_csv(os.path.join(&#39;data&#39;, &#39;car_data.csv&#39;)) df.head() . Car_Name Year Selling_Price Present_Price Kms_Driven Fuel_Type Seller_Type Transmission Owner . 0 ritz | 2014 | 3.35 | 5.59 | 27000 | Petrol | Dealer | Manual | 0 | . 1 sx4 | 2013 | 4.75 | 9.54 | 43000 | Diesel | Dealer | Manual | 0 | . 2 ciaz | 2017 | 7.25 | 9.85 | 6900 | Petrol | Dealer | Manual | 0 | . 3 wagon r | 2011 | 2.85 | 4.15 | 5200 | Petrol | Dealer | Manual | 0 | . 4 swift | 2014 | 4.60 | 6.87 | 42450 | Diesel | Dealer | Manual | 0 | . Convert categorical variable into indicator variables . f_continuous = df[[&#39;Year&#39;, &#39;Selling_Price&#39;, &#39;Present_Price&#39;, &#39;Kms_Driven&#39;, &#39;Owner&#39;]] f_categorical = pd.get_dummies(df[[&#39;Fuel_Type&#39;, &#39;Seller_Type&#39;, &#39;Transmission&#39;]]) df = pd.concat([f_continuous, f_categorical], axis=1) # Drop refundant features df.drop([&#39;Transmission_Automatic&#39;, &#39;Seller_Type_Dealer&#39;, &#39;Fuel_Type_CNG&#39;], axis=1, inplace=True) df.head() . Year Selling_Price Present_Price Kms_Driven Owner Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 0 2014 | 3.35 | 5.59 | 27000 | 0 | 0 | 1 | 0 | 1 | . 1 2013 | 4.75 | 9.54 | 43000 | 0 | 1 | 0 | 0 | 1 | . 2 2017 | 7.25 | 9.85 | 6900 | 0 | 0 | 1 | 0 | 1 | . 3 2011 | 2.85 | 4.15 | 5200 | 0 | 0 | 1 | 0 | 1 | . 4 2014 | 4.60 | 6.87 | 42450 | 0 | 1 | 0 | 0 | 1 | . Visualize histogram of all features . df.hist(bins=14, color=&#39;steelblue&#39;, edgecolor=&#39;black&#39;, linewidth=1.0, xlabelsize=8, ylabelsize=8, grid=False) plt.tight_layout(rect=(0, 0, 1.2, 1.2)) . Most cars on sales are consuming petrol instead of diesel, have had only one owner, are from 2012-present, are manual and have a selling price between 1000 and 10000 dollars. . Heatmap correlation . plt.figure(figsize=(16, 8)) sns.heatmap(df.corr(), square= True, annot=True, fmt=&#39;.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c3a2540988&gt; . Most of the variables are highly correlated. As expected, the present price variable is the most correlated with the target selling price. . Pairwise Plots . cols_viz = [&#39;Kms_Driven&#39;, &#39;Year&#39;, &#39;Selling_Price&#39;, &#39;Present_Price&#39;] pp = sns.pairplot(df[cols_viz], height=1.8, aspect=1.8, plot_kws=dict(edgecolor=&quot;k&quot;, linewidth=0.5), diag_kind=&quot;kde&quot;, diag_kws=dict(shade=True)) fig = pp.fig fig.subplots_adjust(top=0.93, wspace=0.3) t = fig.suptitle(&#39;Wine Attributes Pairwise Plots&#39;, fontsize=14) . Most of the features are highly correlated to each other. Some outliers are present but as there is very few, we will keep them in the training set. The year feature have a polynomial correlation with the selling price. A polynomial regression will most likely overperform a linear regression. . Make train test split . Y = df[&#39;Selling_Price&#39;] X = df.drop(&#39;Selling_Price&#39;, axis=1) . X_t = torch.from_numpy(X.to_numpy()).float() Y_t = torch.from_numpy(Y.to_numpy()).float().unsqueeze(1) X_train, X_test, Y_train, Y_test = train_test_split(X_t, Y_t, test_size=0.33, random_state=42) . Train a multivariate linear regression . Training a linear model using least square regression is equivalent to minimize the mean squared error: . $$ begin{align} large text{Mse}( boldsymbol{ hat{y}}, boldsymbol{y}) &amp;= frac{1}{n} sum_{i=1}^{n}{|| hat{y}_i - y_i ||_{2}^{2}} &amp;= frac{1}{n}|| boldsymbol{X} boldsymbol{w} - boldsymbol{y} ||_2^2 end{align} $$where $n$ is the number of samples, $ hat{y}$ is the predicted value of the model and $y$ is the true target. The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$. . Minimizing the $ text{Mse}$ can be achieved by solving the gradient of this equation equals to zero in regards to the weights $ boldsymbol{w}$: . $$ begin{align} large nabla_{ boldsymbol{w}} text{Mse} &amp;= 0 large ( boldsymbol{X}^ top boldsymbol{X})^{-1} boldsymbol{X}^ top boldsymbol{y} &amp;= boldsymbol{w} end{align} $$For more information on how to find $ boldsymbol{w}$ please visit the following link. . def add_ones_col(X): &quot;&quot;&quot;Add a column a one to the input torch tensor&quot;&quot;&quot; x_0 = torch.ones((X.shape[0],), dtype=torch.float32).unsqueeze(1) X = torch.cat([x_0, X], dim=1) return X def multi_linear_reg(X, y): &quot;&quot;&quot;Multivariate linear regression function Args: X: A torch tensor for the data. y: A torch tensor for the labels. &quot;&quot;&quot; X = add_ones_col(X) # Add a column of ones to X to agregate the bias to the input matrices Xt_X = X.T.mm(X) Xt_y = X.T.mm(y) Xt_X_inv = Xt_X.inverse() w = Xt_X_inv.mm(Xt_y) return w def prediction(X, w): &quot;&quot;&quot;Predicts a selling price for each input Args: X: A torch tensor for the data. w: A torch tensor for the weights of the linear regression mode. &quot;&quot;&quot; X = add_ones_col(X) return X.mm(w) . w = multi_linear_reg(X_train, Y_train) # Predict using matrix multiplication with the weights Y_pred_train = prediction(X_train, w) Y_pred_test = prediction(X_test, w) . Compute prediction error . def mse(Y_true, Y_pred): error = Y_pred - Y_true return (error.T.mm(error) / Y_pred.shape[0]).item() def mae(Y_true, Y_pred): error = Y_pred - Y_true return error.abs().mean().item() . mse_train = mse(Y_train, Y_pred_train) mae_train = mae(Y_train, Y_pred_train) print(&#39;MSE Train: t&#39;, mse_train) print(&#39;MAE Train: t&#39;, mae_train, end=&#39; n n&#39;) mse_test = mse(Y_test, Y_pred_test) mae_test = mae(Y_test, Y_pred_test) print(&#39;MSE Test: t&#39;, mse_test) print(&#39;MAE Test: t&#39;, mae_test, end=&#39; n n&#39;) . MSE Train: 2.808985471725464 MAE Train: 1.1321566104888916 MSE Test: 3.7205495834350586 MAE Test: 1.2941011190414429 . The model has an error of 1.29 on average on the training test. Not bad for a linear model, taking into consideration that the mean of the present price is 7.62. . Principal component analysis visualization . In this section, we will use PCA to reduce the number of feature to two, in order to visualize the plane of the linear regressor. . Suppose a collection of $m$ points $ { boldsymbol{x}^{(1)}, dots, boldsymbol{x}^{(m)} }$ in $ mathbb{R}^n$. The principal components analysis aims to reduce the dimensionality of the points while losing the least precision as possible. For each point $ boldsymbol{x}^{(i)} in mathbb{R}^n$ we will find a corresponding code vector $ boldsymbol{c}^{(i)} in mathbb{R}^l$ where $l$ is smaller than $n$. Let $f$ be the encoding function and $g$ be the decoding function and $ boldsymbol{D} in mathbb{R}^{n,l}$ is the decoding matrix whose columns are orthonormal: $$ begin{align} f( boldsymbol{x}) &amp;= boldsymbol{D}^ top boldsymbol{x} g(f( boldsymbol{x})) &amp;= boldsymbol{D} boldsymbol{D}^ top boldsymbol{x} end{align} $$ . def cov(X): &quot;&quot;&quot;Computes the covariance of the input The covariance matrix gives some sense of how much two values are linearly related to each other, as well as the scale of these variables. It is computed by (1 / (N - 1)) * (X - E[X]).T (X - E[X]). Args: X: A torch tensor as input. &quot;&quot;&quot; X -= X.mean(dim=0, keepdim=True) fact = 1.0 / (X.shape[0] - 1) cov = fact * X.T.mm(X) return cov def pca(X, target_dim=2): &quot;&quot;&quot;Computes the n^th first principal components of the input PCA can be implemented using the n^th principal components of the covariance matrix. We could have been using an eigen decomposition because the covariance matrix is always squared but singular value decomposition does also the trick if we take the right singular vectors and perform a matrix multiplication to the right. Args: X: A torch tensor as the input. target_dim: An integer for selecting the n^th first components. &quot;&quot;&quot; cov_x = cov(X) U, S, V = torch.svd(cov_x) transform_mat = V[:, :target_dim] X_reduced = X.mm(transform_mat) return X_reduced, transform_mat . X_test_pca, _ = pca(X_test, target_dim=2) X_train_pca, _ = pca(X_train, target_dim=2) . points = torch.cat([X_test_pca[:3], Y_pred_test[:3]], axis=1) v1 = points[2, :] - points[0, :] v2 = points[1, :] - points[0, :] cp = torch.cross(v1, v2) a, b, c = cp d = cp.dot(points[2, :]) min_mesh_x = min(X_test_pca[:, 0].min(), X_train_pca[:, 0].min()) max_mesh_x = max(X_test_pca[:, 0].max(), X_train_pca[:, 0].max()) min_mesh_y = min(X_test_pca[:, 1].min(), X_train_pca[:, 1].min()) max_mesh_y = max(X_test_pca[:, 1].max(), X_train_pca[:, 1].max()) mesh_x = np.linspace(min_mesh_x, max_mesh_x, 25) mesh_y = np.linspace(min_mesh_y, max_mesh_y, 25) mesh_xx, mesh_yy = np.meshgrid(mesh_x, mesh_y) mesh_zz = (d - a * mesh_xx - b * mesh_yy) / c . Here we recreate the prediction plane using three points of the prediction. More information at this link. . fig = plt.figure(figsize=(25,7)) ax1 = fig.add_subplot(131, projection=&#39;3d&#39;) ax2 = fig.add_subplot(132, projection=&#39;3d&#39;) ax3 = fig.add_subplot(133, projection=&#39;3d&#39;) axes = [ax1, ax2, ax3] for ax in axes: ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], Y_test, color=&#39;red&#39;, edgecolor=&#39;black&#39;) ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], Y_train, color=&#39;green&#39;, edgecolor=&#39;black&#39;) ax.scatter(mesh_xx.flatten(), mesh_yy.flatten(), mesh_zz.flatten(), facecolor=(0, 0, 0, 0), s=20, edgecolor=&#39;#70b3f0&#39;) ax.set_xlabel(&#39;1st component&#39;, fontsize=12) ax.set_ylabel(&#39;2nd component&#39;, fontsize=12) ax.set_zlabel(&#39;Selling Price&#39;, fontsize=12) ax.ticklabel_format(axis=&quot;x&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax.ticklabel_format(axis=&quot;y&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax.ticklabel_format(axis=&quot;z&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax1.view_init(elev=60, azim=50) ax2.view_init(elev=10, azim=0) ax3.view_init(elev=-15, azim=140) . The plane is fitting pretty well the data ! . Exploratory Data Analysis . We made an attempt to discard some features based on p-value but it didn&#39;t improve the results. More on p-value here. . columns = X.columns while len(columns) &gt; 0: pvalues = [] X_1 = X[columns] X_1 = sm.add_constant(X) # add a columns of ones for the bias model = sm.OLS(Y, X_1).fit() # fit a linear regression pvalues = pd.Series(model.pvalues[1:], index=columns) max_idx = np.argmax(pvalues) max_pval = pvalues[max_idx] if max_pval &gt; 0.05: # if the p_values is greater than 0.05, the feature has not enough # informational value for the training columns = columns.drop(columns[max_idx]) print(&#39;Dropping column &#39; + columns[max_idx] + &#39;, pvalue is: &#39; + str(max_pval)) else: break . Dropping column Seller_Type_Individual, pvalue is: 0.6296373292654155 Dropping column Fuel_Type_Diesel, pvalue is: 0.11176717429491591 Dropping column Seller_Type_Individual, pvalue is: 0.05428653381413104 . X = df[columns] X_t = torch.from_numpy(X.to_numpy()).float() X_train, X_test, Y_train, Y_test = train_test_split(X_t, Y_t, test_size=0.33, random_state=42) w = multi_linear_reg(X_train, Y_train) Y_pred_train = prediction(X_train, w) Y_pred_test = prediction(X_test, w) mse_train = mse(Y_train, Y_pred_train) mse_test = mse(Y_test, Y_pred_test) print(&#39;MSE Train: t&#39;, mse_train) print(&#39;MSE Test: t&#39;, mse_test) . MSE Train: 3.4574925899505615 MSE Test: 3.8332533836364746 . Conclusion . A linear regression can perform pretty well if the data is highly correlated. It is an appropriate method as a baseline in the majority of the regression tasks. The pvalue is not always a good indicator for feature selection. .",
            "url": "https://consciousml.github.io/blog/linear-regression/car-price/pca/pytorch/from-scratch/2020/09/14/Multivariate-Linear-Regression.html",
            "relUrl": "/linear-regression/car-price/pca/pytorch/from-scratch/2020/09/14/Multivariate-Linear-Regression.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Linear Regression from scratch",
            "content": "The swedish auto insurance dataset contains the following attributes: . X = number of claims | Y = total payment for all the claims in thousands of Swedish Kronor for geographical zones in Sweden | . import os import sys import torch import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split . Reading Excel file with Pandas . data_path = os.path.join(&#39;data&#39;, &#39;slr06.xls&#39;) df = pd.read_excel(data_path, encoding_override=&quot;cp1252&quot;) df.head() . *** No CODEPAGE record, no encoding_override: will use &#39;ascii&#39; . X Y . 0 108 | 392.5 | . 1 19 | 46.2 | . 2 13 | 15.7 | . 3 124 | 422.2 | . 4 40 | 119.4 | . Data Visualization Using Seaborn . fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5)) ax1.set_title(&#39;Distribution of number of claims&#39;) ax2.set_title(&#39;Distribution of total payment&#39;) sns.distplot(df.X, bins=50, hist=True, ax=ax1) sns.distplot(df.Y, ax=ax2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d49e403b08&gt; . Two outliers are present between 100 and 125 number of claims. We will keep them in the training set. . sns.heatmap(df.corr(), annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d49e004088&gt; . The correlation heatmap and the plot below highlight that the number of claims and total payment are very highly correlated. . plt.xlabel(&#39;Number of claims&#39;) plt.ylabel(&#39;Total payment&#39;) plt.scatter(df[&#39;X&#39;], df[&#39;Y&#39;]) . &lt;matplotlib.collections.PathCollection at 0x1d49e6b9fc8&gt; . Training a Least Square Regressor . Convert the data to pytorch and separate the data into train &amp; test. . data = df.to_numpy() X = torch.from_numpy(data[:, 0]).float().unsqueeze(1) y = torch.from_numpy(data[:, 1]).float().unsqueeze(1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=50) . We will train a least square regressor on one dimension using the above formula: $ large y = b_0 + b_1x$ $ large b_1 = frac{ text{Cov}(x, y)}{Var(x)} = frac{ sum_i{(x_i - mathbb{E}[x]) * (y_i - mathbb{E}[y])}}{ sum_i{(x_i - mathbb{E}[x])^2}}$ where the expectation $ mathbb{E}[x]$ of a vector of random variables is its mean. . def linear_regression_1d(X, y): &quot;&quot;&quot;Trains a linear regression on 1D data Args: X: A numpy array for the training samples y: A numpy array for the labels of each sample &quot;&quot;&quot; X_m = X.mean(dim=0) y_m = y.mean(dim=0) X_c = (X - X_m) # Compute covariance and variance covar = (X_c * (y - y_m)).sum(dim=0) var = X_c.pow(2).sum(dim=0) # Divide covariance by variance b_1 = covar / var # Get bias b_0 = y_m - b_1 * X_m.sum(dim=0) return b_0, b_1 . We are using Pytorch for the matrix calculus. . Plot Regression Line . b_0, b_1 = linear_regression_1d(X_train, Y_train) plt.scatter(X_train, Y_train, marker=&#39;*&#39;) plt.scatter(X_test, Y_test, marker=&#39;.&#39;, color=&#39;red&#39;) x = [int(elt) for elt in range(0, int(X.cpu().numpy().max()))] y = b_0.numpy() + b_1.numpy() * x plt.plot(x, y, color=&#39;red&#39;) ax = plt.gca() ax.set_xlabel(&#39;Number of claims&#39;) ax.set_ylabel(&#39;Total payment&#39;) plt.show() . Because both the data and target are very highly correlated, the regression line fits pretty well the data distribution. . Compute Mean Square Error . pred_train = b_0 + b_1 * X_train err_train = Y_train - pred_train mse_train = err_train.T.mm(err_train) / Y_train.shape[0] pred_test = b_0 + b_1 * X_test err_test = Y_test - pred_test mse_test = err_test.T.mm(err_test) / Y_test.shape[0] print(&#39;Train MSE: t&#39;, mse_train.item()) print(&#39;Test MSE: t&#39;, mse_test.item()) . Train MSE: 1241.5714111328125 Test MSE: 1279.6766357421875 . The train and test MSE are close to each other. The model neither underfit nor overfit. . Conclusion . Linear regression is a simple and interpretable model that fits very well the purpose of a baseline. As we have seen during this experiment, if the features are highly correlated to the target, linear regression performs pretty well. .",
            "url": "https://consciousml.github.io/blog/linear-regression/insurance-data/pytorch/from-scratch/2020/09/12/Linear-Regression.html",
            "relUrl": "/linear-regression/insurance-data/pytorch/from-scratch/2020/09/12/Linear-Regression.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c =3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c =3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c =3, d=4): pass @delegates(basefoo, but= [&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7fe0ab662790&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from pathlib import Path p = Path(&#39;../_notebooks&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#21) [Path(&#39;../_notebooks/gpt2_simple_mask.jpg&#39;),Path(&#39;../_notebooks/bert_mac_small.jpg&#39;),Path(&#39;../_notebooks/causal_with_prefix.jpg&#39;),Path(&#39;../_notebooks/.DS_Store&#39;),Path(&#39;../_notebooks/2020-03-07-How_to_Create_an_Automatic_Code_Comment_Generator_using_Deep_Learning.ipynb&#39;),Path(&#39;../_notebooks/2020-09-01-fastcore.ipynb&#39;),Path(&#39;../_notebooks/2020-03-07-Syntax-Highlighting.ipynb&#39;),Path(&#39;../_notebooks/2020-03-06-bart.ipynb&#39;),Path(&#39;../_notebooks/README.md&#39;),Path(&#39;../_notebooks/2020-05-01-TrainDonkeyCar.ipynb&#39;)...] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.foundation module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [2,0,18,6,15,17,14,8,12,1...] . Index into a list: . p[2,4,6] . (#3) [18,15,14] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Utilites section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://consciousml.github.io/blog/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://consciousml.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://consciousml.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://consciousml.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}