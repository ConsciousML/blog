{
  
    
        "post0": {
            "title": "Naive Bayes Classifier from scratch",
            "content": ". In this experiment, the Naive Bayes Classifier method will be implemented from scratch using PyTorch. We will train our model on the Student Alcohol Consumption dataset to try to predict if a student frequently drink alcohol or not. . The data contains the following attributes: . school - student&#39;s school (binary: &#39;GP&#39; - Gabriel Pereira or &#39;MS&#39; - Mousinho da Silveira) | sex - student&#39;s sex (binary: &#39;F&#39; - female or &#39;M&#39; - male) | age - student&#39;s age (numeric: from 15 to 22) | address - student&#39;s home address type (binary: &#39;U&#39; - urban or &#39;R&#39; - rural) | famsize - family size (binary: &#39;LE3&#39; - less or equal to 3 or &#39;GT3&#39; - greater than 3) | Pstatus - parent&#39;s cohabitation status (binary: &#39;T&#39; - living together or &#39;A&#39; - apart) | Medu - mother&#39;s education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education) | Fedu - father&#39;s education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education) | Mjob - mother&#39;s job (nominal: &#39;teacher&#39;, &#39;health&#39; care related, civil &#39;services&#39; (e.g. administrative or police), &#39;at_home&#39; or &#39;other&#39;) | Fjob - father&#39;s job (nominal: &#39;teacher&#39;, &#39;health&#39; care related, civil &#39;services&#39; (e.g. administrative or police), &#39;at_home&#39; or &#39;other&#39;) | reason - reason to choose this school (nominal: close to &#39;home&#39;, school &#39;reputation&#39;, &#39;course&#39; preference or &#39;other&#39;) | guardian - student&#39;s guardian (nominal: &#39;mother&#39;, &#39;father&#39; or &#39;other&#39;) | traveltime - home to school travel time (numeric: 1 - 1 hour) | studytime - weekly study time (numeric: 1 - 10 hours) | failures - number of past class failures (numeric: n if 1&lt;=n&lt;3, else 4) | schoolsup - extra educational support (binary: yes or no) | famsup - family educational support (binary: yes or no) | paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no) | activities - extra-curricular activities (binary: yes or no) | nursery - attended nursery school (binary: yes or no) | higher - wants to take higher education (binary: yes or no) | internet - Internet access at home (binary: yes or no) | romantic - with a romantic relationship (binary: yes or no) | famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent) | freetime - free time after school (numeric: from 1 - very low to 5 - very high) | goout - going out with friends (numeric: from 1 - very low to 5 - very high) | health - current health status (numeric: from 1 - very bad to 5 - very good) | absences - number of school absences (numeric: from 0 to 93) | G1 - first period grade (numeric: from 0 to 20) | G2 - second period grade (numeric: from 0 to 20) | G3 - final grade (numeric: from 0 to 20, output target) | Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high) | Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high) | . import os import math import torch import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from pprint import pprint from sklearn.base import BaseEstimator from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import cross_val_score . Read csv with Pandas . students_mat = pd.read_csv(os.path.join(&#39;data&#39;, &#39;student-mat.csv&#39;)) students_por = pd.read_csv(os.path.join(&#39;data&#39;, &#39;student-por.csv&#39;)) # Concatenating students data from math and portuguese class students = pd.concat([students_mat, students_por], axis=0) # Averaging three grades into one single grade students[&#39;grade&#39;] = (students[&#39;G1&#39;] + students[&#39;G1&#39;] + students[&#39;G3&#39;]) / 3 # Combining weekly and weekend alcohol consumption into a single attribute students[&#39;alc&#39;] = students[&#39;Walc&#39;] + students[&#39;Dalc&#39;] # Drop the combined columns students = students.drop(columns=[&#39;G1&#39;, &#39;G2&#39;, &#39;G3&#39;, &#39;school&#39;]) students.head(5) . sex age address famsize Pstatus Medu Fedu Mjob Fjob reason ... romantic famrel freetime goout Dalc Walc health absences grade alc . 0 F | 18 | U | GT3 | A | 4 | 4 | at_home | teacher | course | ... | no | 4 | 3 | 4 | 1 | 1 | 3 | 6 | 5.333333 | 2 | . 1 F | 17 | U | GT3 | T | 1 | 1 | at_home | other | course | ... | no | 5 | 3 | 3 | 1 | 1 | 3 | 4 | 5.333333 | 2 | . 2 F | 15 | U | LE3 | T | 1 | 1 | at_home | other | other | ... | no | 4 | 3 | 2 | 2 | 3 | 3 | 10 | 8.000000 | 5 | . 3 F | 15 | U | GT3 | T | 4 | 2 | health | services | home | ... | yes | 3 | 2 | 2 | 1 | 1 | 5 | 2 | 15.000000 | 2 | . 4 F | 16 | U | GT3 | T | 3 | 3 | other | other | home | ... | no | 4 | 3 | 2 | 1 | 2 | 5 | 4 | 7.333333 | 3 | . 5 rows × 31 columns . Transform string to categorical values . categorical_dict = {} for col in students.columns: # For each column of type object, use sklearn label encoder and add the mapping to a dictionary if students[col].dtype == &#39;object&#39;: le = LabelEncoder() students[col] = le.fit_transform(students[col]) categorical_dict[col] = dict(zip(le.classes_, le.transform(le.classes_))) . pprint(categorical_dict) . {&#39;Fjob&#39;: {&#39;at_home&#39;: 0, &#39;health&#39;: 1, &#39;other&#39;: 2, &#39;services&#39;: 3, &#39;teacher&#39;: 4}, &#39;Mjob&#39;: {&#39;at_home&#39;: 0, &#39;health&#39;: 1, &#39;other&#39;: 2, &#39;services&#39;: 3, &#39;teacher&#39;: 4}, &#39;Pstatus&#39;: {&#39;A&#39;: 0, &#39;T&#39;: 1}, &#39;activities&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;address&#39;: {&#39;R&#39;: 0, &#39;U&#39;: 1}, &#39;famsize&#39;: {&#39;GT3&#39;: 0, &#39;LE3&#39;: 1}, &#39;famsup&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;guardian&#39;: {&#39;father&#39;: 0, &#39;mother&#39;: 1, &#39;other&#39;: 2}, &#39;higher&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;internet&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;nursery&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;paid&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;reason&#39;: {&#39;course&#39;: 0, &#39;home&#39;: 1, &#39;other&#39;: 2, &#39;reputation&#39;: 3}, &#39;romantic&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;schoolsup&#39;: {&#39;no&#39;: 0, &#39;yes&#39;: 1}, &#39;sex&#39;: {&#39;F&#39;: 0, &#39;M&#39;: 1}} . Here is the dictonary of the categorical labels . Correlation heatmap . plt.figure(figsize=(20, 15)) sns.heatmap(students.corr(), annot=True, square=True, fmt=&#39;0.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a6856d48&gt; . Just from the correlation heatmap, we can have an overview of the impact of alcohol consumption on students. Based on the linear correlation between the target and the features, the is a tendency for students consuming alcohol frequently to have more chance to: . have lower grades | have more absences | hang out more often | does not aim to achieve higher education | study less | . Among all theses cases, the attributes that are the most correlated with the target are the grades, the study time and if the student is a men. . Impact of alcohol consumption on students life . def plot_pie(data, column, ax): &quot;&quot;&quot;Plots a pie diagram Args: data: A pandas data frame for the data. columns: A list containing the columns we are interested in. ax: The plt ax from which to plot the pie. &quot;&quot;&quot; counts = data[column].value_counts() percent = counts / counts.sum() * 100 labels = counts.index ax.pie(x=percent, labels=labels, autopct=&#39;%1.0f%%&#39;) . _, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7)) plot_pie(students, column=&#39;Dalc&#39;, ax=ax1) plot_pie(students, column=&#39;Walc&#39;, ax=ax2) ax1.set_title(&#39;Weekday Alcohol Consumption&#39;) ax2.set_title(&#39;Weekend Alcohol Consumption&#39;) . Text(0.5, 1.0, &#39;Weekend Alcohol Consumption&#39;) . The alcohol consumption during workdays is relatively low compared to the weekend consumption. Most students prefer to stay sober during workdays. Let&#39;s see how those behaviors have an impact on students success and life. . _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4)) sns.boxplot(y=&#39;Dalc&#39;, x=&#39;studytime&#39;, orient=&#39;h&#39;, data=students, ax=ax1) sns.boxplot(y=&#39;Walc&#39;, x=&#39;studytime&#39;, orient=&#39;h&#39;, data=students, ax=ax2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a8863cc8&gt; . Students who does not drink alcohol during weekdays usually study more than those who do. But the amount of study hours of students who does not drink during weekend is much more than the ones who do. . sober_absences = students.loc[students[&#39;Dalc&#39;] &lt;= 2, &#39;absences&#39;] drunk_absences = students.loc[students[&#39;Dalc&#39;] &gt; 2, &#39;absences&#39;] _, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4)) ax1.set_xlim(-10, 35) ax2.set_xlim(-10, 35) ax1.set_ylim(0, 0.30) ax2.set_ylim(0, 0.30) ax1.set_title(&#39;Absences distribution of sober workdays students&#39;) ax2.set_title(&#39;Absences distribution of drunk workdays students&#39;) sns.distplot(sober_absences, ax=ax1) sns.distplot(drunk_absences, ax=ax2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a8de7b08&gt; . Students who drink two times or more a week have a tendency to be more absent in class. . _, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4)) sober_grades = students.loc[students[&#39;Dalc&#39;] == 1, &#39;grade&#39;] drunk_grades = students.loc[students[&#39;Dalc&#39;] &gt; 1, &#39;grade&#39;] severe_drunk_grades = students.loc[students[&#39;Dalc&#39;] == 5, &#39;grade&#39;] ax1.set_ylim(0, 0.18) ax2.set_ylim(0, 0.18) ax3.set_ylim(0, 0.18) ax1.set_xlim(0, 20) ax2.set_xlim(0, 20) ax3.set_xlim(0, 20) ax1.set_title(&#39;Grades distribution of sober weekdays students&#39;) ax2.set_title(&#39;Grades distribution of students consuming alcohol on weekdays&#39;) ax3.set_title(&#39;Grades distribution of students with high weekdays consumption&#39;) sns.distplot(sober_grades, ax=ax1) sns.distplot(drunk_grades, ax=ax2) sns.distplot(severe_drunk_grades, ax=ax3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a7d7e7c8&gt; . Students who drink even a little during workdays have lower grades than those who do not. The impact on grades is much more important for students with severe consumption. . _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4)) sober_grades = students.loc[students[&#39;Walc&#39;] == 1, &#39;grade&#39;] severe_drunk_grades = students.loc[students[&#39;Walc&#39;] == 5, &#39;grade&#39;] ax1.set_ylim(0, 0.18) ax2.set_ylim(0, 0.18) ax1.set_xlim(0, 20) ax2.set_xlim(0, 20) ax1.set_title(&#39;Grades distribution of students sober during the weekend&#39;) ax2.set_title(&#39;Grades distribution of students with severe consumption during weekends&#39;) sns.distplot(sober_grades, ax=ax1) sns.distplot(severe_drunk_grades, ax=ax2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a8e2e188&gt; . However, even heavy alcohol consumption during the weekend has little impact on student grades. . Converting alcohol consumption to a categorical label . The original label goes from 1 to 5 from no consumption to severe consumption. It makes more sense to try to predict the weekly consumption of students so we combined the two attributes by summing them. . The fourth values seems like a good threshold to create two categorical classes: . $[2, 3]$ little alcohol consumption | $[4, 5]$ moderate alcohol consumption | $[6, 10]$ severe alcohol consumption | . sns.countplot(students[&#39;alc&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b1a7f9ae88&gt; . # Converting weekly consumption to classes students.loc[students[&#39;alc&#39;] &lt;= 3, &#39;alc&#39;] = 0 # little students.loc[(students[&#39;alc&#39;] &gt; 3) &amp; (students[&#39;alc&#39;] &lt;= 5), &#39;alc&#39;] = 1 # moderate students.loc[students[&#39;alc&#39;] &gt; 5, &#39;alc&#39;] = 2 # severe # We need to distinguish categorical from numeric values to fit different distribution # when we will fit the model numeric_cols = [&#39;age&#39;, &#39;traveltime&#39;, &#39;studytime&#39;, &#39;failures&#39;, &#39;famrel&#39;, &#39;freetime&#39;, &#39;goout&#39;, &#39;health&#39;, &#39;absences&#39;, &#39;grade&#39;, &#39;alc&#39;] students = students.drop(columns=[&#39;Walc&#39;, &#39;Dalc&#39;]) is_categorical = [] for col in students.columns: if col in numeric_cols: is_categorical.append(0) else: is_categorical.append(1) # Convert data to torch tensor X = torch.from_numpy(students.iloc[:, :-1].values).float() y = torch.from_numpy(students.iloc[:, -1].values).float() . Naive Bayes Classifier . Considering a vector of discrete values $ boldsymbol{x} in {1, dots, K }^D$, where $K$ is the number of samples and $D$ the number of features. The naive bayes classifier assumes that the data is conditionally independant given the class label i.e $p( boldsymbol{x}|y=c)$. This assumption allows us to write the class conditional density as a product: $$ large p( boldsymbol{x} | y = c, boldsymbol{ theta}) = prod_{i=1}^{K}p(x_i | y_i = c, boldsymbol{ theta}_{ic}) $$ The model is called “naive” since we do not expect the features to be independent, even conditional on the class label. . Assuming the bayes theorem: $$ large P(A|B) = frac{P(A)P(B|A)}{P(B)} $$ To have an easier understanding of the relation between the training of a model and the bayes theorem, let&#39;s reformulate this equation in terms of class and samples: $$ large P( text{class}| text{sample}) = frac{P( text{class})P( text{sample}| text{class})}{P( text{sample})} $$ Given a new sample, we want to predict its class. We will compute $P( text{sample}| text{class})P( text{sample})$ according to the training data. When predicting, we utilize the following approximation: $$ large P(c_j|x_i) sim P(c_j)P(x_i|c_j) dots P(x_D|c_j) $$ . In other words, for each potential class, we multiply the probability of the class (prior) with the probability of finding each features of $x_i$ in each class $c_j$ (posterior). . For categorical or binay features, we group the training samples according to each class The form of the class-conditional density depends on the type of each feature. We give some possibilities below: . For real values, we can use the Gaussian distribution: $$ large p( boldsymbol{x} | y = c, boldsymbol{ theta}) = prod_{i=1}^{D} mathcal{N}(x_i| mu_{ic}, sigma_{ic}^2) $$ | For binary values, we can use a Bernouilli distribution, where $ mu_{ic}$ is the probability that feature $i$ occurs in class $c$: $$ large p( boldsymbol{x} | y = c, boldsymbol{ theta}) = prod_{i=1}^{D} text{Ber}(x_i | mu_{ic}) $$ | For categorical features, we can use a Multinouilli distribution, where $ boldsymbol{ mu}_{ic}$ is an histogram over the possible values for $x_i$ in class $c$: $$ large p( boldsymbol{x} | y = c, boldsymbol{ theta}) = prod_{i=1}^{D} text{Cat}(x_i | boldsymbol{ mu}_{ic}) $$ | . These are the training steps: . group data according to the class label $c_i$ | compute the prior probability i.e $p(c_i)$ the proportion of samples inside each class $c_i$ of the whole training set | for each feature: if the feature is categorical, compute $p( boldsymbol{x_j} | c_i)$ for $j = 1, dots, D$ and $i = 1, dots, C$: for each possible values of this feature in the training samples of class $c_i$, compute the probability that this feature appears in class $c_i$ | . | if the feature is continuous, compute $p( boldsymbol{x_j} | c_i)$ for $j = 1, dots, D$ and $i = 1, dots, C$: compute the mean $ mu$ and standard deviation $ sigma$ of the training samples of class $c_i$ and fit a normal distribution $ mathcal{N}( mu, sigma^2)$ | . | . | . To predict on a new samples: . for each class $c_i$, compute $p(c_i | x)$ as: multiply the prior of each class $p(c_i)$ by: | for each features $k$: if categorical, multiply by the probabilities calculated earlier $p( boldsymbol{x_k} | c_i)$ where $x_k$ is the value of the input on feature $k$. | if continuous, multiply by $ mathcal{N}(x_k | mu, sigma^2)$ the likelihood of the gaussian distribution given the input $x_k$ | . | . | return the highest probability $p(c_i | x)$ of all classes | . class NaiveBayesClassifier(BaseEstimator): &quot;&quot;&quot;Class for the naive bayes classifier Inherits from sklearn BaseEstimator class to use cross validation. Attributes: offset: An integer to increment the conditional probabilities in order to smooth probabilities to avoid that a posterior probability be 0. is_categorical: A list containing 0 and 1 for indicating if a feature is categorical or numerical. nb_features: An integer for the numbers of feature of the data. nb_class: An integer for the number of classes in the labels. class_probs: A torch tensor for the proportion of each class. cond_probs: A torch tensor for the conditional probability of having a given value on a certain feature in the population of each class. &quot;&quot;&quot; def __init__(self, offset=1): &quot;&quot;&quot;Init function for the naive bayes class&quot;&quot;&quot; self.offset = offset def fit(self, X, y, **kwargs): &quot;&quot;&quot;Fits the model given data and labels as input Args: X: A torch tensor for the data. y: A torch tensor for the labels. &quot;&quot;&quot; # It is mandatory to pass a list describing if each feature is categorical or numerical if &#39;is_categorical&#39; not in kwargs: raise ValueError(&#39;must pass &#39;is_categorical &#39; to fit through **kwargs&#39;) self.is_categorical = kwargs[&#39;is_categorical&#39;] size = X.shape[0] self.nb_features = X.shape[1] y_uvals = y.unique() self.nb_class = len(y_uvals) # Probability of each class in the training set self.class_probs = y.int().bincount().float() / size features_maxvals = torch.zeros((self.nb_features,), dtype=torch.int32) for j in range(self.nb_features): features_maxvals[j] = X[:, j].max() # All the posterior probabilites cond_probs = [] for i in range(self.nb_class): cond_probs.append([]) # Group samples by class idx = torch.where(y == y_uvals[i])[0] elts = X[idx] size_class = elts.shape[0] for j in range(self.nb_features): cond_probs[i].append([]) if self.is_categorical[j]: # If categorical # For each features for k in range(features_maxvals[j] + 1): # Count the number of occurence of each value in this feature given the group class # Divided by the number of samples in the class p_x_k = (torch.where(elts[:, j] == k)[0].shape[0] + self.offset) / size_class # Append to posteriors probabilities cond_probs[i][j].append(p_x_k) else: # If numerical features_class = elts[:, j] # Compute mean and std mean = features_class.mean() std = (features_class - mean).pow(2).mean().sqrt() # Store these value to use them for the gaussian likelihood cond_probs[i][j] = [mean, std] self.cond_probs = cond_probs return 0 def gaussian_likelihood(self, X, mean, std): &quot;&quot;&quot;Computes the gaussian likelihood Args: X: A torch tensor for the data. mean: A float for the mean of the gaussian. std: A flot for the standard deviation of the gaussian. &quot;&quot;&quot; return (1 / (2 * math.pi * std.pow(2))) * torch.exp(-0.5 * ((X - mean) / std).pow(2)) def predict(self, X): &quot;&quot;&quot;Predicts labels given an input Args: X: A torch tensor containing a batch of data. &quot;&quot;&quot; if len(X.shape) == 1: X = X.unsqueeze(0) nb_samples = X.shape[0] pred_probs = torch.zeros((nb_samples, self.nb_class), dtype=torch.float32) for k in range(nb_samples): elt = X[k] for i in range(self.nb_class): # Set probability by the prior (class probability) pred_probs[k][i] = self.class_probs[i] prob_feature_per_class = self.cond_probs[i] for j in range(self.nb_features): if self.is_categorical[j]: # If categorical get the probability of drawing the value of the input on feature j # inside class i pred_probs[k][i] *= prob_feature_per_class[j][elt[j].int()] else: # If numerical, multiply by the gaussian likelihood with parameters # mean and std of the class i on feature j mean, std = prob_feature_per_class[j] pred_probs[k][i] *= self.gaussian_likelihood(elt[j], mean, std) # Get to highest probability among all classes return pred_probs.argmax(dim=1) . nbc = NaiveBayesClassifier() fit_params = {&#39;is_categorical&#39; : is_categorical} cross_val_score(nbc, X, y, cv=5, scoring=&#39;accuracy&#39;, fit_params=fit_params).mean() . 0.5689409274935591 . Conclusion . Even if the naive bayes model makes a strong assumption that the features are conditionaly independant given the class label, it achieved almost 57% accuracy on three output classes. This model does not perform as well as the more sophisticated models but it is very fast and suited as a baseline model for most classification tasks. On the other hand, naive bayes models can be descent predictors but they are considered as bad estimators i.e the output probabilities are not to be taken seriously. The naive bayes technique is performing better than logistic regression on small datasets, whereas it is the opposite for large datasets. .",
            "url": "https://consciousml.github.io/blog/naive-bayes/alcohol/pytorch/eda/from-scratch/2020/09/24/Naive-Bayes-Classifier.html",
            "relUrl": "/naive-bayes/alcohol/pytorch/eda/from-scratch/2020/09/24/Naive-Bayes-Classifier.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "K-Nearest Neighbors from scratch",
            "content": ". During this experiment, we will train a K-nearest neighbors model on physicochemical data to predict the quality of a red or white wines. The KNN will be implemented from scratch using Pytorch along with clear explanation of how the model works. Exploratory data analysis techniques are also discussed in the last section of the notebook. . import os import sys sys.path.append(&#39;..&#39;) import torch import numpy as np import pandas as pd import seaborn as sns import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder . Read csv using Pandas . wine_red = pd.read_csv(os.path.join(&#39;data&#39;, &#39;winequality-red.csv&#39;), sep=&#39;;&#39;) wine_white = pd.read_csv(os.path.join(&#39;data&#39;, &#39;winequality-white.csv&#39;), sep=&#39;;&#39;) . wine_red.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11.0 | 34.0 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . 1 7.8 | 0.88 | 0.00 | 2.6 | 0.098 | 25.0 | 67.0 | 0.9968 | 3.20 | 0.68 | 9.8 | 5 | . 2 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15.0 | 54.0 | 0.9970 | 3.26 | 0.65 | 9.8 | 5 | . 3 11.2 | 0.28 | 0.56 | 1.9 | 0.075 | 17.0 | 60.0 | 0.9980 | 3.16 | 0.58 | 9.8 | 6 | . 4 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11.0 | 34.0 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . wine_white.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.0 | 0.27 | 0.36 | 20.7 | 0.045 | 45.0 | 170.0 | 1.0010 | 3.00 | 0.45 | 8.8 | 6 | . 1 6.3 | 0.30 | 0.34 | 1.6 | 0.049 | 14.0 | 132.0 | 0.9940 | 3.30 | 0.49 | 9.5 | 6 | . 2 8.1 | 0.28 | 0.40 | 6.9 | 0.050 | 30.0 | 97.0 | 0.9951 | 3.26 | 0.44 | 10.1 | 6 | . 3 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . 4 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . Merge red and white data . Both dataframes have the same labels, so we can easily concatenate them. . wines = pd.concat([wine_white, wine_red]) . Visualize correlation heatmap . plt.figure(figsize=(16, 8)) wines_corr = wines.corr() sns.heatmap(wines_corr, annot=True, square=True, fmt=&#39;0.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2969e19ed08&gt; . The features most correlated with the quality are: . alcohol | density | volatile acidity | . Most of the features are very highly correlated to each other. We will try to select the best features to avoid redundancies using backward elimination further in the notebook. . Convert quality label to categorical values . sns.barplot(wines[&#39;quality&#39;], wines[&#39;alcohol&#39;], palette=&#39;hot&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2969ef0e448&gt; . Wines between 7 and 9 quality have a higher alcohol values, we will choose this range for the good wines. Thus, it looks appropriate to divide the quality label into low, medium and good wines. . tmp = wines[&#39;quality&#39;].values quality_type = np.empty(tmp.shape, dtype=&#39;U6&#39;) quality_type[tmp &lt; 5] = &#39;bad&#39; quality_type[(4 &lt; tmp) &amp; (tmp &lt; 7)] = &#39;medium&#39; quality_type[6 &lt; tmp] = &#39;good&#39; quality_type = pd.DataFrame(columns=[&#39;quality&#39;], data=quality_type) wines = wines.drop(&#39;quality&#39;, axis=1) wines = pd.concat([wines.reset_index(drop=True), quality_type.reset_index(drop=True)], axis=1) sns.countplot(wines.iloc[:, -1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2969e3b9f08&gt; . Isolate features from the target . Feature scaling is mandatory when using a K-nearest neighbors model. The KNN algorithm has a distance decision based approach to regress or classify an input. In fact, if some features have higher values than other, they will contribute more in the overall distance calculation thus biasing the outcome. . X = wines.iloc[:, :-1].to_numpy() y = wines.iloc[:, -1].to_numpy() . Convert category to integer . y = LabelEncoder().fit_transform(y) . Split data in train test . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Convert data to pytorch tensor X_train = torch.from_numpy(X_train) X_test = torch.from_numpy(X_test) y_train = torch.from_numpy(y_train) y_test = torch.from_numpy(y_test) . We will use Pytorch for this experiment . K-Nearest Neighbors . The K-nearest neighbors classifier looks at the $K^{th}$ closest points in the training set of an input $x$ counts how many members of each class are present and returns the empirical fraction as the estimate. $$ large p( mathbf{y} = c| boldsymbol{ mathbf{x}}, D, K) = frac{1}{K} sum_{i in N_K( boldsymbol{ mathbf{x}}, D)} mathbb{I}(y_i = c)$$ $$ large mathbb{I}(e) = begin{cases} 1 &amp; text{if} ;e ; text{is true} 0 &amp; text{otherwise} end{cases} $$ where $c$ is the class and $N_K$ are the closest sample of $ boldsymbol{ text{x}}$ in $D$. . def knn(sample, X, y, k_neighbors=10): &quot;&quot;&quot;Instance of K-Nearest Neigbhors model Args: X: A torch tensor for the data. y: A torch tensor for the labels. k_neighbors: An integer for the number of nearest neighbors to consider. &quot;&quot;&quot; sample = sample.unsqueeze(1).T # Compute the distance with the train set dist = (X - sample).pow(2).sum(axis=1).sqrt() # Sort the distances _, indices = torch.sort(dist) y = y[indices] # Get the Kth most similar samples and return the predominant class return y[:k_neighbors].bincount().argmax().item() . def train_knn(X_train, X_test, y_train, y_test, k_neighbors=1): &quot;&quot;&quot;Trains a K-Nearest Neigbhors model Args: X_train: A torch tensor for the training data. X_test: A torch tensor for the test data. y_train: A torch tensor for the training labels. y_test: A torch tensor for the test labels. k_neighbors: An integer for the number of nearest neighbors to consider. &quot;&quot;&quot; # Allocate space for the prediction y_pred_test = np.zeros(y_test.shape, dtype=np.uint8) y_pred_train = np.zeros(y_train.shape, dtype=np.uint8) X_train_c = X_train.clone() # Predict on each sample of the train and test for i in range(X_test.shape[0]): y_pred_test[i] = knn(X_test[i], X_train, y_train, k_neighbors=k_neighbors) y_pred_test = torch.from_numpy(y_pred_test).float() return y_pred_test . pred_test = train_knn(X_train, X_test, y_train, y_test, k_neighbors=1) print(classification_report(y_test, pred_test)) # Save data for future visualization X_train_viz = X_train X_test_viz = X_test y_train_viz = y_train y_test_viz = y_test . precision recall f1-score support 0 0.39 0.17 0.24 53 1 0.65 0.67 0.66 252 2 0.88 0.90 0.89 995 accuracy 0.83 1300 macro avg 0.64 0.58 0.60 1300 weighted avg 0.82 0.83 0.82 1300 . The KNN achieves a good performance compared to its simplicity. . Exploratory Data Analysis . Let&#39;s try to discard some features based on p-value but to see if it improves the results. More on p-value here: https://www.statsdirect.com/help/basics/p_values.htm#:~:text=The%20P%20value%2C%20or%20calculated,the%20hypothesis%20is%20being%20tested. . columns = wines.columns[:-1] while len(columns) &gt; 0: pvalues = [] X_1 = wines[columns] X_1 = sm.add_constant(X_1) model = sm.OLS(y, X_1).fit() pvalues = pd.Series(model.pvalues[1:], index=columns) max_idx = np.argmax(pvalues) max_pval = pvalues[max_idx] if max_pval &gt; 0.10: columns = columns.drop(columns[max_idx]) print(&#39;Dropping column &#39; + columns[max_idx] + &#39;, pvalue is: &#39; + str(max_pval)) else: break . Dropping column residual sugar, pvalue is: 0.6594184624811075 Dropping column total sulfur dioxide, pvalue is: 0.40799376734207193 Dropping column total sulfur dioxide, pvalue is: 0.16949863758492337 . columns . Index([&#39;fixed acidity&#39;, &#39;volatile acidity&#39;, &#39;residual sugar&#39;, &#39;total sulfur dioxide&#39;, &#39;density&#39;, &#39;pH&#39;, &#39;sulphates&#39;, &#39;alcohol&#39;], dtype=&#39;object&#39;) . X = wines[columns].to_numpy() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) X_train = torch.from_numpy(X_train) X_test = torch.from_numpy(X_test) y_train = torch.from_numpy(y_train) y_test = torch.from_numpy(y_test) y_pred = train_knn(X_train, X_test, y_train, y_test, k_neighbors=1) print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.14 0.14 0.14 58 1 0.53 0.53 0.53 308 2 0.86 0.86 0.86 1259 accuracy 0.77 1625 macro avg 0.51 0.51 0.51 1625 weighted avg 0.77 0.77 0.77 1625 . Feature selection based on p-value made the performance worse. From 83% to 77%. We tested with a value $k in [1, 10]$ and the best result is given $k = 1$. . Feature Selection by hand . Let&#39;s try to select all the feature that has more than $0.10$ correlation with the target. . columns = [&#39;alcohol&#39;, &#39;density&#39;, &#39;chlorides&#39;, &#39;volatile acidity&#39;] X = wines[columns].to_numpy() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) X_train = torch.from_numpy(X_train) X_test = torch.from_numpy(X_test) y_train = torch.from_numpy(y_train) y_test = torch.from_numpy(y_test) y_pred = train_knn(X_train, X_test, y_train, y_test, k_neighbors=1) print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.19 0.17 0.18 58 1 0.51 0.51 0.51 308 2 0.85 0.86 0.85 1259 accuracy 0.77 1625 macro avg 0.52 0.51 0.51 1625 weighted avg 0.76 0.77 0.77 1625 . Same result as backward elimination. Sometimes all the features are relevant for a given model. . Conclusion . The K-nearest neighbors algorithm is a simple but rather effective approach in some contexts. The KNN method has no training step which is very handy when we have an increasing amount of data. In fact, it is not required to train the KNN model. However, KNN is not a model to pick when the data has a high dimentionality. Computing the neighbor distances accross a large number of dimension is not effective. This phenomena is also called the &quot;curse of dimensionality&quot;. The KNN model is also uneffective when dealing with outliers. .",
            "url": "https://consciousml.github.io/blog/knn/wine-quality/pytorch/eda/from-scratch/2020/09/23/K-Nearest-Neigbors.html",
            "relUrl": "/knn/wine-quality/pytorch/eda/from-scratch/2020/09/23/K-Nearest-Neigbors.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Polynomial Regression from scratch",
            "content": ". During this experiment, we will see how to implement polynomial regression from scratch using Pytorch. . import torch import numpy as np import matplotlib.pyplot as plt . Generate a polynomial distribution with random noise . # Function for creating a vector with value between [r1, r2] def randvec(r1, r2, shape): return (r1 - r2) * torch.rand(shape) + r2 # Defining the range of our distribution X = torch.tensor([i for i in range(-30, 30)]).float() # Creating random points from a gaussian with random noise y = randvec(-1e4, 1e4, X.shape) - (1/2) * X + 3 * X.pow(2) - (6/4) * X.pow(3) plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x23a8500ea88&gt; . Create the polynomial features . The formula of linear regression is as follow: $$ boldsymbol{ hat{y}} = boldsymbol{X} boldsymbol{w} $$ where $ boldsymbol{ hat{y}}$ is the target, $ boldsymbol{w}$ are the weights learned by the model and $ boldsymbol{X}$ is training data. Polynomial regression is still considered as a linear regression because there is only linear learning parameters: $$ boldsymbol{y} = boldsymbol{w}_0 + boldsymbol{X} boldsymbol{w}_1 + boldsymbol{X}^2 boldsymbol{w}_2 + dots + boldsymbol{X}^n boldsymbol{w}_n $$ As you have probably guessed, this equation is not linear. We use a trick to make it linear: . We gather all the $ boldsymbol{X}^2$ to $ boldsymbol{X}^n$ as new features that we created and we concatenate them to $ boldsymbol{X}$. | All the $ boldsymbol{w}_1$ to $ boldsymbol{w}_n$ are concatenated to $ boldsymbol{w}_0$. | . At the end, the polynomial regression has the same formula as the linear regression but with the aggregated arrays. . def create_features(X, degree=2, standardize=True): &quot;&quot;&quot;Creates the polynomial features Args: X: A torch tensor for the data. degree: A intege for the degree of the generated polynomial function. standardize: A boolean for scaling the data or not. &quot;&quot;&quot; if len(X.shape) == 1: X = X.unsqueeze(1) # Concatenate a column of ones to has the bias in X ones_col = torch.ones((X.shape[0], 1), dtype=torch.float32) X_d = torch.cat([ones_col, X], axis=1) for i in range(1, degree): X_pow = X.pow(i + 1) # If we use the gradient descent method, we need to # standardize the features to avoid exploding gradients if standardize: X_pow -= X_pow.mean() std = X_pow.std() if std != 0: X_pow /= std X_d = torch.cat([X_d, X_pow], axis=1) return X_d def predict(features, weights): return features.mm(weights) features = create_features(X, degree=3, standardize=False) y_true = y.unsqueeze(1) . Method 1: Normal equation . The first method is analytical and uses the normal equation. Training a linear model using least square regression is equivalent to minimize the mean squared error: . $$ begin{align} text{Mse}( hat{y}, y) &amp;= frac{1}{n} sum_{i=1}^{n}{|| hat{y}_i - y_i ||_{2}^{2}} text{Mse}( hat{y}, y) &amp;= frac{1}{n}|| boldsymbol{X} boldsymbol{w} - boldsymbol{y} ||_2^2 end{align} $$where $n$ is the number of samples, $ hat{y}$ is the predicted value of the model and $y$ is the true target. The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$. . Minimizing the $ text{Mse}$ can be achieved by solving the gradient of this equation equals to zero in regards to the weights $ boldsymbol{w}$: . $$ begin{align} nabla_{ boldsymbol{w}} text{Mse}( hat{y}, y) &amp;= 0 ( boldsymbol{X}^ top boldsymbol{X})^{-1} boldsymbol{X}^ top boldsymbol{y} &amp;= boldsymbol{w} end{align} $$For more information on how to find $ boldsymbol{w}$ please visit the section &quot;Linear Least Squares&quot; of this link. . def normal_equation(y_true, X): &quot;&quot;&quot;Computes the normal equation Args: y_true: A torch tensor for the labels. X: A torch tensor for the data. &quot;&quot;&quot; XTX_inv = (X.T.mm(X)).inverse() XTy = X.T.mm(y_true) weights = XTX_inv.mm(XTy) return weights weights = normal_equation(y_true, features) y_pred = predict(features, weights) plt.scatter(X, y) plt.plot(X, y_pred, c=&#39;red&#39;) . [&lt;matplotlib.lines.Line2D at 0x23a8689c448&gt;] . With the normal equation method, the polynomial regressor fits well the synthetic data. . Method 2: Gradient Descent . The Gradient descent method takes steps proportional to the negative of the gradient of a function at a given point, in order to iteratively minimize the objective function. The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of $f$ is the vector containing all of the partial derivatives, denoted $ nabla_{ boldsymbol{x}}f( boldsymbol{x})$. . The directional derivative in direction $ boldsymbol{u}$ (a unit vector) is the slope of the function $f$ in direction $ boldsymbol{u}$. In other words, the directional derivative is the derivative of the function $f( boldsymbol{x} + sigma boldsymbol{u})$ with respect to $ sigma$ close to 0. To minimize $f$, we would like to find the direction in which $f$ decreases the fastest. We can do this using the directional derivative: $$ begin{align} &amp; min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{ boldsymbol{u}^ top nabla_{ boldsymbol{x}} f( boldsymbol{x})} = &amp; min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{|| boldsymbol{u}||_2 || nabla_{ boldsymbol{x}}f( boldsymbol{x})||_2 cos theta} end{align} $$ ignoring factors that do not depend on $ boldsymbol{u}$, this simplifies to $ min_{u}{ cos theta}$. This is minimized when $ boldsymbol{u}$ points in the opposite direction as the gradient. Each step of the gradient descent method proposes a new points: $$ boldsymbol{x&#39;} = boldsymbol{x} - epsilon nabla_{ boldsymbol{x}}f( boldsymbol{x}) $$ where $ epsilon$ is the learning rate. In the context of polynomial regression, the gradient descent is as follow: $$ boldsymbol{w} = boldsymbol{w} - epsilon nabla_{ boldsymbol{w}} text{MSE} $$ where: $$ begin{align} nabla_{ boldsymbol{w}} text{MSE} &amp;= nabla_{ boldsymbol{w}} left( frac{1}{n}{|| boldsymbol{X} boldsymbol{w} - boldsymbol{y} ||_2^2} right) &amp;= frac{2}{N} boldsymbol{X}^ top( boldsymbol{X} boldsymbol{w} - boldsymbol{y}) end{align} $$ . def gradient_descent(X, y_true, lr=0.001, it=30000): &quot;&quot;&quot;Computes the gradient descent Args: X: A torch tensor for the data. y_true: A torch tensor for the labels. lr: A scalar for the learning rate. it: A scalar for the number of iteration or number of gradient descent steps. &quot;&quot;&quot; weights_gd = torch.ones((X.shape[1], 1)) n = X.shape[0] fact = 2 / n for _ in range(it): y_pred = predict(X, weights_gd) grad = fact * X.T.mm(y_pred - y_true) weights_gd -= lr * grad return weights_gd features = create_features(X, degree=3, standardize=True) weights_gd = gradient_descent(features, y_true) pred_gd = predict(features, weights_gd) . The mean squared error is even lower when using gradient descent. . plt.scatter(X, y) plt.plot(X, pred_gd, c=&#39;red&#39;) . [&lt;matplotlib.lines.Line2D at 0x23a86812c48&gt;] . Conclusion . The polynomial regression is an appropriate example to learn more about the concept of normal equation and gradient descent. This method work well with data that has polynomial shapes but we need to choose the right polynomial degree for a good bias/variance trade-off. However, the polynomial regression method has an important drawback. In fact, it is necessary to transform the data to a higher dimensional space which can be unfeasable if the data is very large. .",
            "url": "https://consciousml.github.io/blog/polynomial-regression/pytorch/gradient-descent/from-scratch/2020/09/22/Polynomial-Regression.html",
            "relUrl": "/polynomial-regression/pytorch/gradient-descent/from-scratch/2020/09/22/Polynomial-Regression.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Logistic Regression from scratch",
            "content": ". During this experiment, we will train logistic regression on diabetes data, from scratch using Pytorch. The Pima Indians Diabetes Database has been gathered by the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of this dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. This dataset contains the following features: . Pregnancies | Glucose | BloodPressure | SkinThickness | Insuline | BMI | DiabetesPedigreeFunction | Age | Outcome (has diabetes or not) | . import os import torch import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split . Read csv using Pandas . diabetes = pd.read_csv(os.path.join(&#39;data&#39;, &#39;diabetes.csv&#39;)) diabetes.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . diabetes.describe() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . count 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | . mean 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578 | 0.471876 | 33.240885 | 0.348958 | . std 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160 | 0.331329 | 11.760232 | 0.476951 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.078000 | 21.000000 | 0.000000 | . 25% 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000 | 0.243750 | 24.000000 | 0.000000 | . 50% 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000 | 0.372500 | 29.000000 | 0.000000 | . 75% 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000 | 0.626250 | 41.000000 | 1.000000 | . max 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 | 67.100000 | 2.420000 | 81.000000 | 1.000000 | . On this sample of data, the standard devitation of the columns looks reasonably high except for the DiabetesPedigree but it is acceptable because the mean is relatively low. A feature having low std is likely to provide close to no information to the model. Which is not the case here. . sns.countplot(x=&#39;Outcome&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x14ccf75a2c8&gt; . The target distribution is very unbalanced with two times more negative than positives. . Heatmap Correlation . plt.figure(figsize=(20, 5)) sns.heatmap(diabetes.corr(), annot=True, square=True, fmt=&#39;0.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x14ccf67da48&gt; . The glucose level, BMI, age and number of pregnancies are highly correlated with the outcome. Suprisingly, the insulin level is not very correlated with the outcome. Most likely because the insulin is correlated with the glucose and the glucose has 0.47 correlation with the target. . plt.figure(figsize=(15, 3)) sns.boxplot(x=&#39;Glucose&#39;, y=&#39;Outcome&#39;, orient=&#39;h&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x14ccf2508c8&gt; . For patient with diabetes, the glucose level is significantly higher. In other words, a patient with high glucose level is very likely to have diabetes. . plt.figure(figsize=(15, 3)) sns.boxplot(x=&#39;Pregnancies&#39;, y=&#39;Outcome&#39;, orient=&#39;h&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x14ccdeb66c8&gt; . Surprisingly, the number of pregnancies is correlated with diabetes. . Convert data to Torch tensors . X = diabetes.iloc[:, :-1].values y = torch.from_numpy(diabetes.iloc[:, -1].values).float().unsqueeze(1) # Standardize the data X = StandardScaler().fit_transform(X) X = torch.from_numpy(X).float() # Add column of ones for the bias ones_col = torch.ones((X.shape[0], 1), dtype=torch.float32) X = torch.cat([ones_col, X], axis=1) . Split data into train and test . X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25) . Train Logistic Regression . The prediction of a logistic model is as follow: $$ large hat{y} = sigma( boldsymbol{X} boldsymbol{w}) $$ Where $ sigma$ is the sigmoid or logit function: $$ large sigma( boldsymbol{x}) = frac{1}{1 + exp(-x)} $$ The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$ given as input to the logit function. The sigmoid function is used here because it squashes the input in the $[0, 1]$ range suitable for describing a Bernouilli distribution. Where the Bernouilli cases are: . the patient has diabetes with $p$ probability | the patient does not have diabetes with $1 - p$ probability | . It is important to note that the bias is included in $ boldsymbol{X}$ as a column of ones. . Training a classification model can be expressed as maximizing the likelihood of the observed data. In other words, we want the predicted probability of our model that a patient has diabetes to be as close as the true probability of the data. In practice, maximizing the likelihood is equivalent to minimize the negative log likelihood: $$ large L( boldsymbol{ theta}) = - frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) $$ Because we dealing with a binary target, it is appropriate to use the binary cross entropy: $$ large L( boldsymbol{ theta}) = - frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) + (1 - boldsymbol{y_i}) log(1 - hat{ boldsymbol{y}}_i) $$ . Gradient Descent for Logistic Regression . We will use Gradient Descent to train the logistic regression model. The Gradient descent method takes steps proportional to the negative of the gradient of a function at a given point, in order to iteratively minimize the objective function. The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of $f$ is the vector containing all of the partial derivatives, denoted $ nabla_{ boldsymbol{x}}f( boldsymbol{x})$. . The directional derivative in direction $ boldsymbol{u}$ (a unit vector) is the slope of the function $f$ in direction $ boldsymbol{u}$. In other words, the directional derivative is the derivative of the function $f( boldsymbol{x} + sigma boldsymbol{u})$ with respect to $ sigma$ close to 0. To minimize $f$, we would like to find the direction in which $f$ decreases the fastest. We can do this using the directional derivative: $$ large min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{ boldsymbol{u}^ top nabla_{ boldsymbol{x}} f( boldsymbol{x})} $$ $$ large = min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{|| boldsymbol{u}||_2 || nabla_{ boldsymbol{x}}f( boldsymbol{x})||_2 cos theta}$$ ignoring factors that do not depend on $ boldsymbol{u}$, this simplifies to $ min_{u}{ cos theta}$. This is minimized when $ boldsymbol{u}$ points in the opposite direction as the gradient. Each step of the gradient descent method proposes a new point: $$ large boldsymbol{x&#39;} = boldsymbol{x} - epsilon nabla_{ boldsymbol{x}}f( boldsymbol{x}) $$ where $ epsilon$ is the learning rate. In the context of logistic regression, the gradient descent is as follow: $$ large boldsymbol{w} = boldsymbol{w} - epsilon nabla_{ boldsymbol{w}}L( boldsymbol{ theta}) $$ where: $$ large nabla_{ boldsymbol{w}}L( boldsymbol{ theta}) = nabla_{ boldsymbol{w}} left(- frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) + (1 - boldsymbol{y_i}) log(1 - hat{ boldsymbol{y}}_i) right) $$ $$ large = frac{1}{N} boldsymbol{X}^ top( sigma( boldsymbol{X} boldsymbol{w}) - boldsymbol{y}) $$ Here is a nice explanation of how to find the gradient of the binary cross entropy: https://www.youtube.com/watch?v=hWLdFMccpTY . def sigmoid(x): &quot;&quot;&quot;Sigmoid function that squashes the input between 0 and 1&quot;&quot;&quot; return 1 / (1 + torch.exp(-x)) def predict(X, weights): &quot;&quot;&quot;Pedicts the class given the data and the weights Args: X: A torch tensor for the input data. weights: A torch tensor for the parameters calculated during the training of the Logistic regression. &quot;&quot;&quot; return sigmoid(X.mm(weights)) . def binary_cross_entropy(y_true, y_pred): &quot;&quot;&quot;Loss function for the training of the logistic regression We add an epsilon inside the log functions to avoid Nans. Args: y_true: A torch tensor for the labels of the data. y_pred: A torch tensor for the values predicted by the model. &quot;&quot;&quot; fact = 1 / y_true.shape[0] return -fact * (y_true * torch.log(y_pred + 1e-10) + (1 - y_true) * torch.log(1 - y_pred + 1e-10 )).sum() . def train_logit_reg(X, y_true, weights, lr=0.001, it=2000): &quot;&quot;&quot;Trains the logistic regression model Args: X: A torch tensor for the training data. y: A torch tensor for the labels of the data. weights: A torch tensor for the learning parameters of the model. lr: A scalar describing the learning rate for the gradient descent. it: A scalar for the number of steps in the gradient descent. &quot;&quot;&quot; for _ in range(it): y_pred = predict(X, weights) err = (y_pred - y_true) grad = X.T.mm(err) weights -= lr * grad bn_train = binary_cross_entropy(y_true, y_pred).item() return weights, bn_train . # Training the model weights = torch.ones((X.shape[1], 1), dtype=torch.float32) weights, bn_train = train_logit_reg(X_train, y_train, weights) y_pred = predict(X_test, weights) print(&#39;Binary cross-entropy on the train set:&#39;, bn_train) . Binary cross-entropy on the train set: 0.4595394730567932 . Binary cross entropy on test set . # Test the model bn_test = binary_cross_entropy(y_test, y_pred).item() print(&#39;Binary cross-entropy on the test set:&#39;, bn_test) . Binary cross-entropy on the test set: 0.5321948528289795 . Accuracy on test set . To compute the accuracy, we have to find the best threshold to convert our probability output in binary values. . def get_binary_pred(y_true, y_pred): &quot;&quot;&quot;Finds the best threshold based on the prediction and the labels Args: y_true: A torch tensor for the labels of the data. y_pred: A torch tensor for the values predicted by the model. &quot;&quot;&quot; y_pred_thr = y_pred.clone() accs = [] thrs = [] for thr in np.arange(0, 1, 0.01): y_pred_thr[y_pred &gt;= thr] = 1 y_pred_thr[y_pred &lt; thr] = 0 cur_acc = classification_report(y_test, y_pred_thr, output_dict=True)[&#39;accuracy&#39;] accs.append(cur_acc) thrs.append(thr) accs = torch.FloatTensor(accs) thrs = torch.FloatTensor(thrs) idx = accs.argmax() best_thr = thrs[idx].item() best_acc = accs[idx].item() y_pred[y_pred &gt;= best_thr] = 1 y_pred[y_pred &lt; best_thr] = 0 return y_pred . import sklearn y_pred = get_binary_pred(y_test, y_pred) print(classification_report(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) . precision recall f1-score support 0.0 0.79 0.90 0.84 123 1.0 0.76 0.57 0.65 69 accuracy 0.78 192 macro avg 0.78 0.73 0.75 192 weighted avg 0.78 0.78 0.77 192 [[111 12] [ 30 39]] . With a threshold of 0.66, we achieve an accuracy of 78% which is quite good for a linear model. . Polynomial Logistic Regression . In this section, we will add some polynomial features to the logistic regressor. It is the same principle as the logistic regression except that $ boldsymbol{X}$ is the concatenation of $ boldsymbol{X_1} dots boldsymbol{X_m}$ where $m$ is the degree of the polynomial function and $ boldsymbol{w}$ is the concatenation of $ boldsymbol{w_1} dots boldsymbol{w_m}$ such as: $$ large boldsymbol{y} = sigma( boldsymbol{w}_0 + boldsymbol{X} boldsymbol{w}_1 + boldsymbol{X}^2 boldsymbol{w}_2 + dots + boldsymbol{X}^m boldsymbol{w}_m) $$ This method is still linear because predicting $ hat{y} = sigma( boldsymbol{X} boldsymbol{w})$ is still linear in the parameters. . X = torch.from_numpy(diabetes.iloc[:, :-1].values).float() y = torch.from_numpy(diabetes.iloc[:, -1].values).float().unsqueeze(1) . def create_poly_features(X, degree=2): &quot;&quot;&quot;Creates the augmented features for the polynomial model This function concatenates the augmented data into a single torch tensor. Args: X: A torch tensor for the data. degree: A integer for the degree of the polynomial function that we model. &quot;&quot;&quot; if len(X.shape) == 1: X = X.unsqueeze(1) ones_col = torch.ones((X.shape[0], 1)) # Standardize the output to avoid exploding gradients X_cat = X.clone() X_cat = (X_cat - X_cat.mean()) / X_cat.std() X_cat = torch.cat([ones_col, X_cat], axis=1) for i in range(1, degree): X_p = X.pow(i + 1) X_p = torch.from_numpy(StandardScaler().fit_transform(X_p)).float() X_cat = torch.cat([X_cat, X_p], axis=1) return X_cat def create_weights(features): &quot;&quot;&quot;Creates a column of ones&quot;&quot;&quot; return torch.ones((features.shape[1], 1), dtype=torch.float32) . features = create_poly_features(X, degree=2) X_train, X_test, y_train, y_test = train_test_split(features, y, random_state=42, test_size=0.25) weights = create_weights(X_train) weights, bn_train = train_logit_reg(X_train, y_train, weights, it=10000) y_pred = predict(X_test, weights) print(&#39;Binary cross-entropy on the train set:&#39;, bn_train) . Binary cross-entropy on the train set: 0.44449102878570557 . binary_cross_entropy(y_test, y_pred).item() . 0.5331151485443115 . y_pred = get_binary_pred(y_test, y_pred) print(classification_report(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) . precision recall f1-score support 0.0 0.75 0.95 0.84 123 1.0 0.84 0.45 0.58 69 accuracy 0.77 192 macro avg 0.80 0.70 0.71 192 weighted avg 0.78 0.77 0.75 192 [[117 6] [ 38 31]] . Conclusion . The polynomial logistic regression model overfitted compared to the classic logistic regression model because it lost 1% accuracy on the test set. For some very highly correlated data, logistic regression without polynomial features has better performance than with polynomial features. Logistic regression is a very simple and interpretable model suited as a baseline in most classification problems. However, it does not perform well when the feature space is large. In fact, it is difficult to compute feature transformation (such as polynomials) when the data doesn&#39;t fit in ram. .",
            "url": "https://consciousml.github.io/blog/logistic-regression/polynomial-regression/diabetes/pytorch/from-scratch/2020/09/17/Logistic-Regression.html",
            "relUrl": "/logistic-regression/polynomial-regression/diabetes/pytorch/from-scratch/2020/09/17/Logistic-Regression.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Multivariate Linear Regression from scratch",
            "content": ". The CarDekho dataset contains information about used cars listed on the website of the same name. This dataset has 301 unique entities with the following features: . car name | year of release | selling price | present price | kilometers driven | fuel: such as petrol or diesel | transmission: such as manual or automatic | owner: how many times the car changed owner | . The goal of this experiment is to train a linear model to predict the selling price of a car. We will use the framework Pytorch for the matrix calculus. . import os import sys import torch import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split . Read data from csv using Pandas . import os import sys import torch import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split . df = pd.read_csv(os.path.join(&#39;data&#39;, &#39;car_data.csv&#39;)) df.head() . Car_Name Year Selling_Price Present_Price Kms_Driven Fuel_Type Seller_Type Transmission Owner . 0 ritz | 2014 | 3.35 | 5.59 | 27000 | Petrol | Dealer | Manual | 0 | . 1 sx4 | 2013 | 4.75 | 9.54 | 43000 | Diesel | Dealer | Manual | 0 | . 2 ciaz | 2017 | 7.25 | 9.85 | 6900 | Petrol | Dealer | Manual | 0 | . 3 wagon r | 2011 | 2.85 | 4.15 | 5200 | Petrol | Dealer | Manual | 0 | . 4 swift | 2014 | 4.60 | 6.87 | 42450 | Diesel | Dealer | Manual | 0 | . Convert categorical variable into indicator variables . f_continuous = df[[&#39;Year&#39;, &#39;Selling_Price&#39;, &#39;Present_Price&#39;, &#39;Kms_Driven&#39;, &#39;Owner&#39;]] f_categorical = pd.get_dummies(df[[&#39;Fuel_Type&#39;, &#39;Seller_Type&#39;, &#39;Transmission&#39;]]) df = pd.concat([f_continuous, f_categorical], axis=1) # Drop refundant features df.drop([&#39;Transmission_Automatic&#39;, &#39;Seller_Type_Dealer&#39;, &#39;Fuel_Type_CNG&#39;], axis=1, inplace=True) df.head() . Year Selling_Price Present_Price Kms_Driven Owner Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 0 2014 | 3.35 | 5.59 | 27000 | 0 | 0 | 1 | 0 | 1 | . 1 2013 | 4.75 | 9.54 | 43000 | 0 | 1 | 0 | 0 | 1 | . 2 2017 | 7.25 | 9.85 | 6900 | 0 | 0 | 1 | 0 | 1 | . 3 2011 | 2.85 | 4.15 | 5200 | 0 | 0 | 1 | 0 | 1 | . 4 2014 | 4.60 | 6.87 | 42450 | 0 | 1 | 0 | 0 | 1 | . Visualize histogram of all features . df.hist(bins=14, color=&#39;steelblue&#39;, edgecolor=&#39;black&#39;, linewidth=1.0, xlabelsize=8, ylabelsize=8, grid=False) plt.tight_layout(rect=(0, 0, 1.2, 1.2)) . Most cars on sales are consuming petrol instead of diesel, have had only one owner, are from 2012-present, are manual and have a selling price between 1000 and 10000 dollars. . Heatmap correlation . plt.figure(figsize=(16, 8)) sns.heatmap(df.corr(), square= True, annot=True, fmt=&#39;.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1f5bb762b08&gt; . Most of the variables are highly correlated. As expected, the present price variable is the most correlated with the target selling price. . Pairwise Plots . cols_viz = [&#39;Kms_Driven&#39;, &#39;Year&#39;, &#39;Selling_Price&#39;, &#39;Present_Price&#39;] pp = sns.pairplot(df[cols_viz], height=1.8, aspect=1.8, plot_kws=dict(edgecolor=&quot;k&quot;, linewidth=0.5), diag_kind=&quot;kde&quot;, diag_kws=dict(shade=True)) fig = pp.fig fig.subplots_adjust(top=0.93, wspace=0.3) t = fig.suptitle(&#39;Wine Attributes Pairwise Plots&#39;, fontsize=14) . Most of the features are highly correlated to each other. Some outliers are present but as there is very few, we will keep them in the training set. The year feature have a polynomial correlation with the selling price. A polynomial regression will most likely overperform a linear regression. . Make train test split . # Separate the target from the dataFrame Y = df[&#39;Selling_Price&#39;] X = df.drop(&#39;Selling_Price&#39;, axis=1) # Convert data to Pytorch tensor X_t = torch.from_numpy(X.to_numpy()).float() Y_t = torch.from_numpy(Y.to_numpy()).float().unsqueeze(1) X_train, X_test, Y_train, Y_test = train_test_split(X_t, Y_t, test_size=0.33, random_state=42) . Train a multivariate linear regression . Training a linear model using least square regression is equivalent to minimize the mean squared error: . $$ begin{align} large text{Mse}( boldsymbol{ hat{y}}, boldsymbol{y}) &amp;= frac{1}{n} sum_{i=1}^{n}{|| hat{y}_i - y_i ||_{2}^{2}} &amp;= frac{1}{n}|| boldsymbol{X} boldsymbol{w} - boldsymbol{y} ||_2^2 end{align} $$where $n$ is the number of samples, $ hat{y}$ is the predicted value of the model and $y$ is the true target. The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$. . Minimizing the $ text{Mse}$ can be achieved by solving the gradient of this equation equals to zero in regards to the weights $ boldsymbol{w}$: . $$ begin{align} large nabla_{ boldsymbol{w}} text{Mse} &amp;= 0 large ( boldsymbol{X}^ top boldsymbol{X})^{-1} boldsymbol{X}^ top boldsymbol{y} &amp;= boldsymbol{w} end{align} $$For more information on how to find $ boldsymbol{w}$ please visit the following link. . def add_ones_col(X): &quot;&quot;&quot;Add a column a one to the input torch tensor&quot;&quot;&quot; x_0 = torch.ones((X.shape[0],), dtype=torch.float32).unsqueeze(1) X = torch.cat([x_0, X], dim=1) return X def multi_linear_reg(X, y): &quot;&quot;&quot;Multivariate linear regression function Args: X: A torch tensor for the data. y: A torch tensor for the labels. &quot;&quot;&quot; X = add_ones_col(X) # Add a column of ones to X to agregate the bias to the input matrices Xt_X = X.T.mm(X) Xt_y = X.T.mm(y) Xt_X_inv = Xt_X.inverse() w = Xt_X_inv.mm(Xt_y) return w def prediction(X, w): &quot;&quot;&quot;Predicts a selling price for each input Args: X: A torch tensor for the data. w: A torch tensor for the weights of the linear regression mode. &quot;&quot;&quot; X = add_ones_col(X) return X.mm(w) . # Fit the training set into the model to get the weights w = multi_linear_reg(X_train, Y_train) # Predict using matrix multiplication with the weights Y_pred_train = prediction(X_train, w) Y_pred_test = prediction(X_test, w) . Compute prediction error . def mse(Y_true, Y_pred): error = Y_pred - Y_true return (error.T.mm(error) / Y_pred.shape[0]).item() def mae(Y_true, Y_pred): error = Y_pred - Y_true return error.abs().mean().item() . mse_train = mse(Y_train, Y_pred_train) mae_train = mae(Y_train, Y_pred_train) print(&#39;MSE Train: t&#39;, mse_train) print(&#39;MAE Train: t&#39;, mae_train, end=&#39; n n&#39;) mse_test = mse(Y_test, Y_pred_test) mae_test = mae(Y_test, Y_pred_test) print(&#39;MSE Test: t&#39;, mse_test) print(&#39;MAE Test: t&#39;, mae_test, end=&#39; n n&#39;) . MSE Train: 2.808985471725464 MAE Train: 1.1321566104888916 MSE Test: 3.7205495834350586 MAE Test: 1.2941011190414429 . The model has an error of 1.29 on average on the training test. Not bad for a linear model, taking into consideration that the mean of the present price is 7.62. . Principal component analysis visualization . In this section, we will use PCA to reduce the number of feature to two, in order to visualize the plane of the linear regressor. . Suppose a collection of $m$ points $ { boldsymbol{x}^{(1)}, dots, boldsymbol{x}^{(m)} }$ in $ mathbb{R}^n$. The principal components analysis aims to reduce the dimensionality of the points while losing the least precision as possible. For each point $ boldsymbol{x}^{(i)} in mathbb{R}^n$ we will find a corresponding code vector $ boldsymbol{c}^{(i)} in mathbb{R}^l$ where $l$ is smaller than $n$. Let $f$ be the encoding function and $g$ be the decoding function and $ boldsymbol{D} in mathbb{R}^{n,l}$ is the decoding matrix whose columns are orthonormal: $$ begin{align} f( boldsymbol{x}) &amp;= boldsymbol{D}^ top boldsymbol{x} g(f( boldsymbol{x})) &amp;= boldsymbol{D} boldsymbol{D}^ top boldsymbol{x} end{align} $$ . def cov(X): &quot;&quot;&quot;Computes the covariance of the input The covariance matrix gives some sense of how much two values are linearly related to each other, as well as the scale of these variables. It is computed by (1 / (N - 1)) * (X - E[X]).T (X - E[X]). Args: X: A torch tensor as input. &quot;&quot;&quot; X -= X.mean(dim=0, keepdim=True) fact = 1.0 / (X.shape[0] - 1) cov = fact * X.T.mm(X) return cov def pca(X, target_dim=2): &quot;&quot;&quot;Computes the n^th first principal components of the input PCA can be implemented using the n^th principal components of the covariance matrix. We could have been using an eigen decomposition because the covariance matrix is always squared but singular value decomposition does also the trick if we take the right singular vectors and perform a matrix multiplication to the right. Args: X: A torch tensor as the input. target_dim: An integer for selecting the n^th first components. &quot;&quot;&quot; cov_x = cov(X) U, S, V = torch.svd(cov_x) transform_mat = V[:, :target_dim] X_reduced = X.mm(transform_mat) return X_reduced, transform_mat . X_test_pca, _ = pca(X_test, target_dim=2) X_train_pca, _ = pca(X_train, target_dim=2) points = torch.cat([X_test_pca[:3], Y_pred_test[:3]], axis=1) v1 = points[2, :] - points[0, :] v2 = points[1, :] - points[0, :] cp = torch.cross(v1, v2) a, b, c = cp d = cp.dot(points[2, :]) min_mesh_x = min(X_test_pca[:, 0].min(), X_train_pca[:, 0].min()) max_mesh_x = max(X_test_pca[:, 0].max(), X_train_pca[:, 0].max()) min_mesh_y = min(X_test_pca[:, 1].min(), X_train_pca[:, 1].min()) max_mesh_y = max(X_test_pca[:, 1].max(), X_train_pca[:, 1].max()) mesh_x = np.linspace(min_mesh_x, max_mesh_x, 25) mesh_y = np.linspace(min_mesh_y, max_mesh_y, 25) mesh_xx, mesh_yy = np.meshgrid(mesh_x, mesh_y) mesh_zz = (d - a * mesh_xx - b * mesh_yy) / c . Here we recreate the prediction plane using three points of the prediction. More information at this link. . fig = plt.figure(figsize=(25,7)) ax1 = fig.add_subplot(131, projection=&#39;3d&#39;) ax2 = fig.add_subplot(132, projection=&#39;3d&#39;) ax3 = fig.add_subplot(133, projection=&#39;3d&#39;) axes = [ax1, ax2, ax3] for ax in axes: ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], Y_test, color=&#39;red&#39;, edgecolor=&#39;black&#39;) ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], Y_train, color=&#39;green&#39;, edgecolor=&#39;black&#39;) ax.scatter(mesh_xx.flatten(), mesh_yy.flatten(), mesh_zz.flatten(), facecolor=(0, 0, 0, 0), s=20, edgecolor=&#39;#70b3f0&#39;) ax.set_xlabel(&#39;1st component&#39;, fontsize=12) ax.set_ylabel(&#39;2nd component&#39;, fontsize=12) ax.set_zlabel(&#39;Selling Price&#39;, fontsize=12) ax.ticklabel_format(axis=&quot;x&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax.ticklabel_format(axis=&quot;y&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax.ticklabel_format(axis=&quot;z&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax1.view_init(elev=60, azim=50) ax2.view_init(elev=10, azim=0) ax3.view_init(elev=-15, azim=140) . The plane is fitting pretty well the data ! . Exploratory Data Analysis . We made an attempt to discard some features based on p-value but it didn&#39;t improve the results. More on p-value here. . columns = X.columns while len(columns) &gt; 0: pvalues = [] X_1 = X[columns] X_1 = sm.add_constant(X) # add a columns of ones for the bias model = sm.OLS(Y, X_1).fit() # fit a linear regression pvalues = pd.Series(model.pvalues[1:], index=columns) max_idx = np.argmax(pvalues) max_pval = pvalues[max_idx] if max_pval &gt; 0.05: # if the p_values is greater than 0.05, the feature has not enough # informational value for the training columns = columns.drop(columns[max_idx]) print(&#39;Dropping column &#39; + columns[max_idx] + &#39;, pvalue is: &#39; + str(max_pval)) else: break . Dropping column Seller_Type_Individual, pvalue is: 0.6296373292654155 Dropping column Fuel_Type_Diesel, pvalue is: 0.11176717429491591 Dropping column Seller_Type_Individual, pvalue is: 0.05428653381413104 . # Keeping only the columns with very low p-value X = df[columns] X_t = torch.from_numpy(X.to_numpy()).float() X_train, X_test, Y_train, Y_test = train_test_split(X_t, Y_t, test_size=0.33, random_state=42) w = multi_linear_reg(X_train, Y_train) Y_pred_train = prediction(X_train, w) Y_pred_test = prediction(X_test, w) mse_train = mse(Y_train, Y_pred_train) mse_test = mse(Y_test, Y_pred_test) print(&#39;MSE Train: t&#39;, mse_train) print(&#39;MSE Test: t&#39;, mse_test) . MSE Train: 3.4574925899505615 MSE Test: 3.8332533836364746 . Conclusion . A linear regression can perform pretty well if the data is highly correlated. It is an appropriate method as a baseline in the majority of the regression tasks. The pvalue is not always a good indicator for feature selection. .",
            "url": "https://consciousml.github.io/blog/linear-regression/car-price/pca/pytorch/from-scratch/2020/09/14/Multivariate-Linear-Regression.html",
            "relUrl": "/linear-regression/car-price/pca/pytorch/from-scratch/2020/09/14/Multivariate-Linear-Regression.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Linear Regression from scratch",
            "content": ". This blog post is about implementing linear regression from scratch using Pytorch. We will train the model on insurance data to predict the total payment received by a customer in regards to his number of claims. . For more details, the swedish auto insurance dataset contains the following attributes: . X = number of claims | Y = total payment for all the claims in thousands of Swedish Kronor for geographical zones in Sweden | . import os import sys import torch import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split . Reading Excel file with Pandas . data_path = os.path.join(&#39;data&#39;, &#39;slr06.xls&#39;) df = pd.read_excel(data_path, encoding_override=&quot;cp1252&quot;) df.head() . *** No CODEPAGE record, no encoding_override: will use &#39;ascii&#39; . X Y . 0 108 | 392.5 | . 1 19 | 46.2 | . 2 13 | 15.7 | . 3 124 | 422.2 | . 4 40 | 119.4 | . Data Visualization Using Seaborn . fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5)) ax1.set_title(&#39;Distribution of number of claims&#39;) ax2.set_title(&#39;Distribution of total payment&#39;) sns.distplot(df.X, bins=50, hist=True, ax=ax1) sns.distplot(df.Y, ax=ax2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d49e403b08&gt; . Two outliers are present between 100 and 125 number of claims. We will keep them in the training set. . sns.heatmap(df.corr(), annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d49e004088&gt; . The correlation heatmap and the plot below highlight that the number of claims and total payment are very highly correlated. . plt.xlabel(&#39;Number of claims&#39;) plt.ylabel(&#39;Total payment&#39;) plt.scatter(df[&#39;X&#39;], df[&#39;Y&#39;]) . &lt;matplotlib.collections.PathCollection at 0x1d49e6b9fc8&gt; . Training a Least Square Regressor . Convert the data to pytorch and separate the data into train &amp; test. . data = df.to_numpy() X = torch.from_numpy(data[:, 0]).float().unsqueeze(1) y = torch.from_numpy(data[:, 1]).float().unsqueeze(1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=50) . We will train a least square regressor on one dimension using the above formula: $ large y = b_0 + b_1x$ $ large b_1 = frac{ text{Cov}(x, y)}{Var(x)} = frac{ sum_i{(x_i - mathbb{E}[x]) * (y_i - mathbb{E}[y])}}{ sum_i{(x_i - mathbb{E}[x])^2}}$ where the expectation $ mathbb{E}[x]$ of a vector of random variables is its mean. . def linear_regression_1d(X, y): &quot;&quot;&quot;Trains a linear regression on 1D data Args: X: A numpy array for the training samples y: A numpy array for the labels of each sample &quot;&quot;&quot; X_m = X.mean(dim=0) y_m = y.mean(dim=0) X_c = (X - X_m) # Compute covariance and variance covar = (X_c * (y - y_m)).sum(dim=0) var = X_c.pow(2).sum(dim=0) # Divide covariance by variance b_1 = covar / var # Get bias b_0 = y_m - b_1 * X_m.sum(dim=0) return b_0, b_1 . We are using Pytorch for the matrix calculus. . Plot Regression Line . b_0, b_1 = linear_regression_1d(X_train, Y_train) plt.scatter(X_train, Y_train, marker=&#39;*&#39;) plt.scatter(X_test, Y_test, marker=&#39;.&#39;, color=&#39;red&#39;) x = [int(elt) for elt in range(0, int(X.cpu().numpy().max()))] y = b_0.numpy() + b_1.numpy() * x plt.plot(x, y, color=&#39;red&#39;) ax = plt.gca() ax.set_xlabel(&#39;Number of claims&#39;) ax.set_ylabel(&#39;Total payment&#39;) plt.show() . Because both the data and target are very highly correlated, the regression line fits pretty well the data distribution. . Compute Mean Square Error . pred_train = b_0 + b_1 * X_train err_train = Y_train - pred_train mse_train = err_train.T.mm(err_train) / Y_train.shape[0] pred_test = b_0 + b_1 * X_test err_test = Y_test - pred_test mse_test = err_test.T.mm(err_test) / Y_test.shape[0] print(&#39;Train MSE: t&#39;, mse_train.item()) print(&#39;Test MSE: t&#39;, mse_test.item()) . Train MSE: 1241.5714111328125 Test MSE: 1279.6766357421875 . The train and test MSE are close to each other. The model neither underfit nor overfit. . Conclusion . Linear regression is a simple and interpretable model that fits very well the purpose of a baseline. As we have seen during this experiment, if the features are highly correlated to the target, linear regression performs pretty well. .",
            "url": "https://consciousml.github.io/blog/linear-regression/insurance-data/pytorch/from-scratch/2020/09/12/Linear-Regression.html",
            "relUrl": "/linear-regression/insurance-data/pytorch/from-scratch/2020/09/12/Linear-Regression.html",
            "date": " • Sep 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://consciousml.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://consciousml.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}