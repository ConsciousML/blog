{
  
    
        "post0": {
            "title": "Polynomial Regression from scratch",
            "content": "import torch import numpy as np import matplotlib.pyplot as plt . Generate a polynomial distribution with random noise . X = torch.tensor([i for i in range(-30, 30)]).float() . def randvec(r1, r2, shape): return (r1 - r2) * torch.rand(shape) + r2 . y = randvec(-1e4, 1e4, X.shape) - (1/2) * X + 3 * X.pow(2) - (6/4) * X.pow(3) . plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x2d0193b3448&gt; . Create the polynomial features . The formula of linear regression is as follow: $$ large boldsymbol{ hat{y}} = boldsymbol{X} boldsymbol{w} $$ where $ boldsymbol{ hat{y}}$ is the target, $ boldsymbol{w}$ are the weights learned by the model and $ boldsymbol{X}$ is training data. Polynomial regression is still considered as a linear regression because there is only linear learning parameters: $$ large boldsymbol{y} = boldsymbol{w}_0 + boldsymbol{X} boldsymbol{w}_1 + boldsymbol{X}^2 boldsymbol{w}_2 + dots + boldsymbol{X}^n boldsymbol{w}_n $$ As you have probably guessed, this equation is not linear. We use a trick to make it linear: . We gather all the $ boldsymbol{X}^2$ to $ boldsymbol{X}^n$ as new features that we created and we concatenate them to $ boldsymbol{X}$. | All the $ boldsymbol{w}_1$ to $ boldsymbol{w}_n$ are concatenated to $ boldsymbol{w}_0$. | . At the end, the polynomial regression has the same formula as the linear regression but with the aggregated arrays. . def create_features(X, degree=2, standardize=True): &quot;&quot;&quot;Creates the polynomial features Args: X: A torch tensor for the data. degree: A intege for the degree of the generated polynomial function. standardize: A boolean for scaling the data or not. &quot;&quot;&quot; if len(X.shape) == 1: X = X.unsqueeze(1) # Concatenate a column of ones to has the bias in X ones_col = torch.ones((X.shape[0], 1), dtype=torch.float32) X_d = torch.cat([ones_col, X], axis=1) for i in range(1, degree): X_pow = X.pow(i + 1) # If we use the gradient descent method, we need to # standardize the features to avoid exploding gradients if standardize: X_pow -= X_pow.mean() std = X_pow.std() if std != 0: X_pow /= std X_d = torch.cat([X_d, X_pow], axis=1) return X_d . features = create_features(X, degree=3, standardize=False) . def predict(features, weights): return features.mm(weights) . y_true = y.unsqueeze(1) . Method 1: Normal equation . The first method is analytical and uses the normal equation. Training a linear model using least square regression is equivalent to minimize the mean squared error: . $$ begin{align} text{Mse}( hat{y}, y) &amp;= frac{1}{n} sum_{i=1}^{n}{|| hat{y}_i - y_i ||_{2}^{2}} text{Mse}( hat{y}, y) &amp;= frac{1}{n}|| boldsymbol{X} boldsymbol{w} - boldsymbol{y} ||_2^2 end{align} $$where $n$ is the number of samples, $ hat{y}$ is the predicted value of the model and $y$ is the true target. The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$. . Minimizing the $ text{Mse}$ can be achieved by solving the gradient of this equation equals to zero in regards to the weights $ boldsymbol{w}$: . $$ begin{align} nabla_{ boldsymbol{w}} text{Mse}( hat{y}, y) &amp;= 0 ( boldsymbol{X}^ top boldsymbol{X})^{-1} boldsymbol{X}^ top boldsymbol{y} &amp;= boldsymbol{w} end{align} $$For more information on how to find $ boldsymbol{w}$ please visit the section &quot;Linear Least Squares&quot; of this link. . def normal_equation(y_true, X): &quot;&quot;&quot;Computes the normal equation Args: y_true: A torch tensor for the labels. X: A torch tensor for the data. &quot;&quot;&quot; XTX_inv = (X.T.mm(X)).inverse() XTy = X.T.mm(y_true) weights = XTX_inv.mm(XTy) return weights . weights = normal_equation(y_true, features) . y_pred = predict(features, weights) . plt.scatter(X, y) plt.plot(X, y_pred, c=&#39;red&#39;) . [&lt;matplotlib.lines.Line2D at 0x2ac7bb7ba88&gt;] . With the normal equation method, the polynomial regressor fits well the synthetic data. . Method 2: Gradient Descent . The Gradient descent method takes steps proportional to the negative of the gradient of a function at a given point, in order to iteratively minimize the objective function. The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of $f$ is the vector containing all of the partial derivatives, denoted $ nabla_{ boldsymbol{x}}f( boldsymbol{x})$. . The directional derivative in direction $ boldsymbol{u}$ (a unit vector) is the slope of the function $f$ in direction $ boldsymbol{u}$. In other words, the directional derivative is the derivative of the function $f( boldsymbol{x} + sigma boldsymbol{u})$ with respect to $ sigma$ close to 0. To minimize $f$, we would like to find the direction in which $f$ decreases the fastest. We can do this using the directional derivative: $$ min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{ boldsymbol{u}^ top nabla_{ boldsymbol{x}} f( boldsymbol{x})}$$ $$= min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{|| boldsymbol{u}||_2 || nabla_{ boldsymbol{x}}f( boldsymbol{x})||_2 cos theta}$$ ignoring factors that do not depend on $ boldsymbol{u}$, this simplifies to $ min_{u}{ cos theta}$. This is minimized when $ boldsymbol{u}$ points in the opposite direction as the gradient. Each step of the gradient descent method proposes a new points: $$ boldsymbol{x&#39;} = boldsymbol{x} - epsilon nabla_{ boldsymbol{x}}f( boldsymbol{x})$$ where $ epsilon$ is the learning rate. In the context of polynomial regression, the gradient descent is as follow: $$ boldsymbol{w} = boldsymbol{w} - epsilon nabla_{ boldsymbol{w}} text{MSE}$$ where: $$ nabla_{ boldsymbol{w}} text{MSE} = nabla_{ boldsymbol{w}} left( frac{1}{n}{|| boldsymbol{X} boldsymbol{w} - boldsymbol{y} ||_2^2} right)$$ $$= frac{2}{N} boldsymbol{X}^ top( boldsymbol{X} boldsymbol{w} - boldsymbol{y})$$ . def gradient_descent(X, y_true, lr=0.001, it=30000): &quot;&quot;&quot;Computes the gradient descent Args: X: A torch tensor for the data. y_true: A torch tensor for the labels. lr: A scalar for the learning rate. it: A scalar for the number of iteration or number of gradient descent steps. &quot;&quot;&quot; weights_gd = torch.ones((X.shape[1], 1)) n = X.shape[0] fact = 2 / n for _ in range(it): y_pred = predict(X, weights_gd) grad = fact * X.T.mm(y_pred - y_true) weights_gd -= lr * grad return weights_gd . features = create_features(X, degree=3, standardize=True) weights_gd = gradient_descent(features, y_true) . pred_gd = predict(features, weights_gd) . The mean squared error is even lower when using gradient descent. . plt.scatter(X, y) plt.plot(X, pred_gd, c=&#39;red&#39;) . [&lt;matplotlib.lines.Line2D at 0x2ac7bc9be08&gt;] . Conclusion . The polynomial regression is an appropriate example to learn more about the concept of normal equation and gradient descent. This method work well with data that has polynomial shapes but we need to choose the right polynomial degree for a good bias/variance trade-off. However, the polynomial regression method has an important drawback. In fact, it is necessary to transform the data to a higher dimensional space which can be unfeasable if the data is very large. .",
            "url": "https://consciousml.github.io/blog/2020/09/22/Polynomial-Regression.html",
            "relUrl": "/2020/09/22/Polynomial-Regression.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Logistic Regression from scratch",
            "content": "During this experiment, we will train logistic regression on diabetes data, from scratch using Pytorch. The Pima Indians Diabetes Database has been gathered by the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of this dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. This dataset contains the following features: . Pregnancies | Glucose | BloodPressure | SkinThickness | Insuline | BMI | DiabetesPedigreeFunction | Age | Outcome (has diabetes or not) | . import os import torch import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split . Read csv using Pandas . diabetes = pd.read_csv(os.path.join(&#39;data&#39;, &#39;diabetes.csv&#39;)) . diabetes.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . diabetes.describe() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . count 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | . mean 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578 | 0.471876 | 33.240885 | 0.348958 | . std 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160 | 0.331329 | 11.760232 | 0.476951 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.078000 | 21.000000 | 0.000000 | . 25% 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000 | 0.243750 | 24.000000 | 0.000000 | . 50% 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000 | 0.372500 | 29.000000 | 0.000000 | . 75% 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000 | 0.626250 | 41.000000 | 1.000000 | . max 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 | 67.100000 | 2.420000 | 81.000000 | 1.000000 | . On this sample of data, the standard devitation of the columns looks reasonably high except for the DiabetesPedigree but it is acceptable because the mean is relatively low. A feature having low std is likely to provide close to no information to the model. Which is not the case here. . sns.countplot(x=&#39;Outcome&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1827e88b588&gt; . The target distribution is very unbalanced with two times more negative than positives. . Heatmap Correlation . plt.figure(figsize=(20, 5)) sns.heatmap(diabetes.corr(), annot=True, square=True, fmt=&#39;0.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1827ea30e48&gt; . The glucose level, BMI, age and number of pregnancies are highly correlated with the outcome. Suprisingly, the insulin level is not very correlated with the outcome. Most likely because the insulin is correlated with the glucose and the glucose has 0.47 correlation with the target. . plt.figure(figsize=(15, 3)) sns.boxplot(x=&#39;Glucose&#39;, y=&#39;Outcome&#39;, orient=&#39;h&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1827ee46e08&gt; . For patient with diabetes, the glucose level is significantly higher. In other words, a patient with high glucose level is very likely to have diabetes. . plt.figure(figsize=(15, 3)) sns.boxplot(x=&#39;Pregnancies&#39;, y=&#39;Outcome&#39;, orient=&#39;h&#39;, data=diabetes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1827fe92108&gt; . Surprisingly, the number of pregnancies is correlated with diabetes. . Convert data to Torch tensors . X = diabetes.iloc[:, :-1].values y = torch.from_numpy(diabetes.iloc[:, -1].values).float().unsqueeze(1) . X = StandardScaler().fit_transform(X) X = torch.from_numpy(X).float() # Add column of ones for the bias ones_col = torch.ones((X.shape[0], 1), dtype=torch.float32) X = torch.cat([ones_col, X], axis=1) . Split data into train and test . X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25) . Train Logistic Regression . The prediction of a logistic model is as follow: $$ large hat{y} = sigma( boldsymbol{X} boldsymbol{w}) $$ Where $ sigma$ is the sigmoid or logit function: $$ large sigma( boldsymbol{x}) = frac{1}{1 + exp(-x)} $$ The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$ given as input to the logit function. The sigmoid function is used here because it squashes the input in the $[0, 1]$ range suitable for describing a Bernouilli distribution. Where the Bernouilli cases are: . the patient has diabetes with $p$ probability | the patient does not have diabetes with $1 - p$ probability | . It is important to note that the bias is included in $ boldsymbol{X}$ as a column of ones. . Training a classification model can be expressed as maximizing the likelihood of the observed data. In other words, we want the predicted probability of our model that a patient has diabetes to be as close as the true probability of the data. In practice, maximizing the likelihood is equivalent to minimize the negative log likelihood: $$ large L( boldsymbol{ theta}) = - frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) $$ Because we dealing with a binary target, it is appropriate to use the binary cross entropy: $$ large L( boldsymbol{ theta}) = - frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) + (1 - boldsymbol{y_i}) log(1 - hat{ boldsymbol{y}}_i) $$ . Gradient Descent for Logistic Regression . We will use Gradient Descent to train the logistic regression model. The Gradient descent method takes steps proportional to the negative of the gradient of a function at a given point, in order to iteratively minimize the objective function. The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of $f$ is the vector containing all of the partial derivatives, denoted $ nabla_{ boldsymbol{x}}f( boldsymbol{x})$. . The directional derivative in direction $ boldsymbol{u}$ (a unit vector) is the slope of the function $f$ in direction $ boldsymbol{u}$. In other words, the directional derivative is the derivative of the function $f( boldsymbol{x} + sigma boldsymbol{u})$ with respect to $ sigma$ close to 0. To minimize $f$, we would like to find the direction in which $f$ decreases the fastest. We can do this using the directional derivative: $$ large min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{ boldsymbol{u}^ top nabla_{ boldsymbol{x}} f( boldsymbol{x})} $$ $$ large = min_{ boldsymbol{u}, boldsymbol{u}^ top boldsymbol{u} = 1}{|| boldsymbol{u}||_2 || nabla_{ boldsymbol{x}}f( boldsymbol{x})||_2 cos theta}$$ ignoring factors that do not depend on $ boldsymbol{u}$, this simplifies to $ min_{u}{ cos theta}$. This is minimized when $ boldsymbol{u}$ points in the opposite direction as the gradient. Each step of the gradient descent method proposes a new point: $$ large boldsymbol{x&#39;} = boldsymbol{x} - epsilon nabla_{ boldsymbol{x}}f( boldsymbol{x}) $$ where $ epsilon$ is the learning rate. In the context of logistic regression, the gradient descent is as follow: $$ large boldsymbol{w} = boldsymbol{w} - epsilon nabla_{ boldsymbol{w}}L( boldsymbol{ theta}) $$ where: $$ large nabla_{ boldsymbol{w}}L( boldsymbol{ theta}) = nabla_{ boldsymbol{w}} left(- frac{1}{N} sum_{i=1}^{n} boldsymbol{y_i} log( hat{ boldsymbol{y}}_i) + (1 - boldsymbol{y_i}) log(1 - hat{ boldsymbol{y}}_i) right) $$ $$ large = frac{1}{N} boldsymbol{X}^ top( sigma( boldsymbol{X} boldsymbol{w}) - boldsymbol{y}) $$ Here is a nice explanation of how to find the gradient of the binary cross entropy: https://www.youtube.com/watch?v=hWLdFMccpTY . def sigmoid(x): &quot;&quot;&quot;Sigmoid function that squashes the input between 0 and 1&quot;&quot;&quot; return 1 / (1 + torch.exp(-x)) def predict(X, weights): &quot;&quot;&quot;Pedicts the class given the data and the weights Args: X: A torch tensor for the input data. weights: A torch tensor for the parameters calculated during the training of the Logistic regression. &quot;&quot;&quot; return sigmoid(X.mm(weights)) . def binary_cross_entropy(y_true, y_pred): &quot;&quot;&quot;Loss function for the training of the logistic regression We add an epsilon inside the log functions to avoid Nans. Args: y_true: A torch tensor for the labels of the data. y_pred: A torch tensor for the values predicted by the model. &quot;&quot;&quot; fact = 1 / y_true.shape[0] return -fact * (y_true * torch.log(y_pred + 1e-10) + (1 - y_true) * torch.log(1 - y_pred + 1e-10 )).sum() . def train_logit_reg(X, y_true, weights, lr=0.001, it=2000): &quot;&quot;&quot;Trains the logistic regression model Args: X: A torch tensor for the training data. y: A torch tensor for the labels of the data. weights: A torch tensor for the learning parameters of the model. lr: A scalar describing the learning rate for the gradient descent. it: A scalar for the number of steps in the gradient descent. &quot;&quot;&quot; for _ in range(it): y_pred = predict(X, weights) err = (y_pred - y_true) grad = X.T.mm(err) weights -= lr * grad bn_train = binary_cross_entropy(y_true, y_pred).item() return weights, bn_train . weights = torch.ones((X.shape[1], 1), dtype=torch.float32) weights, bn_train = train_logit_reg(X_train, y_train, weights) y_pred = predict(X_test, weights) print(&#39;Binary cross-entropy on the train set:&#39;, bn_train) . Binary cross-entropy on the train set: 0.4595394730567932 . Binary cross entropy on test set . bn_test = binary_cross_entropy(y_test, y_pred).item() print(&#39;Binary cross-entropy on the test set:&#39;, bn_test) . Binary cross-entropy on the test set: 0.5321948528289795 . Accuracy on test set . To compute the accuracy, we have to find the best threshold to convert our probability output in binary values. . def get_binary_pred(y_true, y_pred): &quot;&quot;&quot;Finds the best threshold based on the prediction and the labels Args: y_true: A torch tensor for the labels of the data. y_pred: A torch tensor for the values predicted by the model. &quot;&quot;&quot; y_pred_thr = y_pred.clone() accs = [] thrs = [] for thr in np.arange(0, 1, 0.01): y_pred_thr[y_pred &gt;= thr] = 1 y_pred_thr[y_pred &lt; thr] = 0 cur_acc = classification_report(y_test, y_pred_thr, output_dict=True)[&#39;accuracy&#39;] accs.append(cur_acc) thrs.append(thr) accs = torch.FloatTensor(accs) thrs = torch.FloatTensor(thrs) idx = accs.argmax() best_thr = thrs[idx].item() best_acc = accs[idx].item() y_pred[y_pred &gt;= best_thr] = 1 y_pred[y_pred &lt; best_thr] = 0 return y_pred . import sklearn y_pred = get_binary_pred(y_test, y_pred) print(classification_report(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) . precision recall f1-score support 0.0 0.79 0.90 0.84 123 1.0 0.76 0.57 0.65 69 accuracy 0.78 192 macro avg 0.78 0.73 0.75 192 weighted avg 0.78 0.78 0.77 192 [[111 12] [ 30 39]] . With a threshold of 0.66, we achieve an accuracy of 78% which is quite good for a linear model. . Polynomial Logistic Regression . In this section, we will add some polynomial features to the logistic regressor. It is the same principle as the logistic regression except that $ boldsymbol{X}$ is the concatenation of $ boldsymbol{X_1} dots boldsymbol{X_m}$ where $m$ is the degree of the polynomial function and $ boldsymbol{w}$ is the concatenation of $ boldsymbol{w_1} dots boldsymbol{w_m}$ such as: $$ large boldsymbol{y} = sigma( boldsymbol{w}_0 + boldsymbol{X} boldsymbol{w}_1 + boldsymbol{X}^2 boldsymbol{w}_2 + dots + boldsymbol{X}^m boldsymbol{w}_m) $$ This method is still linear because predicting $ hat{y} = sigma( boldsymbol{X} boldsymbol{w})$ is still linear in the parameters. . X = torch.from_numpy(diabetes.iloc[:, :-1].values).float() y = torch.from_numpy(diabetes.iloc[:, -1].values).float().unsqueeze(1) . def create_poly_features(X, degree=2): &quot;&quot;&quot;Creates the augmented features for the polynomial model This function concatenates the augmented data into a single torch tensor. Args: X: A torch tensor for the data. degree: A integer for the degree of the polynomial function that we model. &quot;&quot;&quot; if len(X.shape) == 1: X = X.unsqueeze(1) ones_col = torch.ones((X.shape[0], 1)) # Standardize the output to avoid exploding gradients X_cat = X.clone() X_cat = (X_cat - X_cat.mean()) / X_cat.std() X_cat = torch.cat([ones_col, X_cat], axis=1) for i in range(1, degree): X_p = X.pow(i + 1) X_p = torch.from_numpy(StandardScaler().fit_transform(X_p)).float() X_cat = torch.cat([X_cat, X_p], axis=1) return X_cat . features = create_poly_features(X, degree=2) . def create_weights(features): &quot;&quot;&quot;Creates a column of ones&quot;&quot;&quot; return torch.ones((features.shape[1], 1), dtype=torch.float32) . X_train, X_test, y_train, y_test = train_test_split(features, y, random_state=42, test_size=0.25) . weights = create_weights(X_train) weights, bn_train = train_logit_reg(X_train, y_train, weights, it=10000) y_pred = predict(X_test, weights) print(&#39;Binary cross-entropy on the train set:&#39;, bn_train) . Binary cross-entropy on the train set: 0.44449102878570557 . binary_cross_entropy(y_test, y_pred).item() . 0.5331151485443115 . y_pred = get_binary_pred(y_test, y_pred) print(classification_report(y_test, y_pred)) print(confusion_matrix(y_test, y_pred)) . precision recall f1-score support 0.0 0.75 0.95 0.84 123 1.0 0.84 0.45 0.58 69 accuracy 0.77 192 macro avg 0.80 0.70 0.71 192 weighted avg 0.78 0.77 0.75 192 [[117 6] [ 38 31]] . Conclusion . The polynomial logistic regression model overfitted compared to the classic logistic regression model because it lost 1% accuracy on the test set. For some very highly correlated data, logistic regression without polynomial features has better performance than with polynomial features. Logistic regression is a very simple and interpretable model suited as a baseline in most classification problems. However, it does not perform well when the feature space is large. In fact, it is difficult to compute feature transformation (such as polynomials) when the data doesn&#39;t fit in ram. .",
            "url": "https://consciousml.github.io/blog/logistic-regression/polynomial-regression/diabetes/pytorch/from-scratch/2020/09/17/Logistic-Regression.html",
            "relUrl": "/logistic-regression/polynomial-regression/diabetes/pytorch/from-scratch/2020/09/17/Logistic-Regression.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Multivariate Linear Regression from scratch",
            "content": "The CarDekho dataset contains information about used cars listed on the website of the same name. This dataset has 301 unique entities with the following features: . car name | year of release | selling price | present price | kilometers driven | fuel: such as petrol or diesel | transmission: such as manual or automatic | owner: how many times the car changed owner | . The goal of this experiment is to train a linear model to predict the selling price of a car. We will use the framework Pytorch for the matrix calculus. . import os import sys import torch import pandas as pd import numpy as np import seaborn as sns import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split . Read data from csv using Pandas . df = pd.read_csv(os.path.join(&#39;data&#39;, &#39;car_data.csv&#39;)) df.head() . Car_Name Year Selling_Price Present_Price Kms_Driven Fuel_Type Seller_Type Transmission Owner . 0 ritz | 2014 | 3.35 | 5.59 | 27000 | Petrol | Dealer | Manual | 0 | . 1 sx4 | 2013 | 4.75 | 9.54 | 43000 | Diesel | Dealer | Manual | 0 | . 2 ciaz | 2017 | 7.25 | 9.85 | 6900 | Petrol | Dealer | Manual | 0 | . 3 wagon r | 2011 | 2.85 | 4.15 | 5200 | Petrol | Dealer | Manual | 0 | . 4 swift | 2014 | 4.60 | 6.87 | 42450 | Diesel | Dealer | Manual | 0 | . Convert categorical variable into indicator variables . f_continuous = df[[&#39;Year&#39;, &#39;Selling_Price&#39;, &#39;Present_Price&#39;, &#39;Kms_Driven&#39;, &#39;Owner&#39;]] f_categorical = pd.get_dummies(df[[&#39;Fuel_Type&#39;, &#39;Seller_Type&#39;, &#39;Transmission&#39;]]) df = pd.concat([f_continuous, f_categorical], axis=1) # Drop refundant features df.drop([&#39;Transmission_Automatic&#39;, &#39;Seller_Type_Dealer&#39;, &#39;Fuel_Type_CNG&#39;], axis=1, inplace=True) df.head() . Year Selling_Price Present_Price Kms_Driven Owner Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 0 2014 | 3.35 | 5.59 | 27000 | 0 | 0 | 1 | 0 | 1 | . 1 2013 | 4.75 | 9.54 | 43000 | 0 | 1 | 0 | 0 | 1 | . 2 2017 | 7.25 | 9.85 | 6900 | 0 | 0 | 1 | 0 | 1 | . 3 2011 | 2.85 | 4.15 | 5200 | 0 | 0 | 1 | 0 | 1 | . 4 2014 | 4.60 | 6.87 | 42450 | 0 | 1 | 0 | 0 | 1 | . Visualize histogram of all features . df.hist(bins=14, color=&#39;steelblue&#39;, edgecolor=&#39;black&#39;, linewidth=1.0, xlabelsize=8, ylabelsize=8, grid=False) plt.tight_layout(rect=(0, 0, 1.2, 1.2)) . Most cars on sales are consuming petrol instead of diesel, have had only one owner, are from 2012-present, are manual and have a selling price between 1000 and 10000 dollars. . Heatmap correlation . plt.figure(figsize=(16, 8)) sns.heatmap(df.corr(), square= True, annot=True, fmt=&#39;.2f&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c3a2540988&gt; . Most of the variables are highly correlated. As expected, the present price variable is the most correlated with the target selling price. . Pairwise Plots . cols_viz = [&#39;Kms_Driven&#39;, &#39;Year&#39;, &#39;Selling_Price&#39;, &#39;Present_Price&#39;] pp = sns.pairplot(df[cols_viz], height=1.8, aspect=1.8, plot_kws=dict(edgecolor=&quot;k&quot;, linewidth=0.5), diag_kind=&quot;kde&quot;, diag_kws=dict(shade=True)) fig = pp.fig fig.subplots_adjust(top=0.93, wspace=0.3) t = fig.suptitle(&#39;Wine Attributes Pairwise Plots&#39;, fontsize=14) . Most of the features are highly correlated to each other. Some outliers are present but as there is very few, we will keep them in the training set. The year feature have a polynomial correlation with the selling price. A polynomial regression will most likely overperform a linear regression. . Make train test split . Y = df[&#39;Selling_Price&#39;] X = df.drop(&#39;Selling_Price&#39;, axis=1) . X_t = torch.from_numpy(X.to_numpy()).float() Y_t = torch.from_numpy(Y.to_numpy()).float().unsqueeze(1) X_train, X_test, Y_train, Y_test = train_test_split(X_t, Y_t, test_size=0.33, random_state=42) . Train a multivariate linear regression . Training a linear model using least square regression is equivalent to minimize the mean squared error: . $$ begin{align} large text{Mse}( boldsymbol{ hat{y}}, boldsymbol{y}) &amp;= frac{1}{n} sum_{i=1}^{n}{|| hat{y}_i - y_i ||_{2}^{2}} &amp;= frac{1}{n}|| boldsymbol{X} boldsymbol{w} - boldsymbol{y} ||_2^2 end{align} $$where $n$ is the number of samples, $ hat{y}$ is the predicted value of the model and $y$ is the true target. The prediction $ hat{y}$ is obtained by matrix multiplication between the input $ boldsymbol{X}$ and the weights of the model $ boldsymbol{w}$. . Minimizing the $ text{Mse}$ can be achieved by solving the gradient of this equation equals to zero in regards to the weights $ boldsymbol{w}$: . $$ begin{align} large nabla_{ boldsymbol{w}} text{Mse} &amp;= 0 large ( boldsymbol{X}^ top boldsymbol{X})^{-1} boldsymbol{X}^ top boldsymbol{y} &amp;= boldsymbol{w} end{align} $$For more information on how to find $ boldsymbol{w}$ please visit the following link. . def add_ones_col(X): &quot;&quot;&quot;Add a column a one to the input torch tensor&quot;&quot;&quot; x_0 = torch.ones((X.shape[0],), dtype=torch.float32).unsqueeze(1) X = torch.cat([x_0, X], dim=1) return X def multi_linear_reg(X, y): &quot;&quot;&quot;Multivariate linear regression function Args: X: A torch tensor for the data. y: A torch tensor for the labels. &quot;&quot;&quot; X = add_ones_col(X) # Add a column of ones to X to agregate the bias to the input matrices Xt_X = X.T.mm(X) Xt_y = X.T.mm(y) Xt_X_inv = Xt_X.inverse() w = Xt_X_inv.mm(Xt_y) return w def prediction(X, w): &quot;&quot;&quot;Predicts a selling price for each input Args: X: A torch tensor for the data. w: A torch tensor for the weights of the linear regression mode. &quot;&quot;&quot; X = add_ones_col(X) return X.mm(w) . w = multi_linear_reg(X_train, Y_train) # Predict using matrix multiplication with the weights Y_pred_train = prediction(X_train, w) Y_pred_test = prediction(X_test, w) . Compute prediction error . def mse(Y_true, Y_pred): error = Y_pred - Y_true return (error.T.mm(error) / Y_pred.shape[0]).item() def mae(Y_true, Y_pred): error = Y_pred - Y_true return error.abs().mean().item() . mse_train = mse(Y_train, Y_pred_train) mae_train = mae(Y_train, Y_pred_train) print(&#39;MSE Train: t&#39;, mse_train) print(&#39;MAE Train: t&#39;, mae_train, end=&#39; n n&#39;) mse_test = mse(Y_test, Y_pred_test) mae_test = mae(Y_test, Y_pred_test) print(&#39;MSE Test: t&#39;, mse_test) print(&#39;MAE Test: t&#39;, mae_test, end=&#39; n n&#39;) . MSE Train: 2.808985471725464 MAE Train: 1.1321566104888916 MSE Test: 3.7205495834350586 MAE Test: 1.2941011190414429 . The model has an error of 1.29 on average on the training test. Not bad for a linear model, taking into consideration that the mean of the present price is 7.62. . Principal component analysis visualization . In this section, we will use PCA to reduce the number of feature to two, in order to visualize the plane of the linear regressor. . Suppose a collection of $m$ points $ { boldsymbol{x}^{(1)}, dots, boldsymbol{x}^{(m)} }$ in $ mathbb{R}^n$. The principal components analysis aims to reduce the dimensionality of the points while losing the least precision as possible. For each point $ boldsymbol{x}^{(i)} in mathbb{R}^n$ we will find a corresponding code vector $ boldsymbol{c}^{(i)} in mathbb{R}^l$ where $l$ is smaller than $n$. Let $f$ be the encoding function and $g$ be the decoding function and $ boldsymbol{D} in mathbb{R}^{n,l}$ is the decoding matrix whose columns are orthonormal: $$ begin{align} f( boldsymbol{x}) &amp;= boldsymbol{D}^ top boldsymbol{x} g(f( boldsymbol{x})) &amp;= boldsymbol{D} boldsymbol{D}^ top boldsymbol{x} end{align} $$ . def cov(X): &quot;&quot;&quot;Computes the covariance of the input The covariance matrix gives some sense of how much two values are linearly related to each other, as well as the scale of these variables. It is computed by (1 / (N - 1)) * (X - E[X]).T (X - E[X]). Args: X: A torch tensor as input. &quot;&quot;&quot; X -= X.mean(dim=0, keepdim=True) fact = 1.0 / (X.shape[0] - 1) cov = fact * X.T.mm(X) return cov def pca(X, target_dim=2): &quot;&quot;&quot;Computes the n^th first principal components of the input PCA can be implemented using the n^th principal components of the covariance matrix. We could have been using an eigen decomposition because the covariance matrix is always squared but singular value decomposition does also the trick if we take the right singular vectors and perform a matrix multiplication to the right. Args: X: A torch tensor as the input. target_dim: An integer for selecting the n^th first components. &quot;&quot;&quot; cov_x = cov(X) U, S, V = torch.svd(cov_x) transform_mat = V[:, :target_dim] X_reduced = X.mm(transform_mat) return X_reduced, transform_mat . X_test_pca, _ = pca(X_test, target_dim=2) X_train_pca, _ = pca(X_train, target_dim=2) . points = torch.cat([X_test_pca[:3], Y_pred_test[:3]], axis=1) v1 = points[2, :] - points[0, :] v2 = points[1, :] - points[0, :] cp = torch.cross(v1, v2) a, b, c = cp d = cp.dot(points[2, :]) min_mesh_x = min(X_test_pca[:, 0].min(), X_train_pca[:, 0].min()) max_mesh_x = max(X_test_pca[:, 0].max(), X_train_pca[:, 0].max()) min_mesh_y = min(X_test_pca[:, 1].min(), X_train_pca[:, 1].min()) max_mesh_y = max(X_test_pca[:, 1].max(), X_train_pca[:, 1].max()) mesh_x = np.linspace(min_mesh_x, max_mesh_x, 25) mesh_y = np.linspace(min_mesh_y, max_mesh_y, 25) mesh_xx, mesh_yy = np.meshgrid(mesh_x, mesh_y) mesh_zz = (d - a * mesh_xx - b * mesh_yy) / c . Here we recreate the prediction plane using three points of the prediction. More information at this link. . fig = plt.figure(figsize=(25,7)) ax1 = fig.add_subplot(131, projection=&#39;3d&#39;) ax2 = fig.add_subplot(132, projection=&#39;3d&#39;) ax3 = fig.add_subplot(133, projection=&#39;3d&#39;) axes = [ax1, ax2, ax3] for ax in axes: ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], Y_test, color=&#39;red&#39;, edgecolor=&#39;black&#39;) ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], Y_train, color=&#39;green&#39;, edgecolor=&#39;black&#39;) ax.scatter(mesh_xx.flatten(), mesh_yy.flatten(), mesh_zz.flatten(), facecolor=(0, 0, 0, 0), s=20, edgecolor=&#39;#70b3f0&#39;) ax.set_xlabel(&#39;1st component&#39;, fontsize=12) ax.set_ylabel(&#39;2nd component&#39;, fontsize=12) ax.set_zlabel(&#39;Selling Price&#39;, fontsize=12) ax.ticklabel_format(axis=&quot;x&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax.ticklabel_format(axis=&quot;y&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax.ticklabel_format(axis=&quot;z&quot;, style=&quot;sci&quot;, scilimits=(0,0)) ax1.view_init(elev=60, azim=50) ax2.view_init(elev=10, azim=0) ax3.view_init(elev=-15, azim=140) . The plane is fitting pretty well the data ! . Exploratory Data Analysis . We made an attempt to discard some features based on p-value but it didn&#39;t improve the results. More on p-value here. . columns = X.columns while len(columns) &gt; 0: pvalues = [] X_1 = X[columns] X_1 = sm.add_constant(X) # add a columns of ones for the bias model = sm.OLS(Y, X_1).fit() # fit a linear regression pvalues = pd.Series(model.pvalues[1:], index=columns) max_idx = np.argmax(pvalues) max_pval = pvalues[max_idx] if max_pval &gt; 0.05: # if the p_values is greater than 0.05, the feature has not enough # informational value for the training columns = columns.drop(columns[max_idx]) print(&#39;Dropping column &#39; + columns[max_idx] + &#39;, pvalue is: &#39; + str(max_pval)) else: break . Dropping column Seller_Type_Individual, pvalue is: 0.6296373292654155 Dropping column Fuel_Type_Diesel, pvalue is: 0.11176717429491591 Dropping column Seller_Type_Individual, pvalue is: 0.05428653381413104 . X = df[columns] X_t = torch.from_numpy(X.to_numpy()).float() X_train, X_test, Y_train, Y_test = train_test_split(X_t, Y_t, test_size=0.33, random_state=42) w = multi_linear_reg(X_train, Y_train) Y_pred_train = prediction(X_train, w) Y_pred_test = prediction(X_test, w) mse_train = mse(Y_train, Y_pred_train) mse_test = mse(Y_test, Y_pred_test) print(&#39;MSE Train: t&#39;, mse_train) print(&#39;MSE Test: t&#39;, mse_test) . MSE Train: 3.4574925899505615 MSE Test: 3.8332533836364746 . Conclusion . A linear regression can perform pretty well if the data is highly correlated. It is an appropriate method as a baseline in the majority of the regression tasks. The pvalue is not always a good indicator for feature selection. .",
            "url": "https://consciousml.github.io/blog/linear-regression/car-price/pca/pytorch/from-scratch/2020/09/14/Multivariate-Linear-Regression.html",
            "relUrl": "/linear-regression/car-price/pca/pytorch/from-scratch/2020/09/14/Multivariate-Linear-Regression.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Linear Regression from scratch",
            "content": "The swedish auto insurance dataset contains the following attributes: . X = number of claims | Y = total payment for all the claims in thousands of Swedish Kronor for geographical zones in Sweden | . import os import sys import torch import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split . Reading Excel file with Pandas . data_path = os.path.join(&#39;data&#39;, &#39;slr06.xls&#39;) df = pd.read_excel(data_path, encoding_override=&quot;cp1252&quot;) df.head() . *** No CODEPAGE record, no encoding_override: will use &#39;ascii&#39; . X Y . 0 108 | 392.5 | . 1 19 | 46.2 | . 2 13 | 15.7 | . 3 124 | 422.2 | . 4 40 | 119.4 | . Data Visualization Using Seaborn . fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5)) ax1.set_title(&#39;Distribution of number of claims&#39;) ax2.set_title(&#39;Distribution of total payment&#39;) sns.distplot(df.X, bins=50, hist=True, ax=ax1) sns.distplot(df.Y, ax=ax2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d49e403b08&gt; . Two outliers are present between 100 and 125 number of claims. We will keep them in the training set. . sns.heatmap(df.corr(), annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d49e004088&gt; . The correlation heatmap and the plot below highlight that the number of claims and total payment are very highly correlated. . plt.xlabel(&#39;Number of claims&#39;) plt.ylabel(&#39;Total payment&#39;) plt.scatter(df[&#39;X&#39;], df[&#39;Y&#39;]) . &lt;matplotlib.collections.PathCollection at 0x1d49e6b9fc8&gt; . Training a Least Square Regressor . Convert the data to pytorch and separate the data into train &amp; test. . data = df.to_numpy() X = torch.from_numpy(data[:, 0]).float().unsqueeze(1) y = torch.from_numpy(data[:, 1]).float().unsqueeze(1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=50) . We will train a least square regressor on one dimension using the above formula: $ large y = b_0 + b_1x$ $ large b_1 = frac{ text{Cov}(x, y)}{Var(x)} = frac{ sum_i{(x_i - mathbb{E}[x]) * (y_i - mathbb{E}[y])}}{ sum_i{(x_i - mathbb{E}[x])^2}}$ where the expectation $ mathbb{E}[x]$ of a vector of random variables is its mean. . def linear_regression_1d(X, y): &quot;&quot;&quot;Trains a linear regression on 1D data Args: X: A numpy array for the training samples y: A numpy array for the labels of each sample &quot;&quot;&quot; X_m = X.mean(dim=0) y_m = y.mean(dim=0) X_c = (X - X_m) # Compute covariance and variance covar = (X_c * (y - y_m)).sum(dim=0) var = X_c.pow(2).sum(dim=0) # Divide covariance by variance b_1 = covar / var # Get bias b_0 = y_m - b_1 * X_m.sum(dim=0) return b_0, b_1 . We are using Pytorch for the matrix calculus. . Plot Regression Line . b_0, b_1 = linear_regression_1d(X_train, Y_train) plt.scatter(X_train, Y_train, marker=&#39;*&#39;) plt.scatter(X_test, Y_test, marker=&#39;.&#39;, color=&#39;red&#39;) x = [int(elt) for elt in range(0, int(X.cpu().numpy().max()))] y = b_0.numpy() + b_1.numpy() * x plt.plot(x, y, color=&#39;red&#39;) ax = plt.gca() ax.set_xlabel(&#39;Number of claims&#39;) ax.set_ylabel(&#39;Total payment&#39;) plt.show() . Because both the data and target are very highly correlated, the regression line fits pretty well the data distribution. . Compute Mean Square Error . pred_train = b_0 + b_1 * X_train err_train = Y_train - pred_train mse_train = err_train.T.mm(err_train) / Y_train.shape[0] pred_test = b_0 + b_1 * X_test err_test = Y_test - pred_test mse_test = err_test.T.mm(err_test) / Y_test.shape[0] print(&#39;Train MSE: t&#39;, mse_train.item()) print(&#39;Test MSE: t&#39;, mse_test.item()) . Train MSE: 1241.5714111328125 Test MSE: 1279.6766357421875 . The train and test MSE are close to each other. The model neither underfit nor overfit. . Conclusion . Linear regression is a simple and interpretable model that fits very well the purpose of a baseline. As we have seen during this experiment, if the features are highly correlated to the target, linear regression performs pretty well. .",
            "url": "https://consciousml.github.io/blog/linear-regression/insurance-data/pytorch/from-scratch/2020/09/12/Linear-Regression.html",
            "relUrl": "/linear-regression/insurance-data/pytorch/from-scratch/2020/09/12/Linear-Regression.html",
            "date": " • Sep 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://consciousml.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://consciousml.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}